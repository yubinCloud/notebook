(window.webpackJsonp=window.webpackJsonp||[]).push([[60],{812:function(t,s,a){"use strict";a.r(s);var n=a(22),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("blockquote",[a("p",[t._v("参考博客：")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://mp.weixin.qq.com/s/b_5_0juNglczXYksT6jSeA",target:"_blank",rel:"noopener noreferrer"}},[t._v("机器如何认识文本 ？NLP中的 Tokenization 方法总结"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/512719309",target:"_blank",rel:"noopener noreferrer"}},[t._v("Word, Subword, and Character-Based Tokenization: Tokenizer for Deep learning（上）"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/38546218",target:"_blank",rel:"noopener noreferrer"}},[t._v("Subword Regularization 阅读笔记"),a("OutboundLink")],1)])])]),t._v(" "),a("p",[t._v('关于 Tokenization，网上有翻译成"分词"的，但是我觉得不是很准确，容易引起误导。一直找不到合适的中文来恰当表达，所以下文采用原汁原味的英文表达。')]),t._v(" "),a("p",[t._v("在正式进入主题之前，先来看看 NLP 任务中最基础也最先需要进行的一步："),a("mark",[t._v("tokenization")]),t._v("。简单说，"),a("strong",[t._v("该操作的目地是将输入文本分割成一个个token，和词典配合以让机器认识文本")]),t._v("。Tokenization 的难点在于如何获得理想的切分，使文本中所有的 token 都具有正确的表义，并且不会存在遗漏（"),a("strong",[t._v("OOV 问题，out of vocabulary")]),t._v("）。")]),t._v(" "),a("p",[t._v("接下来，我们简单梳理下目前主流的 tokenization 方法，及其优缺点。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230106214342770.png",alt:"image-20230106214342770"}}),t._v(" "),a("h2",{attrs:{id:"_1-词粒度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-词粒度"}},[t._v("#")]),t._v(" 1. 词粒度")]),t._v(" "),a("p",[t._v("词粒度的切分就跟人类平时理解文本原理一样，常常用一些工具来完成，例如英文的 NLTK、SpaCy，中文的 jieba、LTP 等。举个栗子：")]),t._v(" "),a("blockquote",[a("p",[t._v("英文：")]),t._v(" "),a("ul",[a("li",[t._v("live in New York ------\x3e live / in / New York /")])])]),t._v(" "),a("blockquote",[a("p",[t._v("中文：")]),t._v(" "),a("ul",[a("li",[t._v("在纽约生活 -----\x3e 在 / 纽约 / 生活")])])]),t._v(" "),a("p",[t._v("在这个切分过程中，注意考虑标点符号的 tokenization，因为每个不同标点都会导致同一单词的不同表示。")]),t._v(" "),a("p",[t._v("词粒度的切分能够非常好地保留完整语义信息，但是如果出现拼写错误、英文中的缩写等情况，鲁棒性一般。另一方面，词切分会产生非常巨大的词表，而且这都不能确保不会出现 out of vocabulary 问题。")]),t._v(" "),a("p",[t._v("空格、标点符号和基于规则的分词都是词粒度的 tokenization 的方法。然后使用 ID 表示每个单词，每个 ID 包含句子中每个单词的上下文和语义信息。")]),t._v(" "),a("h2",{attrs:{id:"_2-字粒度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-字粒度"}},[t._v("#")]),t._v(" 2. 字粒度")]),t._v(" "),a("p",[t._v("字粒度最早应该是2015年 Karpathy"),a("sup",[t._v("[1]")]),t._v(" 提出，简单说英文就是以字母为单位（对于大小写不敏感的任务，甚至可以先转小写再切分），中文就是以字为单位，举个栗子：")]),t._v(" "),a("blockquote",[a("p",[t._v("英文：")]),t._v(" "),a("ul",[a("li",[t._v("live in New York -----\x3e l / i / v /e / i / n / N / e / w / Y / o / r /k")])])]),t._v(" "),a("blockquote",[a("p",[t._v("中文：")]),t._v(" "),a("ul",[a("li",[t._v("在纽约生活 -----\x3e 在 / 纽 / 约 / 生 / 活")])])]),t._v(" "),a("p",[t._v("可以看出，字粒度的切分很好地解决了词粒度的缺陷，鲁棒性增强、词表大大减小。但另一方面，也会带来一些麻烦：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("毫无意义")]),t._v("：一个字母或一个单字本质上并没有任何语义意义；")]),t._v(" "),a("li",[a("strong",[t._v("增加输入计算压力")]),t._v("：减小词表的代价就是输入长度大大增加，从而输入计算变得更耗时耗力；")])]),t._v(" "),a("p",[t._v("如果词粒度不理想，而且字粒度似乎也有自己的问题，那么还有什么替代方法呢？")]),t._v(" "),a("p",[t._v("Here comes subword tokenization!")]),t._v(" "),a("h2",{attrs:{id:"_3-subword-粒度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-subword-粒度"}},[t._v("#")]),t._v(" 3. Subword 粒度")]),t._v(" "),a("p",[t._v("我们理想中的 tokenization 需要满足：")]),t._v(" "),a("ul",[a("li",[t._v("它能够在不需要无限词汇表的情况下处理缺失的标记，即通过有限的已知单词列表来处理无限的潜在词汇；")]),t._v(" "),a("li",[t._v("此外，我们不希望将所有内容分解为单个字符的额外复杂性，因为字符级别可能会丢失单词级别的一些含义和语义细节。")])]),t._v(" "),a("p",[t._v("为此，我们需要考虑如何重新利用『小』单词来创建『大』单词。"),a("mark",[t._v("Subword Tokenization")]),t._v(" 不转换最常见的单词，而是将稀有单词分解成有意义的子词单元。如果"),a("code",[t._v("unfriendly")]),t._v("被标记为一个稀有词，它将被分解为"),a("code",[t._v("un-friendly-ly")]),t._v("，这些单位都是有意义的单位，"),a("code",[t._v("un")]),t._v("的意思是相反的，"),a("code",[t._v("friend")]),t._v("是一个名词，"),a("code",[t._v("ly")]),t._v("则变成副词。这里的挑战是如何进行细分，我们如何获得"),a("code",[t._v("un-friend-ly")]),t._v("而不是"),a("code",[t._v("unfr-ien-dly")]),t._v("。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230106223224223.png",alt:"image-20230106223224223"}})]),t._v(" "),a("p",[t._v("基于 subword 的 tokenization 算法两大原则，也是核心优点：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("原则 1")]),t._v("：不要将常用词拆分成更小的子词。例如，“boy”不应拆分。")]),t._v(" "),a("li",[a("strong",[t._v("原则 2")]),t._v("：将稀有词拆分成更小的有意义的子词。但“boys”应拆分为“boy”和“s”，词义略有不同，但词根相同。")])]),t._v(" "),a("p",[t._v("subword 的两个原则能够继续有效提升 tokenization 效果，既满足 vocabulary 不太大，又满足语义相似性要求。它甚至可以处理它以前从未见过的 word，因为分解会产生已知的 subword，从而方便后续模型的处理。")]),t._v(" "),a("p",[t._v("NLP最火的网红 Transformer 和 BERT 就是 Subword 的带盐人，来看个它们做 tokenization 的栗子：")]),t._v(" "),a("blockquote",[a("p",[t._v("I have a new GPU  ----\x3e [’i’, ’have’, ’a’, ’new’, ’gp’, ’##u’, ’.’]")])]),t._v(" "),a("p",[t._v("subword 粒度切分算法又有以下几种：")]),t._v(" "),a("ul",[a("li",[t._v("BPE")]),t._v(" "),a("li",[t._v("WordPiece")]),t._v(" "),a("li",[t._v("ULM")]),t._v(" "),a("li",[t._v("SentencePiece")])]),t._v(" "),a("blockquote",[a("ul",[a("li",[t._v("BERT 和 DistilBert 使用了 WordPiece")]),t._v(" "),a("li",[t._v("XLNet 和 ALBERT 使用了 Unigram")]),t._v(" "),a("li",[t._v("GPT-2 和 RoBERTa 使用了 BPE")]),t._v(" "),a("li",[t._v("XLM 使用了 SentencePiece 并为中文、日文和泰文添加特定的预分词器。")])])]),t._v(" "),a("h3",{attrs:{id:"_3-1-bpe"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-bpe"}},[t._v("#")]),t._v(" 3.1 BPE")]),t._v(" "),a("p",[a("mark",[t._v("BPE")]),t._v(" 全称 "),a("strong",[t._v("B")]),t._v("yte "),a("strong",[t._v("P")]),t._v("air "),a("strong",[t._v("E")]),t._v("ncoding，字节对编码，首先在 Neural Machine Translation of Rare Words with Subword Units"),a("sup",[t._v("[2]")]),t._v(" 中提出。BPE 迭代地合并最频繁出现的字符或字符序列，具体步骤：")]),t._v(" "),a("ol",[a("li",[t._v("准备足够大的 corpus")]),t._v(" "),a("li",[t._v("定义好所需要的 vocabulary 大小")]),t._v(" "),a("li",[t._v("在每个 word 末尾添加后缀 "),a("code",[t._v("</w>")])]),t._v(" "),a("li",[t._v("将单词拆分为字符序列，并统计单词频率。本阶段的 subword 的粒度是字符。例如，“low”的频率为 5，那么我们将其改写为 "),a("code",[t._v("l o w </w>：5")])]),t._v(" "),a("li",[t._v("统计每一个连续字节对的出现频率，选择最高频者合并成新的 subword")]),t._v(" "),a("li",[t._v("重复第 5 步直到达到第 2 步设定的 subword 词表大小或下一个最高频的字节对出现频率为 1")])]),t._v(" "),a("p",[t._v("所以 BPE 算法就是迭代的每一轮都寻找最频繁的配对（byte pair），然后将他们合并，直到达到 token 限制或迭代限制。")]),t._v(" "),a("p",[t._v("举个例子，我们输入：")]),t._v(" "),a("p",[a("code",[t._v("{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}")])]),t._v(" "),a("p",[t._v("第一轮迭代，统计连续的每两个字节出现的次数，发现 "),a("code",[t._v("e")]),t._v(" 和 "),a("code",[t._v("s")]),t._v(" 共现次数最大，合并成 "),a("code",[t._v("es")]),t._v("，有：")]),t._v(" "),a("p",[a("code",[t._v("{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}")])]),t._v(" "),a("p",[t._v("第二轮迭代，统计连续的每两个字节出现的次数，发现 es 和 t 共现次数最大，合并成 "),a("code",[t._v("est")]),t._v("，有：")]),t._v(" "),a("p",[a("code",[t._v("{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}")])]),t._v(" "),a("p",[t._v("依次继续迭代直到达到预设的 subword 词表大小或下一个最高频的字节对出现频率为 1。")]),t._v(" "),a("p",[t._v("以上是 BPE 的整体流程，关于 BPE 更多细节可以参考：Byte Pair Encoding"),a("sup",[t._v("[3]")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"_3-2-unigram-lm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-unigram-lm"}},[t._v("#")]),t._v(" 3.2 Unigram LM")]),t._v(" "),a("p",[a("mark",[t._v("Unigram 语言建模")]),t._v("首先在 Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"),a("sup",[t._v("[4]")]),t._v(" 中提出，基于所有子词出现是独立的假设，因此子词序列由子词出现概率的乘积生成。")]),t._v(" "),a("p",[t._v("算法步骤如下：")]),t._v(" "),a("ol",[a("li",[t._v("准备足够大的 corpus")]),t._v(" "),a("li",[t._v("定义好所需要的 vocabulary 大小")]),t._v(" "),a("li",[t._v("根据训练语料设置一个合理的 seed 词表")]),t._v(" "),a("li",[t._v("给定词序列优化下一个词出现的概率")]),t._v(" "),a("li",[t._v("计算每个 subword "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 的损失 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"l"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"o"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("，这里的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"l"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"o"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 表示当 subword "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 被移出当前词表的时候，似然函数减小的可能性。")],1),t._v(" "),a("li",[t._v("基于损失 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"l"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"o"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"s"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 对subword排序并保留前 X% 的 subword。为了避免 OOV，保留字符级的单元")],1),t._v(" "),a("li",[t._v("重复第 3 至第 5 步直到达到第 2 步设定的 subword 词表大小或第 5 步的结果不再变化")])]),t._v(" "),a("p",[t._v("这里有一些选择 seed 词表的方法，常用方法是结合语料中所有字符和最高频的子字符串。而且最终的词表也要包含所有独立的字符。因此这种分割可以看作是字符、subword 和 word 的概率混合。")]),t._v(" "),a("p",[t._v("可以看出，Unigram LM 是从一大堆基本符号开始，并逐轮减少词表的大小。")]),t._v(" "),a("h3",{attrs:{id:"_3-3-wordpiece"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-wordpiece"}},[t._v("#")]),t._v(" 3.3 WordPiece")]),t._v(" "),a("p",[a("mark",[t._v("WordPiece")]),t._v(" 首先在 JAPANESE AND KOREAN VOICE SEARCH"),a("sup",[t._v("[5]")]),t._v(" 中提出，最初用于解决日语和韩语语音问题。它在许多方面类似于BPE，只是它基于可能性而不是下一个最高频率对来形成一个新的子词。算法步骤如下：")]),t._v(" "),a("ol",[a("li",[t._v("准备足够大的 corpus")]),t._v(" "),a("li",[t._v("定义好所需要的 vocabulary 大小")]),t._v(" "),a("li",[t._v("将单词拆分成字符序列")]),t._v(" "),a("li",[t._v("基于第 3 步数据训练语言模型")]),t._v(" "),a("li",[t._v("从所有可能的 subword 单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元")]),t._v(" "),a("li",[t._v("重复第 5 步直到达到第 2 步设定的 subword 词表大小或概率增量低于某一阈值")])]),t._v(" "),a("p",[t._v("WordPiece 更像是 BPE 和 Unigram LM 的结合。")]),t._v(" "),a("div",{staticClass:"custom-block warning"},[a("p",{staticClass:"custom-block-title"},[t._v("Summary")]),t._v(" "),a("p",[t._v("简单几句话总结下 Subword 的三种算法：")]),t._v(" "),a("ul",[a("li",[t._v("BPE：只需在每次迭代中使用「出现频率」来确定最佳匹配，直到达到预定义的词汇表大小；")]),t._v(" "),a("li",[t._v("Unigram：使用概率模型训练LM，移除提高整体可能性最小的token；然后迭代进行，直到达到预定义的词汇表大小；")]),t._v(" "),a("li",[t._v("WordPiece：结合 BPE 与 Unigram，使用「出现频率」来确定潜在匹配，但根据合并 token 的概率做出最终决定.")])])]),t._v(" "),a("h3",{attrs:{id:"_3-4-sentencepiece"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-sentencepiece"}},[t._v("#")]),t._v(" 3.4 SentencePiece")]),t._v(" "),a("p",[t._v("到目前为止，可以发现 subword 结合了词粒度和字粒度方法的优点，并避免了其不足。但是，仔细想会发现上述三种 subword 算法都存在一些问题：")]),t._v(" "),a("ul",[a("li",[t._v("都需要提前切分（pretokenization）：这对于某些语言来说，可能是不合理的，因为不可以用空格来分隔单词；")]),t._v(" "),a("li",[t._v("无法逆转：原始输入和切分后序列是不可逆的。举个栗子，下面两者的结果是相等的，即空格的信息经过该操作被丢失：")])]),t._v(" "),a("blockquote",[a("p",[t._v("Tokenize(“World.”) == Tokenize(“World .”)")])]),t._v(" "),a("ul",[a("li",[t._v("不是 end-to-end：使用起来没有那么方便")])]),t._v(" "),a("p",[t._v("ok，here comes "),a("mark",[t._v("SentencePiece")]),t._v("！来看看是怎么解决上述问题的：")]),t._v(" "),a("ul",[a("li",[t._v("SentencePiece 首先将所有输入转换为 unicode 字符。这意味着它不必担心不同的语言、字符或符号，可以以相同的方式处理所有输入；")]),t._v(" "),a("li",[t._v("空白也被当作普通符号来处理。Sentencepiece 显式地将空白作为基本 token 来处理，用一个元符号 “▁”（ U+2581 ）转义空白，这样就可以实现简单地 decoding")]),t._v(" "),a("li",[t._v("SentencePiece 可以直接从 raw text 进行训练，并且官方称非常快！")])]),t._v(" "),a("p",[t._v("SentencePiece 集成了两种 subword 算法，BPE 和 UniLM， WordPiece 则是谷歌内部的子词包，没对外公开。感兴趣的可以去官方开源代码库玩玩：google/sentencepiece"),a("sup",[t._v("[6]")]),t._v("。")]),t._v(" "),a("p",[t._v("放个栗子：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sentencepiece "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" spm\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" s "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SentencePieceProcessor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_file"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'spm.model'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("     s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'New York'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_type"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" enable_sampling"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nbest"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'N'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'e'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁York'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'New'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁York'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'New'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁Y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'o'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'k'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'New'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁York'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'New'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'▁York'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])]),a("p",[t._v("最后，如果想尝试 "),a("code",[t._v("WordPiece")]),t._v("，大家也可以试试 HuggingFace 的 Tokenization 库"),a("sup",[t._v("[7]")]),t._v("：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tokenizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Tokenizer\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tokenizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BPE\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tokenizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pre_tokenizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Whitespace\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tokenizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BpeTrainer\n\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BPE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pre_tokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Whitespace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrainer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BpeTrainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("special_tokens"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[UNK]"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[CLS]"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[SEP]"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[PAD]"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[MASK]"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"wiki.train.raw"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"wiki.valid.raw"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"wiki.test.raw"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\noutput "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hello, y\'all! How are you 😁 ?"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tokens"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# ["Hello", ",", "y", "\'", "all", "!", "How", "are", "you", "[UNK]", "?"]')]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br")])]),a("blockquote",[a("p",[t._v("参考资料：")]),t._v(" "),a("p",[t._v("[1] 2015年Karpathy: https://github.com/karpathy/char-rnn")]),t._v(" "),a("p",[t._v("[2] Neural Machine Translation of Rare Words with Subword Units: https://arxiv.org/abs/1508.07909")]),t._v(" "),a("p",[t._v("[3] Byte Pair Encoding: https://leimao.github.io/blog/Byte-Pair-Encoding/")]),t._v(" "),a("p",[t._v("[4] Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates: https://arxiv.org/abs/1804.10959")]),t._v(" "),a("p",[t._v("[5] JAPANESE AND KOREAN VOICE SEARCH: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf")]),t._v(" "),a("p",[t._v("[6] google/sentencepiece: https://github.com/google/sentencepiece")]),t._v(" "),a("p",[t._v("[7] HuggingFace的 Tokenization 库: https://github.com/huggingface/tokenizers")])])],1)}),[],!1,null,null,null);s.default=e.exports}}]);