(window.webpackJsonp=window.webpackJsonp||[]).push([[95],{849:function(t,a,s){"use strict";s.r(a);var m=s(22),i=Object(m.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[s("mark",[t._v("Life Long Learning")]),t._v("（"),s("strong",[t._v("LLL")]),t._v("）比较符合人们对 AI 的幻想：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"55%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220930231539008.png",alt:"image-20220930231539008"}})]),t._v(" "),s("ul",[s("li",[t._v("人们教他一个 task，他就学会一个 task，最终他就变成了“天网”从而统治人类。")])]),t._v(" "),s("p",[t._v("Life Long Learning 有很多很潮的名字："),s("mark",[t._v("Continual Learning")]),t._v("、Never Ending Learning、Incremental Learning。")]),t._v(" "),s("p",[t._v("在 real-world application 中，LLL 也是有用处的：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916105403356.png",alt:"image-20220916105403356"}})]),t._v(" "),s("h2",{attrs:{id:"_1-catastrophic-forgetting"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-catastrophic-forgetting"}},[t._v("#")]),t._v(" 1. Catastrophic Forgetting")]),t._v(" "),s("h3",{attrs:{id:"_1-1-catastrophic-forgetting-是什么"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-catastrophic-forgetting-是什么"}},[t._v("#")]),t._v(" 1.1 Catastrophic Forgetting 是什么")]),t._v(" "),s("p",[t._v("下面通过一些 example 来说明 Life Long Learning 的一个难点：Catastrophic Forgetting。如下图所示，假设我们有如下两个 task（这看起来像是同一 task 的两个不同 domain，但目前文献上说的就是这样，还做不到真的两个完全不同的任务），我们现在 task 1 上训练一个 model，可以看到其结果在 task 2 也已经达到了 96% 的正确率了，效果非常好，然后我们继续把这个学好的 model 放到 task 2 上去继续学，学完后分析结果发现，这个 model 预期般地在 task 2 上 performance 更好了，但 task 1 上的 performance 却变得很差了，它发生了遗忘：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916110401418.png",alt:"image-20220916110401418"}})]),t._v(" "),s("p",[t._v("也许你会认为可能是因为 model 大小有限，导致其能力有限，从而导致其学完 task 2 后就忘了 task 1。那接下来看另外一个实验：如下图，我们把 task 1 和 task 2 的训练资料倒在一起同时去训练这个 model，可以发现这个 model 是可以同时学好两个 task 的：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916110923520.png",alt:"image-20220916110923520"}})]),t._v(" "),s("p",[t._v("这说明这个 model 有能力同时学好两个 task，但如果依次去学习这两个 task，它会遗忘旧的 task。")]),t._v(" "),s("p",[t._v("这种现象是很普遍的，在一个 QA 的任务上，这个任务是 Given a document, answer the question based on the document. 这个任务使用的是 bAbi corpus，它是一个很小的数据集，包含了 20 个 QA task，如下图：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916132723497.png",alt:"image-20220916132723497"}})]),t._v(" "),s("p",[t._v("用该数据集训练 model，结果如下：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916133356643.png",alt:"image-20220916133356643"}})]),t._v(" "),s("ul",[s("li",[t._v("左图表示一次用 20 个 task 来训练 model，纵轴表示它在 task 5 上的 accuracy，可以看到当训练完 task 5 后 performance 很好，但一旦用 task 6 训完，它在 task 5 上的 accuracy 就直接掉到 0 了")]),t._v(" "),s("li",[t._v("右图表示同时训练这 20 个 task，每个点表示在某个 task 上的 accuracy，可以看到这个 model 是可以在多个 task 上都表现很好的。")])]),t._v(" "),s("p",[t._v("所以我们说 machine “是不为也，非不能也”。这种遗忘很厉害的现象称为 "),s("mark",[t._v("Catastrophic Forgetting")]),t._v("（"),s("strong",[t._v("灾难性遗忘")]),t._v("）：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916135616510.png",alt:"image-20220916135616510"}})]),t._v(" "),s("h3",{attrs:{id:"_1-2-为什么不用-multi-task-training-来解决"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-为什么不用-multi-task-training-来解决"}},[t._v("#")]),t._v(" 1.2 为什么不用 multi-task training 来解决")]),t._v(" "),s("p",[t._v("根据刚刚说的，那你可能觉得将多个 task 的资料一块训练就可以了，这种方式叫做 "),s("mark",[t._v("Multi-task training")]),t._v("，但这会导致 Computation issue 和 Storage issue：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916140338649.png",alt:"image-20220916140338649"}})]),t._v(" "),s("blockquote",[s("p",[t._v("这就好比说，你要让一个人学一门新的课，他学的时候还得把以前学过的也再看一遍才不至于遗忘。")])]),t._v(" "),s("p",[t._v("Multi-task training can be considered as the upper bound of LLL. 因此在做 LLL 研究时，往往先跑个 multi-task training 的结果，知道它的 upper bound 在哪里，然后再比较新的技术和这个 upper bound。")]),t._v(" "),s("h3",{attrs:{id:"_1-3-为什么不-train-a-model-for-each-task"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-为什么不-train-a-model-for-each-task"}},[t._v("#")]),t._v(" 1.3 为什么不 train a model for each task")]),t._v(" "),s("ul",[s("li",[t._v("Eventually we cannot store all the models …")]),t._v(" "),s("li",[t._v("Knowledge cannot transfer across different tasks.")])]),t._v(" "),s("h3",{attrs:{id:"_1-4-life-long-learning-v-s-transfer-learning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-life-long-learning-v-s-transfer-learning"}},[t._v("#")]),t._v(" 1.4 Life-Long learning v.s. Transfer learning")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916151759272.png",alt:"image-20220916151759272"}})]),t._v(" "),s("ul",[s("li",[t._v("transfer learning 只关注在 task 2 上的 performance")]),t._v(" "),s("li",[t._v("life-long learning 同时关注在 task 1 上的 performance")])]),t._v(" "),s("h2",{attrs:{id:"_2-evaluation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-evaluation"}},[t._v("#")]),t._v(" 2. Evaluation")]),t._v(" "),s("p",[t._v("在讲 life-long learning 之前，我们先讲一下对 LLL performance 的"),s("strong",[t._v("评估方法")]),t._v("。")]),t._v(" "),s("p",[t._v("首先我们需要一系列的 task：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916153100486.png",alt:"image-20220916153100486"}})]),t._v(" "),s("ul",[s("li",[t._v("在图片的上面部分中，task 1 是正常的手写数字辨识，task 2 其实也是，只不过这里面的图片是 task 1 中的图片经过某种固定规律转换过来的。甚至有的直接将图片旋转个 15 度来作为新的 task。")]),t._v(" "),s("li",[t._v("图片的下面部分中，task 1 是分类 0 和 1，task 2 是分类 2 和 3 …")])]),t._v(" "),s("p",[t._v("然后我们就可以进行 life-long learning，并对 model 建立如下 evaluation：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916170211883.png",alt:"image-20220916170211883"}})]),t._v(" "),s("p",[t._v("其中 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1)],1)],1),t._v("：after training task i, performance on task j")],1),t._v(" "),s("ul",[s("li",[t._v("If "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:">"}})],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1),t._v("，那 i 是比较后面的任务，j 是比较前面的任务，此时 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1)],1)],1),t._v(" 表示在已经训练完 task i 后，在过去 task j 上的 performance 怎样")],1),t._v(" "),s("li",[t._v("If "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"<"}})],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1),t._v("，那代表我们刚学完 task i 还没去学 task j，但 machine 会不会就无师自通了呢，也就看的是 machine 的 transfer 的能力")],1)]),t._v(" "),s("p",[t._v("最常见的评价方法是 "),s("mark",[t._v("Accuracy")]),t._v("：")]),t._v(" "),s("p"),s("p",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[s("mjx-math",{staticClass:" MJX-TEX",attrs:{display:"true"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"A"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"c"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"c"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"u"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"c"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"y"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mfrac",{attrs:{space:"4"}},[s("mjx-frac",{attrs:{type:"d"}},[s("mjx-num",[s("mjx-nstrut",{attrs:{type:"d"}}),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1),s("mjx-dbox",[s("mjx-dtable",[s("mjx-line",{attrs:{type:"d"}}),s("mjx-row",[s("mjx-den",[s("mjx-dstrut",{attrs:{type:"d"}}),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1)],1)],1)],1)],1)],1)],1),s("mjx-munderover",{attrs:{space:"2"}},[s("mjx-over",{staticStyle:{"padding-bottom":"0.2em","padding-left":"0.473em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"T"}})],1)],1),s("mjx-box",[s("mjx-munder",[s("mjx-row",[s("mjx-base",[s("mjx-mo",{staticClass:"mjx-lop"},[s("mjx-c",{attrs:{c:"2211"}})],1)],1)],1),s("mjx-row",[s("mjx-under",{staticStyle:{"padding-top":"0.167em","padding-left":"0.148em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1)],1)],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1)],1)],1),s("p"),t._v(" "),s("ul",[s("li",[t._v("也就是将上表中最后一行的 R 全部累加起来求平均，用这个值来代表 life long learning 的方法的好坏。")]),t._v(" "),s("li",[t._v("其中，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1)],1)],1)],1)],1)],1),t._v(" 可能是最高的，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1)],1),t._v(" 可能是最低的，因为学完 task T 后，task 1 已经忘得干干净净了。")],1)]),t._v(" "),s("p",[t._v("还有一种评估方法是 "),s("mark",[t._v("Backward Transfer")]),t._v("：")]),t._v(" "),s("p"),s("p",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[s("mjx-math",{staticClass:" MJX-TEX",attrs:{display:"true"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"B"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"c"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"d"}})],1),s("mjx-mtext",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"A0"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"n"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"s"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mfrac",{attrs:{space:"4"}},[s("mjx-frac",{attrs:{type:"d"}},[s("mjx-num",[s("mjx-nstrut",{attrs:{type:"d"}}),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1),s("mjx-dbox",[s("mjx-dtable",[s("mjx-line",{attrs:{type:"d"}}),s("mjx-row",[s("mjx-den",[s("mjx-dstrut",{attrs:{type:"d"}}),s("mjx-mrow",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1)],1)],1)],1),s("mjx-munderover",{attrs:{space:"2"}},[s("mjx-over",{staticStyle:{"padding-bottom":"0.142em","padding-left":"0.021em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-box",[s("mjx-munder",[s("mjx-row",[s("mjx-base",[s("mjx-mo",{staticClass:"mjx-lop"},[s("mjx-c",{attrs:{c:"2211"}})],1)],1)],1),s("mjx-row",[s("mjx-under",{staticStyle:{"padding-top":"0.167em","padding-left":"0.148em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"["}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-msub",{attrs:{space:"3"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"]"}})],1)],1)],1)],1),s("p"),t._v(" "),s("ul",[s("li",[t._v("它表示一系列值的平均，上表中的每一列产生一个差值，是用最下面一行的 R 减去这一列上对角线上的 R 得到的，这个过程如下图所示：")])]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918151014362.png",alt:"image-20220918151014362"}})]),t._v(" "),s("p",[t._v("因此每一个差值也就代表着，对 task i 来说，在刚学完 task i 上的正确率，和学完 T 个 task 后在 task i 上的正确率差多少。")]),t._v(" "),s("p",[t._v("还有一种评估方式是 "),s("mark",[t._v("Forward Transfer")]),t._v("：")]),t._v(" "),s("p"),s("p",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[s("mjx-math",{staticClass:" MJX-TEX",attrs:{display:"true"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"o"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"d"}})],1),s("mjx-mtext",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"A0"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"n"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"s"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"r"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mfrac",{attrs:{space:"4"}},[s("mjx-frac",{attrs:{type:"d"}},[s("mjx-num",[s("mjx-nstrut",{attrs:{type:"d"}}),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1),s("mjx-dbox",[s("mjx-dtable",[s("mjx-line",{attrs:{type:"d"}}),s("mjx-row",[s("mjx-den",[s("mjx-dstrut",{attrs:{type:"d"}}),s("mjx-mrow",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1)],1)],1)],1),s("mjx-munderover",{attrs:{space:"2"}},[s("mjx-over",{staticStyle:{"padding-bottom":"0.2em","padding-left":"0.473em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"T"}})],1)],1),s("mjx-box",[s("mjx-munder",[s("mjx-row",[s("mjx-base",[s("mjx-mo",{staticClass:"mjx-lop"},[s("mjx-c",{attrs:{c:"2211"}})],1)],1)],1),s("mjx-row",[s("mjx-under",{staticStyle:{"padding-top":"0.167em","padding-left":"0.148em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"["}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-msub",{attrs:{space:"3"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"R"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"0"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"]"}})],1)],1)],1)],1),s("p"),t._v(" "),s("p",[t._v("这个指标想要问的问题是说，在看过一系列的任务后，在还没有看过 task i 而学完 task 1 ~ task i-1 后，你的模型到底可以学出什么样的成果：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918152153885.png",alt:"image-20220918152153885"}})]),t._v(" "),s("p",[t._v("下面，我们就要讲 life long learning 的解法了，这里讲三种解法：")]),t._v(" "),s("ul",[s("li",[t._v("Selective Synaptic Plasticity")]),t._v(" "),s("li",[t._v("Additional Neural Resource Allocation")]),t._v(" "),s("li",[t._v("Memory Reply")])]),t._v(" "),s("h2",{attrs:{id:"_3-selective-synaptic-plasticity"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-selective-synaptic-plasticity"}},[t._v("#")]),t._v(" 3. Selective Synaptic Plasticity")]),t._v(" "),s("h3",{attrs:{id:"_3-1-why-catastrophic-forgetting"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-why-catastrophic-forgetting"}},[t._v("#")]),t._v(" 3.1 Why Catastrophic Forgetting?")]),t._v(" "),s("p",[t._v("我们先来想一下为什么 Catastrophic Forgetting 这件事情会发生呢？如下图所示，我们有两个 task，model 有两个参数 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v(" 和 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1),t._v("，下图是两个 task 的 loss surface，蓝色越深代表 loss 越小：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918162704219.png",alt:"image-20220918162704219"}})]),t._v(" "),s("ul",[s("li",[t._v("在 task 1 上学习时，参数变化 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"2192"}})],1),s("mjx-msup",{attrs:{space:"4"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1),t._v("，然后再从 task 2 上学习，参数变化 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"2192"}})],1),s("mjx-msup",{attrs:{space:"4"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1),t._v("。可我们再回到 task 1 上来看，会发现在 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1),t._v(" 上 loss 又是很大的，从而产生了 Catastrophic Forgetting 现象。")],1),t._v(" "),s("li",[t._v("那我们在 task 2 上学习时，可不可以是让 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v(" 的移动朝下一点，像红色标志那样，这样在两个 task 上就会都表现不错。这种思路就是将要讲的 Selective Synaptic Plasticity 方法。")],1)]),t._v(" "),s("h3",{attrs:{id:"_3-2-regularization-based-的基本想法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-regularization-based-的基本想法"}},[t._v("#")]),t._v(" 3.2  regularization-based 的基本想法")]),t._v(" "),s("p",[s("strong",[t._v("Basic Idea")]),t._v(": Some parameters in the model are important to the previous tasks. Only change the unimportant parameters.")]),t._v(" "),s("p",[t._v("那我们假设 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1),t._v(" 是从前一个 task 中学习到的，那我们就会给每个 parameter  "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msubsup",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.292em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.18em"}}),s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 一个 guard "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("，然后在学习当前 task 时改写一下 loss function：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918163740382.png",alt:"image-20220918163740382"}})]),t._v(" "),s("ul",[s("li",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 越大，就代表我们越希望 parameter "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 与原先的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msubsup",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.292em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.18em"}}),s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 很接近。")],1)]),t._v(" "),s("p",[t._v("注意，我们并不要求所有参数都与原来很接近，而只是要求部分参数与原来的很接近，具体要求哪些是由 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 决定的。假如我们很极端：")],1),t._v(" "),s("ul",[s("li",[t._v("If "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v(", there is no constraint on "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("  =>  "),s("strong",[t._v("Catastrophic Forgetting")])],1),t._v(" "),s("li",[t._v("If "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mi",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"221E"}})],1)],1)],1),t._v("， "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" would always be equal to "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msubsup",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.292em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.18em"}}),s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("  => "),s("strong",[t._v("Intransigence")])],1)]),t._v(" "),s("h3",{attrs:{id:"_3-3-的设定"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-的设定"}},[t._v("#")]),t._v(" 3.3 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 的设定")],1),t._v(" "),s("p",[t._v("对 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 的设定是研究的重点，这里是采用了人工设定的方法。那怎么设定呢？本节只讲一下大概的思路。")],1),t._v(" "),s("p",[t._v("以下图为例，我们可以让 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 在每个参数方向上动一下，看看对 loss 的影响，从而判断这个参数的重要性，进而来设定 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 的大小：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918165120793.png",alt:"image-20220918165120793"}})]),t._v(" "),s("p",[t._v("如果按照上面分析，我们将 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v(" 设的比较小，将 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1),t._v(" 设的比较大，那么在 task 2 上训练的结果就如下图所示：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918170013332.png",alt:"image-20220918170013332"}})]),t._v(" "),s("p",[t._v("可以看到在 task 2 上 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v(" 的移动方向改变了，而且新的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1),t._v(" 放到 task 1 中对它的伤害也不大。")],1),t._v(" "),s("p",[t._v("接下来的图片就是 paper 中一个真实的实验结果，这种图在 life-long learning 中是非常常见的：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918170635659.png",alt:"image-20220918170635659"}})]),t._v(" "),s("ul",[s("li",[t._v("横轴代表依序训练的过程，纵轴代表在不同 task 上的正确率")]),t._v(" "),s("li",[t._v("来看不同颜色的线，SGD 的蓝线代表 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v(" 的实验；L2 的绿线代表 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v(" 的实验，可以看到 L2 在 task A 上一直表现不错，但学习 task B 时却在 task B 上 performance 一直不太好，出现了 intransigence 现象。因此，如果所有的参数都有一样的限制，那么这对模型来说限制太大了，会导致这个 task 训练不起来。")],1),t._v(" "),s("li",[t._v("再来看红色这条线，如果我们给不同的参数不用的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("，有的参数的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 大，有的参数的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 小，只固定某些参数，而某些参数可以变动，也就得到了红色这条线。可以看到红色这条线在每个任务上表现都是最好的。")],1)]),t._v(" "),s("p",[t._v("其实我们没有真的告诉你 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 怎么算的，具体的算法可以参考一下 paper，每种方法都是他自己要考量的点和所要解决的问题：")],1),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1612.00796",target:"_blank",rel:"noopener noreferrer"}},[t._v("Elastic Weight Consolidation（EWC）"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1703.04200",target:"_blank",rel:"noopener noreferrer"}},[t._v("Synaptic Intelligence（SI）"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1711.09601",target:"_blank",rel:"noopener noreferrer"}},[t._v("Memory Aware Synapses（MAS）"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1801.10112",target:"_blank",rel:"noopener noreferrer"}},[t._v("RWalk"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://openreview.net/forum?id=BJge3TNKwH",target:"_blank",rel:"noopener noreferrer"}},[t._v("Sliced Cramer Preservation（SCP）"),s("OutboundLink")],1)])]),t._v(" "),s("h3",{attrs:{id:"_3-4-gradient-episodic-memory-gem"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-gradient-episodic-memory-gem"}},[t._v("#")]),t._v(" 3.4 Gradient Episodic Memory（GEM）")]),t._v(" "),s("p",[t._v("除了以上讲的 regularization-based 的方法以外，更早期的一个做法叫做 Gradient Episodic Memory（GEM），它也是一个很有效的方法，但它不是在参数上做限制，而是在 gradient update 的方向上做限制。如下图所示：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220930201916262.png",alt:"image-20220930201916262"}})]),t._v(" "),s("p",[t._v("绿色箭头 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1),t._v(" 代表再 task 1 上参数 update 的方向，红色虚线箭头 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1)],1)],1),t._v(" 代表在 task 2 上参数 update 的方向。可以看到，当参数在 task 2 上按照 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1)],1)],1),t._v(" 进行 update 时，由于 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1)],1)],1),t._v(" 和 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1),t._v(" 的方向不一致（即 inner product 值小于 0），那这个 model 就会在 task 1 上 loss 变大。"),s("strong",[t._v("所以我们刻意修改 update 的方向，从 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1)],1)],1),t._v(" 变成 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2032"}})],1)],1)],1)],1)],1),t._v("，这样就可以减轻 catastrophic forgetting 所造成的问题")],1),t._v("。这里修改的 criterion 是希望找到一个 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2032"}})],1)],1)],1)],1)],1),t._v("，让 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2032"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"22C5"}})],1),s("mjx-msup",{attrs:{space:"3"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"2265"}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v("，而且 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2032"}})],1)],1)],1)],1)],1),t._v(" 不能与 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1)],1)],1),t._v(" 差太多。")],1),t._v(" "),s("p",[t._v("讲到这边，有没有感觉这个方法有点有猫腻的地方呢？我们要算 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1),t._v("，就意味着我们需要存储 task 1 的资料，所以 "),s("strong",[t._v("GEM 方法的一个劣势就是说我们需要把以前的训练资料给存下来")]),t._v("。但是这与 Life Long Learning 想要追求的是不一致的，因为 Life Long Learning 是不希望把过去的资料都存下来，否则累积的越来越多最终会存不下。所以 GEM 是有点违反 life long learning 的精神。但其实这个问题也没有特别严重，因为 GEM 只需要存非常少量的资料就好。，只要一点点资料就可以计算出 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1),t._v("。更仔细想一下，像之前的 EWC 的方法也需要占用额外的空间来储存旧的模型和 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("，所以 GEM 只要额外占用的空间没有比 EWC 之类的方法多太多的话，其实也是可以接受的。")],1),t._v(" "),s("h2",{attrs:{id:"_4-additional-neural-resource-allocation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-additional-neural-resource-allocation"}},[t._v("#")]),t._v(" 4. Additional Neural Resource Allocation")]),t._v(" "),s("h3",{attrs:{id:"_4-1-progressive-neural-networks"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-progressive-neural-networks"}},[t._v("#")]),t._v(" 4.1 Progressive Neural Networks")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://arxiv.org/abs/1606.04671",target:"_blank",rel:"noopener noreferrer"}},[t._v("paper"),s("OutboundLink")],1)]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220930205724716.png",alt:"image-20220930205724716"}})]),t._v(" "),s("p",[t._v("它的想法是，训练 task 1 时有一个蓝色的 network，然后训练 task 2 时就不要再动 task 1 学到的那个模型，而是另外再多开一个绿色的 network，这个新开的 network 也会吃之前的 network 的 hidden layer 的 output 作为输入，这样 task 1 有学到什么有用的资讯的话，也是可以被 task 2 的 network 所利用，但是就是不要再变动 task 1 学出来的模型的参数了。task 3 也是如此，需要再开一个 network。")]),t._v(" "),s("p",[t._v("这样的做法就不会再产生 catastrophic forgetting 的问题，因为旧的 network 就完全没有再动它了。但这产生的新的问题是，每一次训练一个新的 task 时，就需要额外的一块空间来产生一个新的 network，这样整个模型会随着学习的 task 的增多而长大，这样模型会最终过大而使得 memory 无法存下它了。")]),t._v(" "),s("h3",{attrs:{id:"_4-2-packnet"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-packnet"}},[t._v("#")]),t._v(" 4.2 PackNet")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://arxiv.org/abs/1711.05769",target:"_blank",rel:"noopener noreferrer"}},[t._v("paper"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("PackNet 的思想就与 Progressive Neural Networks 相反了，它是先有一个很大的 network，然后每次 pruning 掉一部分，并只准用所允许的其中一部分参数来训练。如下图所示：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220930212434906.png",alt:"image-20220930212434906"}})]),t._v(" "),s("p",[t._v("我们把圈圈想象成参数，当有 task 1 的资料进来时，就只准用有黑色框的圈圈的参数；然后 task 2 再进来，就只准用这个橙色的参数 ......")]),t._v(" "),s("p",[t._v("这个方法的好处是，你的参数量不会随着 task 的增多而不断增加。但其实相较于 Progressive Neural Networks 的方法来说，这个方法也只是朝三暮四而已，因为它只不过是一开始就开了一个很大的 network。")]),t._v(" "),s("h3",{attrs:{id:"_4-3-compacting-picking-and-growing-cpg"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-compacting-picking-and-growing-cpg"}},[t._v("#")]),t._v(" 4.3 Compacting, Picking, and Growing（CPG）")]),t._v(" "),s("p",[t._v("刚刚讲的 PackNet 与 Progressive Neural Networks 其实是可以结合在一起的，这就是很出名的 "),s("mark",[t._v("CPG")]),t._v("。这个 model 是既可以增加新的参数，每一次又都只保留部分的参数用来拿来做训练。至于方法的细节可以参考"),s("a",{attrs:{href:"https://arxiv.org/abs/1910.06562",target:"_blank",rel:"noopener noreferrer"}},[t._v("论文"),s("OutboundLink")],1),t._v("。")]),t._v(" "),s("h2",{attrs:{id:"_5-memory-reply"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-memory-reply"}},[t._v("#")]),t._v(" 5. Memory Reply")]),t._v(" "),s("p",[t._v("这个方法非常直觉，我们之前讲过说只要把所有的资料统统倒在一起，就不会有 catastrophic forgetting 的问题，但我们又说，不能够去存储过去的资料，那我们就干脆直接训练一个 generative model，它可以产生 previous task 的 pseudo-data。")]),t._v(" "),s("p",[t._v("这就是说，我们在 solve task 1 时，不仅训练一个 classifier 来解 task 1，同时还训练了一个 generator，它可以产生 task 1 的资料：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220930214433127.png",alt:"image-20220930214433127"}})]),t._v(" "),s("p",[t._v("接下来再训练 task 2 的时候，如果只把 task 2 的资料给 model 看，那么可能会产生 catastrophic forgetting 的问题，这个时候就可以让 task 1 训练的 generator 产生 task 1 的 pseudo-data 混合着给 task 2 的 classifier 来训练：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220930214740044.png",alt:"image-20220930214740044"}})]),t._v(" "),s("p",[t._v("那这个方法合不合理呢？这个其实见仁见智了，因为额外训练的 generator 也会占用额外的空间。但如果这些 generator 所占用的空间比你储存 previous task 的 training data 所需要的空间更小的话，那也许这也是一个有效的方法。事实上，从经验上来看，这种方法其实是非常有效的，这种方法的实验结果往往都可以逼近 life long learning 的 upper bound 了。")]),t._v(" "),s("p",[t._v("可以参考一下文献详细了解：")]),t._v(" "),s("ul",[s("li",[t._v("https://arxiv.org/abs/1705.08690")]),t._v(" "),s("li",[t._v("https://arxiv.org/abs/1711.10563")]),t._v(" "),s("li",[t._v("https://arxiv.org/abs/1909.03329")])]),t._v(" "),s("h3",{attrs:{id:"_6-其他-continual-learning-的-scenarios"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-其他-continual-learning-的-scenarios"}},[t._v("#")]),t._v(" 6. 其他 continual learning 的 scenarios")]),t._v(" "),s("p",[t._v("其实回看我们刚刚讲的 life long learning 的 scenario，我们都是假设说每个任务需要的模型都是一样的，甚至我们强迫说每一个任务所要训练的 classifier 所需要的 class 的量都是一样的。那假设不同的任务中 class 的数目不一样，也就是说新的任务需要“adding new classes”，那有没有办法解呢？也是有的，下面列着一些参考文献：")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1606.09282",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning without forgetting（LwF）"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1611.07725",target:"_blank",rel:"noopener noreferrer"}},[t._v("iCaRL: Incremental Classifier and Representation Learning"),s("OutboundLink")],1)])]),t._v(" "),s("p",[t._v("其实我们今天讲的 life long learning，也就是 continual learning，其实只是整个该领域研究里面的其中一个 scenario，其实 continual learning 还有很多不同的 scenario，可以阅读文献 "),s("a",{attrs:{href:"https://arxiv.org/abs/1904.07734",target:"_blank",rel:"noopener noreferrer"}},[t._v("Three scenarios for continual learning"),s("OutboundLink")],1),t._v("，它将 continual learning 分成了三种 scenario，而今天讲的只是其中最简单的一种。")]),t._v(" "),s("h2",{attrs:{id:"_7-curriculum-learning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-curriculum-learning"}},[t._v("#")]),t._v(" 7. Curriculum Learning")]),t._v(" "),s("p",[t._v("之前遗留着一个问题，交换 task 的训练顺序，会不会有非常不一样的结果呢？确实是会有的，来看下面这个例子。")]),t._v(" "),s("p",[t._v("我们之前是让机器先学有杂讯的图片 task 1，再学没有杂讯的图片 task 2；现在反过来，先学没有杂讯的图片 task 2，再学有杂讯的图片 task 1，结果如下：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220930230241533.png",alt:"image-20220930230241533"}})]),t._v(" "),s("p",[t._v("会发现下面这种训练顺序没有产生 catastrophic forgetting 的问题。")]),t._v(" "),s("p",[t._v("所以看起来，"),s("strong",[t._v("task 的顺序是重要的")]),t._v("。这种研究 what is the proper learning order 的方向叫做 "),s("mark",[t._v("Curriculum Learning")]),t._v("。")])],1)}),[],!1,null,null,null);a.default=i.exports}}]);