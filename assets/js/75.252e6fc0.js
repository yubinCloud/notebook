(window.webpackJsonp=window.webpackJsonp||[]).push([[75],{828:function(t,s,a){"use strict";a.r(s);var n=a(22),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("本章我们将进一步探索 seq2seq 的可能性（以及 RNN 的可能性）。Attention 毫无疑问是近年来深度学习领域最重要的技术之一。本章的目标是在代码层面理解 Attention 的结构，然后将其应用于实际问题。")]),t._v(" "),a("h2",{attrs:{id:"_1-attention-的结构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-attention-的结构"}},[t._v("#")]),t._v(" 1. Attention 的结构")]),t._v(" "),a("p",[t._v("我们将介绍进一步强化 seq2seq 的"),a("strong",[t._v("注意力机制")]),t._v("。基于 Attention 机制，seq2seq 可以像我们人类一样，将“注意力”集中在必要的信息上。")]),t._v(" "),a("blockquote",[a("p",[t._v("上一章我们已经对 seq2seq 进行了改进，但那些只能算是“小改进”。下面将要说明的 "),a("strong",[t._v("Attention 技术才是解决 seq2seq 的问题的“大改进”")]),t._v("。")])]),t._v(" "),a("h3",{attrs:{id:"_1-1-seq2seq-存在的问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-seq2seq-存在的问题"}},[t._v("#")]),t._v(" 1.1 seq2seq 存在的问题")]),t._v(" "),a("p",[t._v("seq2seq 中使用编码器对时序数据进行编码，输出是固定长度的向量，"),a("strong",[t._v("问题在于无论输入语句的长度如何，其信息都会被塞入一个固定长度的向量中")]),t._v("。而这早晚都会遇到瓶颈，有用的信息会从向量中溢出。")]),t._v(" "),a("p",[t._v("现在我们就来改进 seq2seq。首先改进编码器，然后再改进解码器。")]),t._v(" "),a("h3",{attrs:{id:"_1-2-改进-encoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-改进-encoder"}},[t._v("#")]),t._v(" 1.2 改进 Encoder")]),t._v(" "),a("p",[t._v("编码器的输出的长度应该根据输入文本的长度相应地改变。之前我们只将 LSTM 层的最后的隐藏状态传递给解码器，改进为可以使用各个时刻的 LSTM 层的隐藏状态，从而可以获得和输入的单词数相同数量的向量：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402222216049.png",alt:"image-20220402222216049"}}),t._v(" "),a("p",[t._v("如上例，输入了 5 个单词，此时编码器输出 5 个向量。这样一来，编码器就摆脱了“一个固定长度的向量”的制约。")]),t._v(" "),a("blockquote",[a("p",[t._v("在许多深度学习框架中，在初始化 RNN 层时，可以选择是返回“全部时刻的隐藏状态向量”，还是返回“最后时刻的隐藏状态向量”。比如，在 Keras 中，在初始化 RNN 层时，可以设置 "),a("code",[t._v("return_sequences")]),t._v(" 为 True 或者 False。")])]),t._v(" "),a("p",[t._v("我们需要关注 LSTM 层的隐藏状态的“内容”。有一点可以确定的是，"),a("strong",[t._v("各个时刻的隐藏状态中包含了大量当前时刻的输入单词的信息")]),t._v("：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402223535528.png",alt:"image-20220402223535528"}}),t._v(" "),a("ul",[a("li",[t._v("编码器输出的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 矩阵就可以视为各个单词对应的向量集合。")],1)]),t._v(" "),a("blockquote",[a("p",[t._v("因为编码器是从左向右处理的，所以严格来说，刚才的“猫”向量中含有“吾輩”“は”“猫”这3个单词的信息。"),a("u",[t._v("考虑整体的平衡性，最好均衡地含有单词“猫”周围的信息。在这种情况下，从两个方向处理时序数据的双向RNN（或者双向LSTM）比较有效")]),t._v("。")])]),t._v(" "),a("p",[t._v("以上就是我们的改进：将编码器的全部时刻的隐藏状态取出来，从而编码器可以根据输入语句的长度，成比例地编码信息。")]),t._v(" "),a("p",[t._v("接下来，我们对解码器进行改进。因为解码器的改进有许多值得讨论的地方，所以我们分 3 部分进行。")]),t._v(" "),a("h3",{attrs:{id:"_1-3-解码器的改进-1"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-解码器的改进-1"}},[t._v("#")]),t._v(" 1.3 解码器的改进 ①")]),t._v(" "),a("p",[t._v("编码器和解码器的关系：编码器整体输出各个单词对应的 LSTM 层的隐藏状态向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v("，然后传递给解码器进行时间序列的转换。之前的解码器只用到了编码器 LSTM 层的最后一个隐藏状态，现在我们改进解码器，以便能够使用全部 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v("。")],1),t._v(" "),a("p",[t._v("我们在进行翻译时，大脑做了什么呢？比如，在将“吾輩は猫である”这句话翻译为英文时，肯定要用到诸如“吾輩 = I”“猫 = cat”这样的知识。也就是说，"),a("strong",[t._v("可以认为我们是专注于某个单词（或者单词集合），随时对这个单词进行转换的")]),t._v("。那么，我们可以在 seq2seq 中重现同样的事情吗？确切地说，我们可以让 seq2seq 学习“输入和输出中哪些单词与哪些单词有关”这样的对应关系吗？")]),t._v(" "),a("blockquote",[a("p",[t._v("在机器翻译的历史中，很多研究都利用“猫=cat”这样的单词对应关系的知识。这样的表示单词（或者词组）对应关系的信息称为"),a("strong",[t._v("对齐")]),t._v("（alignment）。到目前为止，对齐主要是手工完成的，而我们将要介绍的 "),a("strong",[t._v("Attention 技术则成功地将对齐思想自动引入到了 seq2seq 中")]),t._v("。这也是从“手工操作”到“机械自动化”的演变。")])]),t._v(" "),a("p",[t._v("从现在开始，我们的目标是找出与“翻译目标词”有对应关系的“翻译源词”的信息，然后利用这个信息进行翻译。也就是说，我们的目标是"),a("u",[t._v("仅关注必要的信息，并根据该信息进行时序转换。这个机制称为 "),a("strong",[t._v("Attention")])]),t._v("。")]),t._v(" "),a("p",[t._v("先看一下 Decoder 的整体框架：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402225550054.png",alt:"image-20220402225550054"}}),t._v(" "),a("p",[t._v("我们新增一个进行“某种计算”的层。这个“某种计算”接收（解码器）各个时刻的 LSTM 层的隐藏状态和编码器的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v("。然后，从中选出必要的信息，并输出到 Affine 层。与之前一样，编码器的最后的隐藏状态向量（即 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 的最后一行）传递给解码器最初的 LSTM 层。")],1),t._v(" "),a("p",[t._v("该网络所做的工作是提取单词对齐信息。具体来说，就是"),a("strong",[t._v("从 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 中选出与各个时刻解码器输出的单词有对应关系的单词向量")],1),t._v("（"),a("em",[t._v("这有点像将注意力集中到重要的信息上")]),t._v("），如当解码器输出“I”时，从 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 中选出“吾輩”的对应向量。我们希望“某种计算”实现这种选择操作，不过这里有个问题，就是选择（从多个事物中选取若干个）这一操作是无法进行微分的。")],1),t._v(" "),a("blockquote",[a("p",[t._v("神经网络的学习一般通过误差反向传播法进行。因此，如果使用可微分的运算构造网络，就可以在误差反向传播法的框架内进行学习；而"),a("strong",[t._v("如果不使用可微分的运算，基本上也就没有办法使用误差反向传播法")]),t._v("。")])]),t._v(" "),a("p",[t._v("将“选择”这一操作换成可微分的运算的一个思路是："),a("strong",[t._v("与其“单选”，不如“全选”，并另行计算表示各个单词重要度（贡献值）的权重")]),t._v("：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402231550018.png",alt:"image-20220402231550018"}}),t._v(" "),a("p",[a("strong",[t._v("这里使用了表示各个单词重要度的权重，记为 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1)],1),t._v("。它的各元素是 0.0 ～ 1.0 的标量，总和是 1。然后，计算这个表示各个单词重要度的权重和单词向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 的加权和，可以获得目标向量：")],1),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402231736575.png",alt:"image-20220402231736575"}}),t._v(" "),a("p",[t._v("计算单词向量的加权和，这里将结果称为"),a("strong",[t._v("上下文向量")]),t._v("， 并用符号 c 表示。这个加权和计算基本代替了“选择”向量的操作。比如上图中的上下文向量就含有较多的“吾輩”向量的成分（"),a("em",[t._v("如此便实现了注意力的集中")]),t._v("）。")]),t._v(" "),a("blockquote",[a("p",[t._v("上下文向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"c"}})],1)],1)],1),t._v(" 中包含了当前时刻进行变换（翻译）所需的信息。更确切地说，模型要从数据中学习出这种能力。")],1)]),t._v(" "),a("p",[t._v("这里随意地生成编码器的输出 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 和各个单词的权重 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v("，并给出求它们的加权和的实现：")],1),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\nT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\nhs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\na "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.03")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.02")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nar "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repeat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (5, 4)")]),t._v("\n\nt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" ar\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (5, 4)")]),t._v("\n\nc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (4,)")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br")])]),a("ul",[a("li",[t._v("时序数据的长度 T=5，隐藏状态向量的元素个数 H=4")]),t._v(" "),a("li",[t._v("代码 "),a("code",[t._v("ar = a.reshape(5, 1).repeat(4, axis=1)")]),t._v(" 将 a 转化为 ar：")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403065249268.png",alt:"image-20220403065249268"}}),t._v(" "),a("ul",[a("li",[t._v("先计算 hs 与 ar 的对应元素的乘积，然后通过 "),a("code",[t._v("c=sum(hs*ar, axis=0)")]),t._v(" 消除第 0 个轴得到形状为 (4,) 的张量，即加权和。")])]),t._v(" "),a("blockquote",[a("p",[a("code",[t._v("repeat()")]),t._v(" 方法复制多维数组的元素生成新的多维数组，axis 指定要进行复制的轴（维度），比如在 x 的形 状为 (X, Y, Z) 的情况下，"),a("code",[t._v("x.repeat(3, axis=1)")]),t._v(" 沿 x 的第1个轴方向（第 1个维度）进行复制，生成形状为 (X, 3*Y, Z) 的多维数组。")]),t._v(" "),a("p",[t._v("这里其实也可以不用 repeat 而是使用 numpy 的广播功能，但我们为了显式表现出 repeat 节点，所以采用了显式调用 "),a("code",[t._v("repeat")]),t._v(" 函数：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403065525024.png",alt:"image-20220403065525024"}})]),t._v(" "),a("p",[t._v("这里计算加权和的计算图可以绘制为：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403070500922.png",alt:"image-20220403070500922"}}),t._v(" "),a("p",[t._v("我们将这个计算加权和的计算图实现为 Weight Sum 层。")]),t._v(" "),a("details",{staticClass:"custom-block details"},[a("summary",[t._v("Weight Sum 层的实现")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WeightSum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n\n        ar "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repeat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        t "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" ar\n        c "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" c\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("backward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ar "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache\n        N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n        dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repeat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        dar "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" hs\n        dhs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" ar\n        da "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" dhs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" da\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br")])]),a("ul",[a("li",[t._v("这个层没有要学习的参数")])])]),t._v(" "),a("h3",{attrs:{id:"_1-4-解码器的改进-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-解码器的改进-2"}},[t._v("#")]),t._v(" 1.4 解码器的改进 ②")]),t._v(" "),a("p",[t._v("有了表示各个单词重要度的权重 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v("，就可以通过加权和获得上下文向量。那么，"),a("strong",[t._v("怎么求这个 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 呢？只需要让模型自动学习它")],1),t._v("。")],1),t._v(" "),a("p",[t._v("下面我们来看一下各个单词的权重 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 的求解方法。下图是从编码器的处理开始到解码器第一个 LSTM 层输出隐藏状态向量的处理为止的流程：")],1),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403071308638.png",alt:"image-20220403071308638"}}),t._v(" "),a("p",[t._v("用 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 表示解码器的 LSTM 层的隐藏状态向量，此时，我们的目标是用数值表示这个 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 在多大程度上和 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 的各个单词向量“相似”。这里我们使用最简单的向量内积。")],1),t._v(" "),a("blockquote",[a("p",[t._v("计算向量相似度的方法有好几种。除了内积之外，还有使用小型的神经网络输出得分的做法。")]),t._v(" "),a("p",[t._v("文献"),a("sup",[t._v("[49]")]),t._v("提出了几种输出得分的方法")])]),t._v(" "),a("p",[t._v("下面用图表示基于内积计算向量间相似度的处理流程：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403075145804.png",alt:"image-20220403075145804"}}),t._v(" "),a("ul",[a("li",[t._v("这里通过向量内积算出 h 和 hs 的各个单词向量之间 的相似度，并将其结果表示为 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v("。不过这个 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 是正规化之前的值，也称为得分。")],1)]),t._v(" "),a("p",[t._v("接下来使用 softmax 对 s 进行正规化：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403075338015.png",alt:"image-20220403075338015"}}),t._v(" "),a("p",[t._v("使用 Softmax 函数之后，输出的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 的各个元素的值在 0.0 ～ 1.0，总和 为 1，这样就求得了表示各个单词权重的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v("。")],1),t._v(" "),a("p",[t._v("从代码的角度看一下这个处理过程：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\nhs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nhr "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repeat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# hr = h.reshape(N, 1, H) # 广播")]),t._v("\n\nt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" hr\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (10, 5, 4)")]),t._v("\n\ns "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (10, 5)")]),t._v("\n\nsoftmax "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\na "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (10, 5)")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br")])]),a("p",[t._v("计算图绘制如下：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403075631059.png",alt:"image-20220403075631059"}}),t._v(" "),a("p",[t._v("我们将这个计算图表示的处理实现为 "),a("code",[t._v("AttentionWeight")]),t._v(" 类：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AttentionWeight")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n\n        hr "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repeat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        t "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" hr\n        s "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        a "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" a\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("backward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" da"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hr "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache\n        N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n\n        ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("da"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repeat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        dhs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" hr\n        dhr "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" hs\n        dh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dhr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" dhs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dh\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br")])]),a("h3",{attrs:{id:"_1-5-解码器的改进-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-5-解码器的改进-3"}},[t._v("#")]),t._v(" 1.5 解码器的改进 ③")]),t._v(" "),a("p",[t._v("之前我们实现了 Weight Sum 层和 Attention Weight 层，现在我们将这两层组合起来：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403082632703.png",alt:"image-20220403082632703"}}),t._v(" "),a("p",[t._v("上图显示了用于获取上下文向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"c"}})],1)],1)],1),t._v(" 的计算图的全貌：")],1),t._v(" "),a("ul",[a("li",[t._v("Attention Weight 层关注编码器输出的各个单词向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v("，并计算各个单词的权重 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1)],1),t._v(" "),a("li",[t._v("Weight Sum 层计算 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 和 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 的加权和，并输出上下文向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"c"}})],1)],1)],1)],1)]),t._v(" "),a("p",[t._v("我们将进行这一系列计算的层称为 "),a("strong",[t._v("Attention 层")]),t._v("：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403083525528.png",alt:"image-20220403083525528"}}),t._v(" "),a("p",[t._v("以上就是 Attention 技术的核心内容：它关注编码器传递的信息 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 中的重要元素，基于它算出上下文向量，再传递给上一层。下面是 Attention 层的实现：")],1),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Attention")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weight_layer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AttentionWeight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight_sum_layer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" WeightSum"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        a "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weight_layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight_sum_layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" a\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("backward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        dhs0"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" da "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight_sum_layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        dhs1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weight_layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("da"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        dhs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dhs0 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" dhs1\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" dhs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dh\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br")])]),a("p",[t._v("我们将这个 Attention 层放在 LSTM 层和 Affine 层的中间：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403084318023.png",alt:"image-20220403084318023"}}),t._v(" "),a("p",[t._v("编码器的输出 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),t._v(" 被输入到各个时刻的 Attention 层。另外，这里将 LSTM 层的隐藏状态向量输入 Affine 层。如下图所示，与之前的网络结构相比，我们将基于 Attention 的上下文向量信息“添加”到了之前实现的解码器上：")],1),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403084502759.png",alt:"image-20220403084502759"}}),t._v(" "),a("blockquote",[a("p",[t._v("上下文向量和隐藏状态向量这两个向量被输入 Affine 层。如前所述，这意味着将这两个向量拼接起来，将拼接后的向量输入 Affine 层。")])]),t._v(" "),a("p",[t._v("最后，我们在时序方向上扩展多个 Attention 层整体实现为 Time Attention 层：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403085338283.png",alt:"image-20220403085338283"}}),t._v(" "),a("ul",[a("li",[t._v("Time Attention 层只是组合了多个 Attention 层")])]),t._v(" "),a("details",{staticClass:"custom-block details"},[a("summary",[t._v("Time Attention 的实现")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TimeAttention")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hs_enc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hs_dec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs_dec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("empty_like"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hs_dec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" t "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            layer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Attention"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hs_enc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hs_dec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention_weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("backward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n        dhs_enc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        dhs_dec "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("empty_like"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" t "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            layer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            dhs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            dhs_enc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" dhs\n            dhs_dec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dh\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" dhs_enc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dhs_dec\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br")])]),a("ul",[a("li",[t._v("这里仅创建必要数量的 Attention 层（代码中为 T 个），各自进行正向传播和反向传播")]),t._v(" "),a("li",[a("code",[t._v("attention_weights")]),t._v(" 列表中保存了各个 Attention 层对各个单词的权重")])])]),t._v(" "),a("p",[t._v("下面我们使用 Attention 来实现 seq2seq，并尝试挑战一个真实问题，以确认 Attention 的效果。")]),t._v(" "),a("h2",{attrs:{id:"_2-带-attention-的-seq2seq-的实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-带-attention-的-seq2seq-的实现"}},[t._v("#")]),t._v(" 2. 带 Attention 的 seq2seq 的实现")]),t._v(" "),a("p",[t._v("我们分别实现 3 个类：AttentionEncoder、AttentionDecoder 和 AttentionSeq2seq。")]),t._v(" "),a("h3",{attrs:{id:"_2-1-编码器-attentionencoder-的实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-编码器-attentionencoder-的实现"}},[t._v("#")]),t._v(" 2.1 编码器（AttentionEncoder）的实现")]),t._v(" "),a("p",[t._v("它与上一章的 Encoder 唯一的区别在于 Encoder 类的 forward 仅返回 LSTM 层的最后一个隐藏状态向量，而 AttentionEncoder 返回所有的隐藏状态向量：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AttentionEncoder")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\txs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lstm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" hs\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("h3",{attrs:{id:"_2-2-解码器-attentiondecoder-的实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-解码器-attentiondecoder-的实现"}},[t._v("#")]),t._v(" 2.2 解码器（AttentionDecoder）的实现")]),t._v(" "),a("p",[t._v("使用了 Attention 的解码器的层结构如下图所示：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403090855672.png",alt:"image-20220403090855672"}}),t._v(" "),a("p",[t._v("与之前一样，解码器还多了一个生成新单词序列的 "),a("code",[t._v("generate()")]),t._v(" 方法。")]),t._v(" "),a("p",[t._v("这里给出其核心实现：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AttentionDecoder")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size\n        rn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randn\n\n        embed_W "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        lstm_Wx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        lstm_Wh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        lstm_b "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        affine_W "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        affine_b "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TimeEmbedding"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_W"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lstm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TimeLSTM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lstm_Wx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lstm_Wh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lstm_b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stateful"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TimeAttention"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("affine "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TimeAffine"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("affine_W"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" affine_b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        layers "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lstm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("affine"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" layer "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" layer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" enc_hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        h "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" enc_hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lstm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_state"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        dec_hs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lstm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        c "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("enc_hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dec_hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concatenate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dec_hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        score "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("affine"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" score\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br"),a("span",{staticClass:"line-number"},[t._v("33")]),a("br"),a("span",{staticClass:"line-number"},[t._v("34")]),a("br")])]),a("p",[t._v("最后，我们使用 AttentionEncoder 类和 AttentionDecoder 类来实现 AttentionSeq2seq 类。")]),t._v(" "),a("h3",{attrs:{id:"_2-3-seq2seq-的实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-seq2seq-的实现"}},[t._v("#")]),t._v(" 2.3 seq2seq 的实现")]),t._v(" "),a("p",[t._v("AttentionSeq2seq 类的实现也和上一章实现的 seq2seq 几乎一样。区别仅在于，编码器使用 AttentionEncoder 类，解码器使用 AttentionDecoder 类。因此，只要继承上一章的 Seq2seq 类，并改一下初始化方法，就可以实现 AttentionSeq2seq 类：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br"),a("br"),a("br"),a("br"),a("br")]),a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AttentionSeq2seq")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Seq2seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        args "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AttentionEncoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AttentionDecoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TimeSoftmaxWithLoss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br")])]),a("h2",{attrs:{id:"_3-attenetion-的评价"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-attenetion-的评价"}},[t._v("#")]),t._v(" 3. Attenetion 的评价")]),t._v(" "),a("p",[t._v("我们通过研究“日期格式转换”问题来确认带 Attention 的 seq2seq 的效果：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403093356251.png",alt:"image-20220403093356251"}}),t._v(" "),a("blockquote",[a("p",[t._v("其实应该研究翻译问题来确认其效果，但没能找到合适的数据集。"),a("strong",[t._v("WMT")]),t._v(" 是一个有名的翻译数据集，在许多研究中都被作为基准使用，经常用于评价 seq2seq 的性能，不过它的数据量很大（超过 20 GB），使用起来不是很方便。")])]),t._v(" "),a("p",[t._v("采用该问题的原因：这个问题的输入形式较为复杂，所以手工编写转换规则也比较复杂。其次问句与回答之间存在明显对应关系，可以用于确认 Attention 有没有有正确地关注各自的对应元素。")]),t._v(" "),a("p",[t._v("我们的数据集："),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403093612685.png",alt:"image-20220403093612685"}})]),t._v(" "),a("p",[t._v("我们对输入语句通过填充空格来对齐。因为这个问题输出的字符数是恒定的，所以无须使用分隔符来指示输出的结束。")]),t._v(" "),a("h3",{attrs:{id:"_3-1-带-attention-的-seq2seq-的学习"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-带-attention-的-seq2seq-的学习"}},[t._v("#")]),t._v(" 3.1 带 Attention 的 seq2seq 的学习")]),t._v(" "),a("p",[t._v("我们在日期转换用的数据集上进行 AttentionSeq2seq 的学习，具体的学习代码可见鱼书的附带资源。")]),t._v(" "),a("p",[t._v("在学习数据的过程中还使用了反转输入语句的技巧，在每个 epoch 使用测试数据计算正确率。随着学习的进行，结果如图：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403094641348.png",alt:"image-20220403094641348"}}),t._v(" "),a("p",[t._v("随着学习的深入，带 Attention 的 seq2seq 变聪明了。实际上，没过多久，它就对大多数问题给出了正确答案：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403095011338.png",alt:"image-20220403095011338"}}),t._v(" "),a("p",[t._v("与之前的模型相比：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403095042615.png",alt:"image-20220403095042615"}}),t._v(" "),a("p",[t._v("在这次的实验中，就最终精度来看，Attention 和 Peeky 取得了差不多的结果。但是，随着时序数据变长、变复杂，除了学习速度之外，Attention 在精度上也会变得更有优势。")]),t._v(" "),a("h3",{attrs:{id:"_3-2-attention-的可视化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-attention-的可视化"}},[t._v("#")]),t._v(" 3.2 Attention 的可视化")]),t._v(" "),a("p",[t._v("在我们的实现中，Time Attention 层中的成员变量 attention_weights 保存了各个时刻的 Attention 权重，据此可以将输入语句和输出语句的各个单词的对应关系绘制成一张二维地图：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403100734306.png",alt:"image-20220403100734306"}}),t._v(" "),a("ul",[a("li",[t._v("我们可以看到，当 seq2seq 输出第 1 个“1”时，注意力集中在输入语句的“1”上")]),t._v(" "),a("li",[t._v("输入语句的“AUGUST”对应于表示月份的“08”，这表明 seq2seq 从数据中学习到了“August”和“8 月”的对应关系。")])]),t._v(" "),a("p",[t._v("像这样，使用 Attention，seq2seq 能像我们人一样将注意力集中在必要的信息上。")]),t._v(" "),a("blockquote",[a("p",[t._v("我们没有办法理解神经网络内部进行了什么工作（基于何种逻辑工作），而 Attention 赋予了模型“人类可以理解的结构和意义”。在上面的例子中，通过 Attention，我们看到了单词和单词之间的关联性。由此，我们可以判断模型的工作逻辑是否符合人类的逻辑。")])]),t._v(" "),a("p",[t._v("下一节我们继续围绕 Attention，介绍它的几个高级技巧。")]),t._v(" "),a("h2",{attrs:{id:"_4-关于-attention-的其他话题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-关于-attention-的其他话题"}},[t._v("#")]),t._v(" 4. 关于 Attention 的其他话题")]),t._v(" "),a("p",[t._v("我们研究了带 Attention 的 seq2seq，现在我们介绍几个之前未涉及的话题。")]),t._v(" "),a("h3",{attrs:{id:"_4-1-双向-rnn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-双向-rnn"}},[t._v("#")]),t._v(" 4.1 双向 RNN")]),t._v(" "),a("p",[t._v("这里我们关注 seq2seq 的编码器：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402222216049.png",alt:"image-20220402222216049"}}),t._v(" "),a("p",[t._v("这里我们是从左向右阅读句子的，因此单词“猫”的对应向量编码了“吾輩”“は”“猫”这 3 个单词的信息。如果考虑整体的平衡性，我们希望向量能更均衡地包含单词“猫”周围的信息。")]),t._v(" "),a("p",[t._v("为此，"),a("u",[t._v("可以让 LSTM 从两个方向进行处理，这就是名为"),a("strong",[t._v("双向 LSTM")]),t._v(" 的技术")]),t._v("：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403102331734.png",alt:"image-20220403102331734"}}),t._v(" "),a("ul",[a("li",[t._v("双向 LSTM 在之前的 LSTM 层上添加了一个反方向处理的 LSTM 层。然后，拼接各个时刻的两个 LSTM 层的隐藏状态，将其作为最后的隐藏状态向量（除了拼接之外，也可以“求和”或者“取平均”等）")])]),t._v(" "),a("p",[a("strong",[t._v("通过这样的双向处理，各个单词对应的隐藏状态向量可以从左右两个方向聚集信息。这样一来，这些向量就编码了更均衡的信息")]),t._v("。")]),t._v(" "),a("p",[t._v("双向 LSTM 的实现非常简单。一种实现方式是准备两个 LSTM 层（本章中是 Time LSTM 层），并调整输入各个层的单词的排列。具体而言，其中一个层的输入语句与之前相同，这相当于从左向右处理输入语句的常规的 LSTM 层。而"),a("strong",[t._v("另一个 LSTM 层的输入语句则按照从右到左的顺序输入")]),t._v("。如果原文是“A B C D”，就改为“D C B A”。通过输入改变了顺序的输入语句，另一个 LSTM 层从右向左处理输入语句。之后，只需要拼接这两个 LSTM 层的输出，就可以创建双向 LSTM 层。")]),t._v(" "),a("h3",{attrs:{id:"_4-2-attention-层的使用方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-attention-层的使用方法"}},[t._v("#")]),t._v(" 4.2 Attention 层的使用方法")]),t._v(" "),a("p",[t._v("之前我们将 Attention 层插入了 LSTM 层和 Affine 层之间：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403084318023.png",alt:"image-20220403084318023"}}),t._v(" "),a("p",[t._v("实际上，使用 Attention 的模型还有其他好几种方式。文献"),a("sup",[t._v("[48]")]),t._v("以下图的结构 使用了 Attention：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"C:\\Users\\yubin\\AppData\\Roaming\\Typora\\typora-user-images\\image-20220403103834178.png",alt:"image-20220403103834178"}}),t._v(" "),a("ul",[a("li",[t._v("Attention 层的输出（上下文向量）被连接到了下一时刻的 LSTM 层的输入处。通过这种结构，LSTM 层得以使用上下文向量的信息。相对地，我们实现的模型则是 Affine 层使用了上下文向量。")])]),t._v(" "),a("p",[a("strong",[t._v("Attention 层的位置的不同对最终精度有何影响呢？答案要试一下才知道")]),t._v("。实际上，这只能使用真实数据来验证。不过，在上面的两个模型中，上下文向量都得到了很好的应用。因此，在这两个模型之间，我们可能看不到太大的精度差异。")]),t._v(" "),a("h3",{attrs:{id:"_4-3-seq2seq-的深层化和-skip-connection"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-seq2seq-的深层化和-skip-connection"}},[t._v("#")]),t._v(" 4.3 seq2seq 的深层化和 skip connection")]),t._v(" "),a("p",[t._v("通过加深层，可以创建表现力更强的模型，带 Attention 的 seq2seq 也是如此。那么，如果我们加深带 Attention 的 seq2seq，结果会怎样呢？以下图为例：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403104141207.png",alt:"image-20220403104141207"}}),t._v(" "),a("ul",[a("li",[t._v("编码器和解码器使用了 3 层 LSTM 层")]),t._v(" "),a("li",[t._v("这里将解码器 LSTM 层的隐藏状态输入 Attention 层，然后将上下文向量（Attention 层的输出）传给解码器的多个层（LSTM 层和 Affine 层）")])]),t._v(" "),a("blockquote",[a("p",[t._v("如本例所示，"),a("strong",[t._v("编码器和解码器中通常使用层数相同的 LSTM 层")]),t._v("。")])]),t._v(" "),a("p",[t._v("另外，在加深层时使用到的另一个重要技巧是"),a("strong",[t._v("残差连接")]),t._v("（skip connection，也称为 residual connection 或 shortcut），这时一种跨层连接的简单技巧：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403104347206.png",alt:"image-20220403104347206"}}),t._v(" "),a("p",[t._v("所谓残差连接，就是指“跨层连接”，在残差连接的连接处，有两个输出被相加。因为加法在反向传播时“按原样”传播梯度，所以"),a("strong",[t._v("残差连接中的梯度可以不受任何影响地传播到前一个层")]),t._v("。这样一来，即便加深了层，梯度也能正常传播，而不会发生梯度消失（或者梯度爆炸），学习可以顺利进行。")]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("strong",[t._v("在时间方向上")]),t._v("，RNN 层的反向传播会出现梯度消失或梯度爆炸的问题。梯度消失可以通过 LSTM、GRU 等 Gated RNN 应对，梯度爆炸可以通过梯度裁剪应对。")]),t._v(" "),a("li",[t._v("而"),a("strong",[t._v("在深度方向上")]),t._v("的梯度消失，这里介绍的残差连接很有效。")])])]),t._v(" "),a("h2",{attrs:{id:"_5-attention-的应用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-attention-的应用"}},[t._v("#")]),t._v(" 5. Attention 的应用")]),t._v(" "),a("p",[t._v("到目前为止，我们仅将 Attention 应用在了 seq2seq 上，但是 Attention 这一想法本身是通用的。本节我们将介绍 3 个使用了 Attention 的前沿研究。")]),t._v(" "),a("h3",{attrs:{id:"_5-1-gnmt"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-gnmt"}},[t._v("#")]),t._v(" 5.1 GNMT")]),t._v(" "),a("p",[t._v("回看机器翻译的历史，我们可以发现主流方法随着时代的变迁而演变。从“基于规则的翻译”到“基于用例的翻译”，再到“基于统计的翻译”。现在，"),a("strong",[t._v("神经机器翻译")]),t._v("（Neural Machine Translation）取代了这些过往的技术，获得了广泛关注。")]),t._v(" "),a("blockquote",[a("p",[t._v("神经机器翻译现在已经成为使用了 seq2seq 的机器翻译的统称。")])]),t._v(" "),a("p",[t._v("谷歌推出的 GNMT（Google Neural Machine Translation）也是由由编码器、解码器和 Attention 构成，还有许多为了提高翻译精度而做的改进。除此以外，还进行了低频词处理、用于加速推理的量化等工作，从而得到了非常好的结果。")]),t._v(" "),a("h3",{attrs:{id:"_5-2-transformer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-transformer"}},[t._v("#")]),t._v(" 5.2 Transformer")]),t._v(" "),a("p",[t._v("使用 RNN 可以很好地处理可变长度的时序数据，但"),a("strong",[t._v("存在并行处理的问题")]),t._v("。RNN 需要基于上一个时刻的计算结果逐步进行计算，导致了无法在时间方向上并行计算，这会成为一个很大的瓶颈。")]),t._v(" "),a("p",[t._v("现在关于去除 RNN 的研究（可以并行计算的 RNN 的研究）很活跃，其中一个著名的模型是 Transformer 模型。Transformer 是在“Attention is all you need”这篇论文中提出来的方法。如论文标题所示，"),a("strong",[t._v("Transformer 不用 RNN，而用 Attention 进行处理")]),t._v("。")]),t._v(" "),a("blockquote",[a("p",[t._v("除此之外，还有研究用 CNN 代替 RNN 来实现并行计算。")])]),t._v(" "),a("p",[t._v("Transformer 是基于 Attention 构成的，其中使用了 Self-Attention 技巧，这一点很重要。"),a("u",[a("strong",[t._v("Self-Attention")]),t._v(" 是以一个时序数据为对象的 Attention，旨在观察一个时序数据中每个元素与其他元素的关系")]),t._v("：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403110408187.png",alt:"image-20220403110408187"}}),t._v(" "),a("p",[t._v("上面左图的 Time Attention 层的两个输入中输入的是不同的时序数据，而右图的 Self-Attention 的两个输入中输入的是同一个时序数据，这样可以求得一个时序数据内各个元素之间的对应关系。")]),t._v(" "),a("p",[t._v("至此，对 Self-Attention 的说明就结束了，下面我们看一下 Transformer 的层结构：")]),t._v(" "),a("img",{attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403114543780.png",alt:"image-20220403114543780"}}),t._v(" "),a("ul",[a("li",[t._v("Transformer 中用 Attention 代替了 RNN，编码器和解码器两者都使用了 Self-Attention")]),t._v(" "),a("li",[t._v("Feed Forward 层表示前馈神经网络（在时间方向上独立的网络）")]),t._v(" "),a("li",[t._v("图中的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"N"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"x"}})],1)],1)],1)],1)],1),t._v(" 表示灰色背景包围的元素被堆叠了 N 次")],1),t._v(" "),a("li",[t._v("这个图是简化的 Transformer，实际上，Skip Connection、Layer Normalization 等技巧也会被用到。")])]),t._v(" "),a("p",[t._v("使用 Transformer 可以控制计算量，充分利用 GPU 并行计算带来的好处，使得学习时间得以大幅减少。在翻译精度方面也实现了精度的提升。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403124532511.png",alt:"image-20220403124532511"}}),t._v(" "),a("p",[t._v("由这个研究可知，"),a("strong",[t._v("Attention 其实可以用来替换 RNN")]),t._v("。这样一来，利用 Attention 的机会可能会进一步增加。")]),t._v(" "),a("h3",{attrs:{id:"_5-3-ntm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-3-ntm"}},[t._v("#")]),t._v(" 5.3 NTM")]),t._v(" "),a("p",[t._v("可见计算机的内存操作可以通过神经网络复现。我们可以立刻想到一个方法：在 RNN 的外部配置一个存储信息的存储装置，并使用 Attention 向这个存储装置读写必要的信息。实际上，这样的研究有好几个，NTM （Neural Turing Machine，神经图灵机）"),a("sup",[t._v("[55]")]),t._v(" 就是其中比较有名的一个。")]),t._v(" "),a("p",[t._v("基于外部存储装置的扩展技术和 Attention 会越来越重要，今后将被应用在各种地方。")]),t._v(" "),a("p",[t._v("本部分不再展开，内容可参考鱼书或其他资料。")]),t._v(" "),a("h2",{attrs:{id:"参考文献"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文献"}},[t._v("#")]),t._v(" 参考文献")]),t._v(" "),a("details",{staticClass:"custom-block details"},[a("summary",[t._v("文献引用")]),t._v(" "),a("p",[t._v("[48] Bahdanau, Dzmitry, Kyunghyun Cho, Yoshua Bengio：Neural  machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.")]),t._v(" "),a("p",[t._v("[49] Luong, Minh-Thang, Hieu Pham, Christopher D. Manning.Effective  approaches to attention-based neural machine translation[J]. arXiv  prelprint arXiv:1508.04025, 2015.")]),t._v(" "),a("p",[t._v("[55] Graves, Alex, Greg Wayne, Ivo Danihelka,Neural turing machines[J].  arXiv preprint arXiv:1410.5401, 2014.")])])])}),[],!1,null,null,null);s.default=e.exports}}]);