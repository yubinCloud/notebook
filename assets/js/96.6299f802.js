(window.webpackJsonp=window.webpackJsonp||[]).push([[96],{850:function(t,a,s){"use strict";s.r(a);var i=s(22),m=Object(i.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("Meta Learning: Learn to learn（学习如何学习）")]),t._v(" "),s("blockquote",[s("p",[t._v("What does “meta” mean? meta-X = X about X")])]),t._v(" "),s("p",[t._v("Can machine automatically determine the hyper-parameters?")]),t._v(" "),s("h2",{attrs:{id:"_1-回顾-machine-learning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-回顾-machine-learning"}},[t._v("#")]),t._v(" 1. 回顾 Machine Learning")]),t._v(" "),s("p",[t._v("Machine Learning = Looking for a function")]),t._v(" "),s("p",[s("strong",[t._v("step 1: Function with unknown")])]),t._v(" "),s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001092728938.png",alt:"image-20221001092728938"}}),t._v(" "),s("p",[s("strong",[t._v("step 2: Define loss function")])]),t._v(" "),s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001092901925.png",alt:"image-20221001092901925"}}),t._v(" "),s("p",[s("strong",[t._v("step 3: Optimization")])]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001093245180.png",alt:"image-20221001093245180"}})]),t._v(" "),s("h2",{attrs:{id:"_2-introduction-of-meta-learning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-introduction-of-meta-learning"}},[t._v("#")]),t._v(" 2. Introduction of Meta Learning")]),t._v(" "),s("h3",{attrs:{id:"_2-1-what-is-meta-learning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-what-is-meta-learning"}},[t._v("#")]),t._v(" 2.1 What is Meta Learning?")]),t._v(" "),s("p",[t._v("其实“学习”这件事，它本身也是一个 function F：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001094058012.png",alt:"image-20221001094058012"}})]),t._v(" "),s("p",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"F"}})],1)],1)],1),t._v(" 的 input 是 training dataset，output 是一个 model。在 typical ML 中，这里的 learning algorithm "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"F"}})],1)],1)],1),t._v(" 是 hand-crafted 的，而 "),s("mark",[t._v("Meta Learning")]),t._v(" 就是研究 “Can we learn this function "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"F"}})],1)],1)],1),t._v("?”")],1),t._v(" "),s("p",[t._v("怎么找这个 F 呢？Following the same three steps in ML!")]),t._v(" "),s("h3",{attrs:{id:"_2-2-meta-learning-step-1"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-meta-learning-step-1"}},[t._v("#")]),t._v(" 2.2 Meta Learning - Step 1")]),t._v(" "),s("p",[t._v("What is "),s("strong",[t._v("learnable")]),t._v(" in a learning algorithm?")]),t._v(" "),s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001094947175.png",alt:"image-20221001094947175"}}),t._v(" "),s("p",[t._v("之前我们将 model 中 learnable parameters 记作 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v("，这里我们将 meta learning 中 learnable components 记作 "),s("mark",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1),t._v("，相对应的 F 就记为 "),s("mark",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1)],1),t._v("。")],1),t._v(" "),s("p",[t._v("根据什么是 learnable components "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v("，可以将 meta learning 分成多个种类。")],1),t._v(" "),s("h3",{attrs:{id:"_2-3-meta-learning-step-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-meta-learning-step-2"}},[t._v("#")]),t._v(" 2.3 Meta Learning - Step 2")]),t._v(" "),s("p",[t._v("这一步我们需要 Define loss function "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" for learning algorithm "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1),t._v("。这样如果 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 小的话说明 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1),t._v(" 是一个好的 learning algorithm，反之则是不好的 learning algorithm。")],1),t._v(" "),s("p",[t._v("How to define "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("? 什么情况下 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 应该小呢？如下图所示：")],1),t._v(" "),s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001193549307.png",alt:"image-20221001193549307"}}),t._v(" "),s("p",[t._v("那我们怎样知道一个 classifier 是好还是坏呢？我们可以 Evaluate the classifier on testing set。图示如下：")]),t._v(" "),s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001194031345.png",alt:"image-20221001194031345"}}),t._v(" "),s("p",[t._v("这里怎样计算 loss "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"l"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v(" 呢？计算方式和之前的 machine learning 是差不多的，图示如下：")],1),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001194833091.png",alt:"image-20221001194833091"}}),t._v(" "),s("p",[t._v("这里在测试资料上计算出来的 loss "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"l"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v(" 越小，就代表我们训练出来的 classifier 越好，这样也就代表我们的 learning algorithm 越好。反之亦然。")],1),t._v(" "),s("p",[t._v("注意这里的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1),t._v(" 是一个能够学出 binary classifier 的 learning algorithm，我们上面是在一个 apple 与 orange 的 binary classification 的 task 来评价它的，"),s("strong",[t._v("但在 meta learning 中，我们不会只拿一个 task 来评价一个 binary classifier learning 的 algorithm "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1)],1),t._v("，而是还会拿其他的 binary classification 的 task 来评价。如下图所示：")],1),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001195852322.png",alt:"image-20221001195852322"}}),t._v(" "),s("p",[t._v("在这里，左右两个 learning algorithm "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1),t._v(" 是一样的，但 input 的资料不一样，那 output 的 binary classifier 也不一样。把所有 task 上得到的 loss 加起来，就得到了最终的 total loss "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("。")],1),t._v(" "),s("blockquote",[s("p",[t._v("这里的举例只讲了两个 task，但实际上你会有非常多的 task，那么 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-munderover",{attrs:{space:"4",limits:"false"}},[s("mjx-mo",{staticClass:"mjx-sop"},[s("mjx-c",{attrs:{c:"2211"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.285em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"N"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.291em"}}),s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1),s("mjx-msup",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"l"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1)],1)]),t._v(" "),s("p",[t._v("到了这里，有一点你可能觉得怪怪的。在 typical ML 中，你是用 training examples 去计算 loss，"),s("strong",[t._v("但在 meta learning 中，你却是用 training tasks 的 testing example 来计算的 loss")]),t._v("。这一点 meta learning 与 typical ML 是不同的，在做 meta learning 时，我们是拿 task 作为的训练单位，所以你是可以将 training tasks 的 testing example 用于训练过程中。在之后讲完 meta learning 的整个流程，你会更加清晰。")]),t._v(" "),s("h3",{attrs:{id:"_2-4-meta-learning-step-3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-meta-learning-step-3"}},[t._v("#")]),t._v(" 2.4 Meta Learning - Step 3")]),t._v(" "),s("p",[t._v("我们已经有了 learning algorithm 的 loss function "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-munderover",{attrs:{space:"4",limits:"false"}},[s("mjx-mo",{staticClass:"mjx-sop"},[s("mjx-c",{attrs:{c:"2211"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.285em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"N"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.291em"}}),s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1),s("mjx-msup",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"l"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("，现在我们要做的是寻找到能够 minimize "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v("，即 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mstyle",{staticStyle:{color:"blue"}},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mi",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"a"}}),s("mjx-c",{attrs:{c:"r"}}),s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2061"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"m"}}),s("mjx-c",{attrs:{c:"i"}}),s("mjx-c",{attrs:{c:"n"}})],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("。")],1),t._v(" "),s("p",[t._v("怎么做呢？Using the optimization approach you know:")]),t._v(" "),s("ul",[s("li",[t._v("If you know how to compute "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mfrac",[s("mjx-frac",[s("mjx-num",[s("mjx-nstrut"),s("mjx-mrow",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2202"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),s("mjx-dbox",[s("mjx-dtable",[s("mjx-line"),s("mjx-row",[s("mjx-den",[s("mjx-dstrut"),s("mjx-mrow",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2202"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1)],1)],1)],1)],1)],1),t._v(" => "),s("em",[t._v("Gradient descent is your friend.")])],1),t._v(" "),s("li",[t._v("What if "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" is not differentiable? => "),s("em",[t._v("用 RL / Evalutionary Algorithm 硬 train 一发")])],1)]),t._v(" "),s("p",[t._v("反正不管你用什么方法，你最终可以学习出一个 “learning algorithm” "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mstyle",{staticStyle:{color:"blue"}},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1)],1)],1)],1)],1)],1),t._v(" "),s("p",[t._v("整个 meta learning 的 framework 如下图所示：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"74%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001202936086.png",alt:"image-20221001202936086"}})]),t._v(" "),s("ul",[s("li",[t._v("我们先从 training task 中学习出一个 learned “learning algorithm” "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1)],1)],1)],1),t._v("，然后给他输入 testing task 中的 training examples，得到一个 classifier "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1)],1)],1)],1),t._v("，这时再把这个 classifier 用在 testing task 的 testing examples 里面得到我们想要的结果。")],1),t._v(" "),s("li",[t._v("在这个过程中，testing task 是我们真正关心的 task，而 training tasks 是与 testing task 无关的 tasks，这些 training tasks 就是用来寻找出 learned 的演算法 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1)],1)],1)],1),t._v("。")],1)]),t._v(" "),s("p",[t._v("像这种学习的演算法厉害在哪里呢？一种东西叫做”few-shot learning“，即”小样本学习“，它期待机器只看几个 training example 就可以让 model 学会做 classification。"),s("strong",[t._v("而在这里 meta learning 中，testing task 就只需要 little labeled training data 就可以")]),t._v("。")]),t._v(" "),s("p",[s("strong",[t._v("通常想要实现 few-shot learning，这种演算法是人类难以想象出来的，往往需要 meta learning 来把这个演算法给找出来")]),t._v("。所以注意区分好 few-shot learning 与 meta learning 的微妙区别。")]),t._v(" "),s("blockquote",[s("p",[t._v("在 meta learning 中，单说”training data“是很容易造成误解的，所以使用时要小心。在很多 paper 中就很不讲究，很多说的 training data 很容易导致误解。")])]),t._v(" "),s("h3",{attrs:{id:"_2-5-ml-v-s-meta"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-ml-v-s-meta"}},[t._v("#")]),t._v(" 2.5 ML v.s. Meta")]),t._v(" "),s("h4",{attrs:{id:"_2-5-1-goal"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-1-goal"}},[t._v("#")]),t._v(" 2.5.1 Goal")]),t._v(" "),s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001210356592.png",alt:"image-20221001210356592"}}),t._v(" "),s("h4",{attrs:{id:"_2-5-2-training-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-2-training-data"}},[t._v("#")]),t._v(" 2.5.2 Training Data")]),t._v(" "),s("ul",[s("li",[t._v("在 machine learning 里面，我们是拿一个 task 的 training set 来进行训练；")]),t._v(" "),s("li",[t._v("在 meta learning 里面，我们是拿“task”来进行训练，也就是用 training tasks 来进行训练，在每一个 training task 里面，都有 training data 和 testing data。")])]),t._v(" "),s("p",[t._v("为了避免对“训练资料”这个说法产生歧义，在 meta learning 中，一个 training task 里面的 training examples 称为 "),s("mark",[t._v("Support set")]),t._v("，testing examples 称为 "),s("mark",[t._v("Query set")]),t._v("。在一些文献中就是这么叫的。")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001225153142.png",alt:"image-20221001225153142"}}),t._v(" "),s("h4",{attrs:{id:"_2-5-3-training"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-3-training"}},[t._v("#")]),t._v(" 2.5.3 Training")]),t._v(" "),s("ul",[s("li",[t._v("在 machine learning 中，我们是有一个 hand-crafted 的 learning algorithm，然后把训练资料丢进去，得到一个训练的 classifier "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mstyle",{staticStyle:{color:"green"}},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1)],1)],1)],1)],1)],1),t._v(" "),s("li",[t._v("在 meta learning 中，我们是有一堆 training tasks，然后我们是要用这一堆 training tasks 去得到一个 learned “learning algorithm” "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"F"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mstyle",{staticStyle:{color:"blue"}},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)]),t._v(" "),s("p",[t._v("这里，把 meta learning 的这种 involve 一大堆 tasks 的训练叫做 "),s("mark",[t._v("Across-task Training")])]),t._v(" "),s("p",[t._v("；而把一般 ML 的训练叫做 "),s("mark",[t._v("Within-task Training")]),t._v("。这样就可以区别两种 training 的过程了：")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001232645243.png",alt:"image-20221001232645243"}}),t._v(" "),s("h4",{attrs:{id:"_2-5-4-testing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-4-testing"}},[t._v("#")]),t._v(" 2.5.4 Testing")]),t._v(" "),s("p",[t._v("我们把 meta learning 中这个 testing 过程叫做 "),s("mark",[t._v("Across-task Testing")]),t._v("，而一般 ML 的 testing 过程叫做 "),s("mark",[t._v("Within-task Testing")]),t._v("，图示如下：")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001234414544.png",alt:"image-20221001234414544"}}),t._v(" "),s("p",[t._v("在 meta learning 中，我们要测试的不是一个 classifier 的好坏，而是一个 learning algorithm 的好坏，因此"),s("strong",[t._v("在一个 Across-task Testing 中，还包含了 Within-task Training  和 Within-task Testing 过程")]),t._v("。在有些文献中，一次 Within-task Training 加一次 Within-task Testing 的流程叫做一个 "),s("mark",[t._v("Episode")]),t._v("。")]),t._v(" "),s("h4",{attrs:{id:"_2-5-5-loss"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-5-loss"}},[t._v("#")]),t._v(" 2.5.5 Loss")]),t._v(" "),s("p",[t._v("typical ML 与 meta learning 的 loss 计算方式也不一样：")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001235018279.png",alt:"image-20221001235018279"}}),t._v(" "),s("ul",[s("li",[t._v("ML 的 L 是从一个 task 中算出来的；")]),t._v(" "),s("li",[t._v("Meta learning 的 L 是从一把 tasks 中算出来的。")])]),t._v(" "),s("p",[t._v("我们单独看一下 meta learning 中 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 的计算：")],1),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221001235645349.png",alt:"image-20221001235645349"}}),t._v(" "),s("ul",[s("li",[t._v("在进行一次 Across-task training 计算 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 时，需要计算多个 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"l"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("，而每一个 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"l"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v(" 的计算都需要经过一个 Within-task training + Within-task testing 的 Episode，计算量还是很大的。")],1),t._v(" "),s("li",[t._v("在“Learning to initialize”系列的 paper 中，也称 Across-task training 叫做 "),s("strong",[t._v("Outer Loop")]),t._v("，称 Within-task training 叫做 "),s("strong",[t._v("Inner Loop")]),t._v("。")])]),t._v(" "),s("h4",{attrs:{id:"_2-5-6-两者的相似点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-6-两者的相似点"}},[t._v("#")]),t._v(" 2.5.6 两者的相似点")]),t._v(" "),s("p",[t._v("What you know about ML can usually apply to meta learning:")]),t._v(" "),s("ul",[s("li",[t._v("Overfitting on training tasks")]),t._v(" "),s("li",[t._v("Get more training tasks to improve performance")]),t._v(" "),s("li",[t._v("Task augmentation")]),t._v(" "),s("li",[t._v("There are also hyperparameters when learning a learning algorithm ...... （所以做 meta learning 也是需要暴调一波参数的）")]),t._v(" "),s("li",[t._v("Development task（这类似于 ML 的 validation set 用来调 hyper-parameters，在很多文献里并没有这种 task 而是只有 training tasks 和 testing task，但李老师认为应该有）")])]),t._v(" "),s("blockquote",[s("p",[t._v("有没有可能套娃？这是个梗啦，以后可能有人会提出 meta meta learning ......")])]),t._v(" "),s("h2",{attrs:{id:"_3-what-is-learnable-in-a-learning-algorithm"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-what-is-learnable-in-a-learning-algorithm"}},[t._v("#")]),t._v(" 3. What is learnable in a learning algorithm?")]),t._v(" "),s("h3",{attrs:{id:"_3-1-review-gradient-descent"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-review-gradient-descent"}},[t._v("#")]),t._v(" 3.1 Review: Gradient Descent")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002104525213.png",alt:"image-20221002104525213"}}),t._v(" "),s("p",[t._v("首先这里的 initial parameters "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1)],1)],1),t._v(" 是 learnable 的，而且我们知道好的 initial parameters 对训练有很大作用。那我们在 meta learning 中能不能可以透过一些 learning tasks 找出一些对训练特别有帮助的 initial parameters  "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1)],1)],1),t._v(" 呢？可以！下面介绍。")],1),t._v(" "),s("h3",{attrs:{id:"_3-2-learning-to-initialize"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-learning-to-initialize"}},[t._v("#")]),t._v(" 3.2 Learning to initialize")]),t._v(" "),s("p",[t._v("回答上面问题的最著名的方法就是 "),s("mark",[t._v("MAML")]),t._v("：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002152954312.png",alt:"image-20221002152954312"}})]),t._v(" "),s("ul",[s("li",[t._v("这里不再细讲这些模型的细节，可以参考原论文，作业题会问。")])]),t._v(" "),s("h3",{attrs:{id:"_3-3-how-to-train-your-maml"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-how-to-train-your-maml"}},[t._v("#")]),t._v(" 3.3 How to train your MAML?")]),t._v(" "),s("p",[t._v("正如之前讲的，我们做 meta learning 时也有很多需要调的 hyper-parameters，所以在用 MAML 时，虽然你是去 learn 一个 initialize 的 parameters，但这个 learn 的过程也有很多 hyper-parameters 需要你去决定。其实最开始的 MAML 不是那么好 train 的，于是就有人发了 paper 叫 “How to train your MAML”，他用了三种 MAML random seed 来 train，发现有两次是 train 不起来的，于是这篇 peper 就新提出了一种方法叫做 "),s("strong",[t._v("MAML++")]),t._v(":")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002153741780.png",alt:"image-20221002153741780"}})]),t._v(" "),s("p",[t._v("具体的细节需要再去读原文章。")]),t._v(" "),s("h3",{attrs:{id:"_3-4-maml-v-s-pre-training"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-maml-v-s-pre-training"}},[t._v("#")]),t._v(" 3.4 MAML v.s. Pre-training")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002154231857.png",alt:"image-20221002154231857"}})]),t._v(" "),s("p",[t._v("最直观的区别可能就是，MAML 中训练所用的数据是 labeled 的，而 Pre-training 中则是没有 label 的。")]),t._v(" "),s("p",[t._v("其实这里说的 Pre-training 是近期的做法，在更早之前，人们是把来自不同 tasks 的 data 统统放在一起里面训练一个 model，来得到一个好的初始化参数，这现在也被叫做 "),s("mark",[t._v("multi-task learning")]),t._v("，这种 multi-task learning 往往被用作 meta learning 的 baseline。具体了解它与 MAML 的差别可以参考李老师的相关视频（链接就不放在这里了）。")]),t._v(" "),s("p",[t._v("在 machine learning 中，对 domain 和 task 的界限并没有那么清晰，你也可以说不同的 task 就是不同的 domain，那这 meta learning 其实也很像 domain adaptation 了。")]),t._v(" "),s("blockquote",[s("p",[t._v("我们读文献时不要太拘泥于这些，关键的是这个词汇背后所代表的含义。")])]),t._v(" "),s("h3",{attrs:{id:"_3-5-maml-is-good-because"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-maml-is-good-because"}},[t._v("#")]),t._v(" 3.5 MAML is good because …")]),t._v(" "),s("p",[t._v("MAML 为什么好呢？这里有两个假设：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002170632725.png",alt:"image-20221002170632725"}})]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("Rapid Learning")]),t._v(" 的假设是说，MAML 找到的 initial parameter 很厉害，它可以让我们 gradient descent 这种 learning algorithm 快速地找到每一个 task 上好的参数")]),t._v(" "),s("li",[s("strong",[t._v("Feature Reuse")]),t._v(" 的假设是说，这个 initial parameter 本来就跟每一个 task 上最终好的结果已经非常接近了")])]),t._v(" "),s("p",[t._v("有一篇 paper 是 "),s("a",{attrs:{href:"https://arxiv.org/abs/1909.09157",target:"_blank",rel:"noopener noreferrer"}},[t._v("Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML"),s("OutboundLink")],1),t._v("，它告诉我们 feature reuse 来是 MAML 好的关键，同时提出了另外一种 MAML 的变形 "),s("strong",[t._v("ANIL")]),t._v("(Almost No Inner Loop)。")]),t._v(" "),s("p",[t._v("MAML 有很多变形和相关资料，More about MAML：")]),t._v(" "),s("ul",[s("li",[t._v("More mathematical details behind MAML：https://youtu.be/mxqzGwp_Qys")]),t._v(" "),s("li",[t._v("First order MAML (FOMAML)： https://youtu.be/3z997JhL9Oo")]),t._v(" "),s("li",[t._v("Reptile：https://youtu.be/9jJe2AD35P8")])]),t._v(" "),s("h3",{attrs:{id:"_3-6-学习-optimizer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-学习-optimizer"}},[t._v("#")]),t._v(" 3.6 学习 Optimizer")]),t._v(" "),s("p",[t._v("刚刚讲了说我们可以学习初始化的参数，我们还可以学习 Optimizer，在 update 参数的时候，我们需要决定 learning rate、momentum 等等 hyper-parameter，那像这种 hyper-parameter 能不能用学习的方式把它用 meta learning 学习出来呢？这是可以的。")]),t._v(" "),s("p",[t._v("NIPS 2016 有一篇 paper “Learning to learn by gradient descent by gradient descent”，这篇 paper 里面就是直接 learn 一个 optimizer，是自动根据 learning tasks 学出来的。具体可以参考该 paper。")]),t._v(" "),s("h3",{attrs:{id:"_3-7-network-architecture-search-nas"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-network-architecture-search-nas"}},[t._v("#")]),t._v(" 3.7 Network Architecture Search（NAS）")]),t._v(" "),s("p",[t._v("除了可以训练 initial parameters、optimizer，能不能也训练 Network Architecture 呢？这就是需要将 Network Architecture 当作 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v("：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002214248148.png",alt:"image-20221002214248148"}})]),t._v(" "),s("p",[t._v("研究训练 Network Architecture 的一系列研究就是鼎鼎大名的 "),s("mark",[t._v("Network Architecture Search")]),t._v("（"),s("strong",[t._v("NAS")]),t._v("）。")]),t._v(" "),s("p",[t._v("在 NAS 里面，既然 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v(" 是 Network Architecture，那我们的目标是 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mstyle",{staticStyle:{color:"red"}},[s("mjx-TeXAtom",[s("mjx-mover",[s("mjx-over",{staticStyle:{"padding-bottom":"0.06em","padding-left":"0.131em","margin-bottom":"-0.531em"}},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"^"}})],1)],1),s("mjx-base",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mi",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"a"}}),s("mjx-c",{attrs:{c:"r"}}),s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2061"}})],1),s("mjx-munder",{attrs:{space:"2",limits:"false"}},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"m"}}),s("mjx-c",{attrs:{c:"i"}}),s("mjx-c",{attrs:{c:"n"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("，但由于 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v(" 是 Network Architecture，那显然对 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v(" 做微分 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-n",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"2207"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 就有问题了。怎么办呢？记得我们说过，"),s("strong",[t._v("当你遇到 Optimization 没法算微分的时候，用 RL 硬 train 一发")]),t._v("！")],1),t._v(" "),s("p",[t._v("用 RL 怎么做呢？An agent uses a set of actions to determine the network architecture:")]),t._v(" "),s("ul",[s("li",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mstyle",{staticStyle:{color:"blue"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1)],1),t._v("：the agent’s parameters")],1),t._v(" "),s("li",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"L"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mstyle",{staticStyle:{color:"blue"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(": Reward to be maximized")],1)]),t._v(" "),s("p",[t._v("用 RL 做 NAS 的一个早期 work 的图示如下：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002220422808.png",alt:"image-20221002220422808"}})]),t._v(" "),s("ul",[s("li",[t._v("这里是把 agent 想成一个 RNN，每次会输出一个跟 Network Architecture 有关的参数。")])]),t._v(" "),s("blockquote",[s("p",[t._v("RL 解 NAS 问题的相关文献：")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1611.01578",target:"_blank",rel:"noopener noreferrer"}},[t._v("Neural Architecture Search with Reinforcement Learning"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning Transferable Architectures for Scalable Image Recognition"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"http://proceedings.mlr.press/v80/pham18a.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Efficient Neural Architecture Search via Parameters Sharing"),s("OutboundLink")],1)])]),t._v(" "),s("p",[t._v("Evolution Algorithm 解 NAS 问题的相关文献：")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"http://proceedings.mlr.press/v70/real17a.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Large-Scale Evolution of Image Classifiers"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://ojs.aaai.org/index.php/AAAI/article/view/4405",target:"_blank",rel:"noopener noreferrer"}},[t._v("Regularized Evolution for Image Classifier Architecture Search"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://arxiv.org/abs/1711.00436",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hierarchical Representations for Efficient Architecture Search"),s("OutboundLink")],1)])])]),t._v(" "),s("p",[t._v("其实，如果你硬要把 Network Architecture "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v(" 改一下，让它变得可以微分，也是可以的，有一个经典的做法 "),s("mark",[t._v("DARTS")]),t._v("，让 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v(" 变得可以微分从而可以用 gradient descent 来做 optimization：（"),s("a",{attrs:{href:"https://arxiv.org/abs/1806.09055",target:"_blank",rel:"noopener noreferrer"}},[t._v("DARTS: Differentiable Architecture Search"),s("OutboundLink")],1),t._v("）")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"73%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002221750679.png",alt:"image-20221002221750679"}})]),t._v(" "),s("h3",{attrs:{id:"_3-8-data-processing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-8-data-processing"}},[t._v("#")]),t._v(" 3.8 Data Processing")]),t._v(" "),s("p",[t._v("除了 Network Architecture 以外，还有 Data Process 也是可以 learn 的。比如能不能让 machine 自动找出怎样做 Data Augmentation 呢？这也是有可能的，可以参考如下文献：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002222402986.png",alt:"image-20221002222402986"}})]),t._v(" "),s("h3",{attrs:{id:"_3-9-sample-reweighting"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-9-sample-reweighting"}},[t._v("#")]),t._v(" 3.9 Sample Reweighting")]),t._v(" "),s("p",[t._v("有时候 training 的时候，我们需要给不同的 sample 以不同的 weight，但是要怎么给每一笔 data 不同的权重呢？比如对于离 boundary 近的 sample，有人觉得这些比较难的 sample 应该给一个 larger weights，有人觉得这些 noisy sample 应该给一个 smaller weights。这里我们期望说，可以让 machine 学到根据 data 的特性自动决定说 sample 的 weights 应该怎么设计：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002223007241.png",alt:"image-20221002223007241"}})]),t._v(" "),s("ul",[s("li",[t._v("上图的 sample weighting strategy 是 learnable 的。")])]),t._v(" "),s("h3",{attrs:{id:"_3-10-beyond-gradient-descent"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-10-beyond-gradient-descent"}},[t._v("#")]),t._v(" 3.10 Beyond Gradient Descent")]),t._v(" "),s("p",[t._v("我们刚刚讲的方法都是在围绕着 gradient descent 来做改进，刚才所有方法都是 learn 了一个 gradient descent 的其中一个 component。但我们有没有可能完全舍弃掉 gradient descent 呢？比如说我们就直接 learn 一个 network，这个 network 的 parameters 是 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1)],1)],1),t._v("，输入训练资料，直接输出训练好的结果 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2217"}})],1)],1)],1)],1)],1),t._v("，如果真的这样，那可以说机器发明了新的 learning algorithm 而抛弃了 gradient descent！有可能做到这件事嘛？也不是完全没有可能的，已经有一些论文往这个方向进展：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002224414971.png",alt:"image-20221002224414971"}})]),t._v(" "),s("h3",{attrs:{id:"_3-11-learning-to-compare-metric-based-approach"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-11-learning-to-compare-metric-based-approach"}},[t._v("#")]),t._v(" 3.11 Learning to compare（metric-based approach）")]),t._v(" "),s("p",[t._v("到目前为止，我们都是将训练和测试分成两个阶段：先拿 training data 训练出一个 learning algorithm（function "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"F"}})],1)],1)],1),t._v("），然后输出训练好的结果，再把训练好的结果用到测试资料上。但有没有可能更进一步，把整个 episode（一次 training + 一次 testing）包在一个 network 里面呢？这是有可能的，有一个系列的做法就是直接把 training data 和 testing data 当做 network 的 input：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002225045232.png",alt:"image-20221002225045232"}})]),t._v(" "),s("ul",[s("li",[t._v("这种就是先把 training data 看完后，也不知道里面发生了什么，再直接把 testing data 输入进去。")])]),t._v(" "),s("p",[t._v("有一个系列的 meta learning 的做法叫做 "),s("mark",[t._v("Learning to compare")]),t._v("，又叫做 "),s("mark",[t._v("metric-based approach")]),t._v("，这一系列的做法就可以看做是训练和测试没有分界。具体可以参考过去上课的录影：")]),t._v(" "),s("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002225345826.png",alt:"image-20221002225345826"}}),t._v(" "),s("h2",{attrs:{id:"_4-application"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-application"}},[t._v("#")]),t._v(" 4. Application")]),t._v(" "),s("h3",{attrs:{id:"_4-1-few-shot-image-classification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-few-shot-image-classification"}},[t._v("#")]),t._v(" 4.1 Few-shot Image Classification")]),t._v(" "),s("p",[t._v("今天在做 meta learning 时，最常拿来测试 meta learning 技术的任务是 "),s("strong",[t._v("Few-shot Image Classification")]),t._v("。在这个任务里，每个 class 都只有很少的几张 image，你希望透过这样一点点的资料，就可以训练出一个 model，你给 model 一张 image，它就能告诉你这张 image 属于哪个 class：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002232856764.png",alt:"image-20221002232856764"}})]),t._v(" "),s("p",[t._v("在做这种任务时，你时常会听到一个术语："),s("strong",[t._v("N-ways K-shot classification")]),t._v("：")]),t._v(" "),s("div",{staticClass:"custom-block theorem"},[s("p",{staticClass:"title"},[t._v("N-ways K-shot classification")]),s("p",[s("strong",[s("font",{attrs:{color:"blue"}},[t._v("N-ways")]),t._v(" "),s("font",{attrs:{color:"orange"}},[t._v("K-shot")]),t._v(" classification")],1),t._v(": In each task, there are "),s("font",{attrs:{color:"blue"}},[t._v("N classes")]),t._v(", each has "),s("font",{attrs:{color:"orange"}},[t._v("K examples")]),t._v(".")],1)]),s("p",[t._v("在 meta learning 里面，你需要去准备许多 N-ways K-shot tasks 作为 training 和 testing tasks。那怎样去找这一堆 N-ways K-shot 的 training tasks 呢？")]),t._v(" "),s("p",[t._v("在文献上最常用的是使用 "),s("mark",[t._v("Omniglot")]),t._v(" 这个 corpus 当作 benchmark corpus：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221002235938293.png",alt:"image-20221002235938293"}})]),t._v(" "),s("ul",[s("li",[t._v("在这个 corpus 中，有 1623 个 characters，每个 character 有 20 个 examples，比如右上角的 20 个 example 是同一个 character。")])]),t._v(" "),s("p",[s("strong",[t._v("在有了 Omniglot 这个 corpus 之后，你就可以制造 N-ways K-shot 的 tasks 了")]),t._v("。比如你选出 20 个 characters，然后每个 character 就只取一个 example，这样你就得到一个 20 ways 1 shot 的 task。测试资料的做法是，你再从那 20 个 characters 里面找一个 example 出来，然后接下来问这个 Query set 是这 20 个 class 里面的哪一个。如下图所示：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"72%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221003000713603.png",alt:"image-20221003000713603"}})]),t._v(" "),s("ul",[s("li",[t._v("Split your characters into training and testing characters\n"),s("ul",[s("li",[t._v("Sample N training characters, sample K examples from each sampled characters -> "),s("em",[t._v("one training task")])]),t._v(" "),s("li",[t._v("Sample N testing characters, sample K examples from each sampled characters -> "),s("em",[t._v("one testing task")])])])])]),t._v(" "),s("h3",{attrs:{id:"_4-2-other"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-other"}},[t._v("#")]),t._v(" 4.2 Other")]),t._v(" "),s("p",[t._v("Meta learning 不只是可以用在 Omniglot 上，"),s("a",{attrs:{href:"https://speech.ee.ntu.edu.tw/~tlkagk/meta_learning_table.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("table"),s("OutboundLink")],1),t._v(" 列举了 meta learning 在语音处理、NLP 等方向的应用。至于 meta learning 能走多远，我们拭目以待。")])],1)}),[],!1,null,null,null);a.default=m.exports}}]);