(window.webpackJsonp=window.webpackJsonp||[]).push([[79],{833:function(t,a,e){"use strict";e.r(a);var o=e(22),i=Object(o.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("截止到 22 年底，OpenAI 官方并未放出 ChatGPT 的论文，但是 blog 中提到：“ChatGPT is a sibling model to InstructGPT”，因此这里主要是按照 InstructGPT 的方式来讲解。")]),t._v(" "),e("blockquote",[e("p",[t._v("对比一下 InstructGPT 和 ChatGPT 的训练步骤，两者真的非常非常相似。")])]),t._v(" "),e("p",[t._v("ChatGPT 学习的四个阶段：")]),t._v(" "),e("ol",[e("li",[t._v("学习文字接龙")]),t._v(" "),e("li",[t._v("人类老师引导文字接龙的方向")]),t._v(" "),e("li",[t._v("模仿人类老师的爱好")]),t._v(" "),e("li",[t._v("用强化学习向模拟的老师学习")])]),t._v(" "),e("h2",{attrs:{id:"_1-chatgpt-训练的四个阶段"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-chatgpt-训练的四个阶段"}},[t._v("#")]),t._v(" 1. ChatGPT 训练的四个阶段")]),t._v(" "),e("h3",{attrs:{id:"阶段-1-学习文字接龙"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#阶段-1-学习文字接龙"}},[t._v("#")]),t._v(" 阶段 1. 学习文字接龙")]),t._v(" "),e("p",[t._v("比如你给 GPT 一个不完整的句子“你好”，然后让 GPT 接一个可能的字“美”：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"90%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227212912401.png",alt:"image-20221227212912401"}})]),t._v(" "),e("p",[t._v("这样就是从网上找大量的语料来让 GPT 学文字接龙就好了。")]),t._v(" "),e("p",[t._v("但一个可能让你疑惑的问题是，不完整的句子“你好”后面可能跟很多种可能呀，不一定就接“美”这个字，其实 GPT 的输出是一个 distribution，它学习的其实是“你好”后面接“高”或者“美”的可能性比较高，而接“吗”的也有不低的几率，而接“星”的几率就比较低了：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227213247760.png",alt:"image-20221227213247760"}})]),t._v(" "),e("p",[t._v("然后 GPT 从这个 distribution 中随机 sample 出一个字出来作为本次的 output，也正因为 GPT 每次产生 token 都是具有随机性的，所以 GPT "),e("strong",[t._v("每一次的输出都是不同的")]),t._v("：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227213523301.png",alt:"image-20221227213523301"}})]),t._v(" "),e("p",[e("strong",[t._v("学习文字接龙有什么用呢")]),t._v("？这是有很大作用的，光是学习文字接龙，GPT 就可以拿来回答问题：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227214547596.png",alt:"image-20221227214547596"}})]),t._v(" "),e("ul",[e("li",[t._v("通过这么两次对“不完整的句子”的文字接龙，我们就知道答案是“玉山”了。")])]),t._v(" "),e("p",[t._v("但实际上 GPT 在实际使用时并没有那么好用，比如你要问他“台湾最高的山是哪座”，这时你把这句话当成“不完整的句子”交给 GPT，也许你期待 GPT 产生“玉山”，但其实 GPT 也有可能产生别的东西，比如 GPT 也许从某个资料中看到过地理考试题，于是它决定给你出个选择题也是有可能的：")]),t._v(" "),e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227215919365.png",alt:"image-20221227215919365"}}),t._v(" "),e("p",[t._v("所以问题是："),e("strong",[t._v("如何引导 GPT 产生有用的输出呢")]),t._v("？这就交给了下一个阶段。")]),t._v(" "),e("h3",{attrs:{id:"阶段-2-人类老师引导文字接龙的方向"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#阶段-2-人类老师引导文字接龙的方向"}},[t._v("#")]),t._v(" 阶段 2：人类老师引导文字接龙的方向")]),t._v(" "),e("p",[t._v("这一步，找人来思考想问 GPT 的问题，并人工提供正确答案，比如如下：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227223605967.png",alt:"image-20221227223605967"}})]),t._v(" "),e("p",[t._v("这么做其实就是让 GPT 多看有益的问句和答案，不要让 GPT 去网络上看一些有的没的。这个过程不需要穷举所有的问题，我们只是要告诉 GPT 人类的偏好，因为产生这些答案其实是 GPT 本来就可以的，现在只是在激发它本来就有的力量，让它懂得人类的偏好。")]),t._v(" "),e("blockquote",[e("p",[t._v("并不需要特别多人类编写好的问题与答案，只需要数万则就可以。")])]),t._v(" "),e("h3",{attrs:{id:"阶段-3-模仿人类老师的喜好"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#阶段-3-模仿人类老师的喜好"}},[t._v("#")]),t._v(" 阶段 3：模仿人类老师的喜好")]),t._v(" "),e("p",[t._v("当向 GPT 问出一个问题之后，把 GPT 的不同答案收集起来，然后 OpenAI 雇佣人类去标注处哪些答案是好的答案，哪些答案是不好的答案，这里人类老师只需要给出一个 rank 就可以：")]),t._v(" "),e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227224321833.png",alt:"image-20221227224321833"}}),t._v(" "),e("p",[t._v("有了这些资讯以后，接下来就要去训练一个模仿人类老师的 model，这里称为 "),e("mark",[t._v("Teacher Model")]),t._v("：给它问题和 GPT 的答案，它要输出一个分数。这个 Teacher Model 的学习目标就是去模仿人类老师的评分标准。比如人类老师标注说，当面对问题“台湾最高的山是哪座？”时，答案“玉山”好于“谁来告诉我呀”，那么 Teacher Model 给出的分数应当是好的答案大于不好的答案，如下图所示：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227231852613.png",alt:"image-20221227231852613"}})]),t._v(" "),e("h3",{attrs:{id:"阶段-4-用-rl-向模拟的老师学习"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#阶段-4-用-rl-向模拟的老师学习"}},[t._v("#")]),t._v(" 阶段 4：用 RL 向模拟的老师学习")]),t._v(" "),e("p",[t._v("把问题“世界上最高的山是哪座？”丢给 GPT，然后 GPT 接了一句“世界上最深的海又在哪里？”，我们把这么两句话接起来给 Teacher Model，model 会输出一个分数，会给这句话打一个低分，这里的分数就是 RL 中的 “reward”，而 RL 的目标就是通过调整参数来得到最大的 reward。这个过程如下图所示：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227234215918.png",alt:"image-20221227234215918"}})]),t._v(" "),e("p",[t._v("通过 RL 技术，就希望说在问出“世界上最高的山是哪座？”时，GPT 能够回答出“喜马拉雅山”，从而让 Teacher Model 能给它一个高的 reward。经过 RL 的训练，这里的 GPT 就是 ChatGPT 了。")]),t._v(" "),e("h3",{attrs:{id:"summary"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#summary"}},[t._v("#")]),t._v(" Summary")]),t._v(" "),e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227234932482.png",alt:"image-20221227234932482"}}),t._v(" "),e("h2",{attrs:{id:"_2-chatgpt-不完美"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-chatgpt-不完美"}},[t._v("#")]),t._v(" 2. ChatGPT 不完美")]),t._v(" "),e("p",[t._v("如果你问 ChatGPT 一个正常的问题的话，它大概率已经在训练资料中见过这个题型了，如果你想要考倒它，一个技巧是，你问它一些没用而且简单的问题，很有可能就考倒它了：")]),t._v(" "),e("center",[e("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227234824404.png",alt:"image-20221227234824404"}})])],1)}),[],!1,null,null,null);a.default=i.exports}}]);