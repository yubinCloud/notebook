(window.webpackJsonp=window.webpackJsonp||[]).push([[74],{827:function(t,s,a){"use strict";a.r(s);var n=a(22),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("本章我们将利用 LSTM 实现几个有趣的应用。首先将使用语言模型进行文本生成，然后再介绍 seq2seq 的神经网络，"),a("strong",[t._v("将一个时序数据转换为另一个时序数据")]),t._v("。")]),t._v(" "),a("h2",{attrs:{id:"_1-使用语言模型生成文本"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-使用语言模型生成文本"}},[t._v("#")]),t._v(" 1. 使用语言模型生成文本")]),t._v(" "),a("p",[t._v("语言模型可用于各种各样的应用，其中具有代表性的例子有机器翻译、语音识别和文本生成。这里，我们将使用语言模型来生成文本。")]),t._v(" "),a("h3",{attrs:{id:"_1-1-使用-rnn-生成文本的步骤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-使用-rnn-生成文本的步骤"}},[t._v("#")]),t._v(" 1.1 使用 RNN 生成文本的步骤")]),t._v(" "),a("p",[t._v("上一章我们用 LSTM 层实现了语言模型，网络结构如下：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401205251936.png",alt:"image-20220401205251936"}}),t._v(" "),a("p",[t._v("现在我们来说明一下语言模型生成文本的顺序。这里仍以“you say goobye and i say hello.”这一在语料库上学习好的语言模型为例，考虑将单词 i 赋给这个语言模型的情况：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401210407764.png",alt:"image-20220401210407764"}}),t._v(" "),a("ul",[a("li",[t._v("语言模型根据已经出现的单词输出下一个出现的单词的概率分布。")])]),t._v(" "),a("p",[a("strong",[t._v("语言模型计算出下一个出现的单词的概率分布后，它如何生成下一个新单词呢")]),t._v("？"),a("u",[t._v("一种方法是选择概率最高的单词")]),t._v("，这时结果能唯一确定；"),a("u",[t._v("另一种方法是根据概率分布进行选择")]),t._v("，这样概率高的单词容易被选到，这时被采样到的单词每次都不一样。")]),t._v(" "),a("p",[t._v("这里我们想让每次生成的文本有所变化，因此我们使用后一种方法来选择单词。下面是生成的过程：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401212135069.png",alt:"image-20220401212135069"}}),t._v(" "),a("p",[t._v("之后根据需要重复此过程即可，直到出现 "),a("code",[t._v("<eos>")]),t._v(" 这一结尾记号，这样一来，我们就可以生成新的文本。")]),t._v(" "),a("blockquote",[a("p",[t._v("这里需要注意的是，像上面这样生成的新文本是训练数据中没有的新生成的文本。因为"),a("strong",[t._v("语言模型并不是背诵了训练数据，而是学习了训练数据中单词的排列模式")]),t._v("。如果语言模型通过语料库正确学习了单词的出现模式，我们就可以期待该语言模型生成的文本对人类而言是自然的、有意义的。")])]),t._v(" "),a("h3",{attrs:{id:"_1-2-文本生成的实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-文本生成的实现"}},[t._v("#")]),t._v(" 1.2 文本生成的实现")]),t._v(" "),a("p",[t._v("下面我们进行文本生成的实现，我们基于上一章的 Rnnlm 类来创建继承自它的 RnnlmGen 类，然后向这个类添加生成文本的方法：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("RnnlmGen")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Rnnlm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("generate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skip_ids"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sample_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        word_ids "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" start_id\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" sample_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            score "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            sampled "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("skip_ids "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sampled "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" skip_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sampled\n                word_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" word_ids\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br")])]),a("p",[t._v("这个类用 "),a("code",[t._v("generate(start_id, skip_ids, sample_size)")]),t._v(" 生成本文，此处 "),a("code",[t._v("start_id")]),t._v(" 是第一个单词 ID，"),a("code",[t._v("sample_size")]),t._v(" 表示要采样的单词数量，"),a("code",[t._v("skip_ids")]),t._v(" 是单词 ID 列表，它指定的单词将不被采样，这个参数用于排除 PTB 数据集中的 "),a("code",[t._v("<unk>")]),t._v(" 、N 等被预处理过的单词。generate 方法首先通过 "),a("code",[t._v("model.predict(x)")]),t._v(" 输出各个单词的得分，然后基于 "),a("code",[t._v("p=softmax(score)")]),t._v(" 对得分进行正则化，这样就获得了我们想要的概率分布。")]),t._v(" "),a("blockquote",[a("p",[t._v("PTB 数据集对原始文本进行了预处理，稀有单词被 "),a("code",[t._v("<unk>")]),t._v(" 替换，数字被 N 替换。另外，我们用 "),a("code",[t._v("<eos>")]),t._v(" 作为文本的分隔符。")])]),t._v(" "),a("p",[t._v("现在，使用这个 RnnlmGen 类进行文本生成，这里先在完全没有学习的状态（即权重参数是随机初始值的状态）下生成文本。我们将第一个单词设为 you，可以得到如下的句子：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401224157250.png",alt:"image-20220401224157250"}}),t._v(" "),a("p",[t._v("可见，输出的文本是一堆乱七八糟的单词，不过这可以理解，因此这里的模型权重使用的是随机初始值，所以输出了没有意义的文本。我们再利用上一章学习好的权重来进行文本生生成，为此使用 "),a("code",[t._v("model.load_params(...)")]),t._v(" 读入学习好的参数，并生成文本：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401224448455.png",alt:"image-20220401224448455"}})]),t._v(" "),a("p",[t._v("虽然上面的结果中可以看到多处语法错误和意思不通的地方，不过也有几处读起来已经比较像句子了。这个实验生成的文本在某种程度上可以说是正确的，不过结果中仍有许多不自然的地方，改进空间很大。")]),t._v(" "),a("h3",{attrs:{id:"_1-3-更好的文本生成"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-更好的文本生成"}},[t._v("#")]),t._v(" 1.3 更好的文本生成")]),t._v(" "),a("p",[t._v("之前我们改进了语音模型，实现了 BetterRnnlm 类们现在我们继承它并用于文本生成，结果如下：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401224753529.png",alt:"image-20220401224753529"}})]),t._v(" "),a("p",[t._v("可以看出，这个模型生成了比之前更自然的文本。")]),t._v(" "),a("h2",{attrs:{id:"_2-seq2seq-模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-seq2seq-模型"}},[t._v("#")]),t._v(" 2. seq2seq 模型")]),t._v(" "),a("p",[t._v("文本数据、音频数据和视频数据都是时序数据，并存在许多需要将一种时序数据转换为另一种时序数据的任务，比如机器翻译、语音识别、聊天机器人、将源代码转为机器语言的编译器等。像这样，世界上存在许多输入输出均为时序数据的任务。现在我们会"),a("strong",[t._v("考察将时序数据转换为其他时序数据的模型")]),t._v("，作为实现，我们将介绍使用两个 RNN 的 seq2seq 模型。")]),t._v(" "),a("h3",{attrs:{id:"_2-1-seq2seq-的原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-seq2seq-的原理"}},[t._v("#")]),t._v(" 2.1 seq2seq 的原理")]),t._v(" "),a("p",[t._v("seq2seq 模型也称为 "),a("strong",[t._v("Encoder-Decoder 模型")]),t._v("。这个模型"),a("strong",[t._v("有两个模块——Encoder（编码器）和 Decoder（解码器）")]),t._v("。编码器对输入数据进行编码，解码器对被编码的数据进行解码。seq2seq 基于编码器和解码器"),a("strong",[t._v("将一个时序数据转换为另一个时序数据")]),t._v("。")]),t._v(" "),a("p",[t._v("举一个例子来说明其机制，如下图，seq2seq 将日语翻译成了英语：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402093558733.png",alt:"image-20220402093558733"}}),t._v(" "),a("ul",[a("li",[t._v("编码器编码的信息浓缩了翻译所必需的信息，解码器基于这个浓缩的信息生成目标文本")])]),t._v(" "),a("p",[t._v("我们首先看一下编码器的细节：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402094135432.png",alt:"image-20220402094135432"}}),t._v(" "),a("p",[t._v("可以看出，编码器利用 RNN 将时序数据转换为隐藏状态 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("，它是一个固定长度的向量。说到底，编码就是将任意长度的文本转换为一个固定长度的向量：")],1),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402094345128.png",alt:"image-20220402094345128"}}),t._v(" "),a("p",[t._v("解码器是如何“处理”这个编码好的向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("，从而生成目标文本的呢？其实我们只需要利用之前讨论的进行文本生成的模型即可：")],1),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402094647101.png",alt:"image-20220402094647101"}}),t._v(" "),a("ul",[a("li",[t._v("可以看出解码器的结构和文本生成的神经网络完全相同，不过存在一点点差异，就是 LSTM 层会接收向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("，而之前的模型不接收任何信息，"),a("strong",[t._v("这个唯一的、微小的改变使得普通的语言模型进化为可以驾驭翻译的解码器")]),t._v("。")],1)]),t._v(" "),a("blockquote",[a("p",[t._v("我们使用了 "),a("code",[t._v("<eos>")]),t._v(" 这一分隔符作为通知解码器开始生成文本的信号，另外，解码器采样到 "),a("code",[t._v("<eos>")]),t._v(" 为止，作为结束信号。其他文献中也有使用 "),a("code",[t._v("<go>")]),t._v("、"),a("code",[t._v("<start>")]),t._v(" 或 "),a("code",[t._v("_")]),t._v(" 作为分隔符。")])]),t._v(" "),a("p",[t._v("现在我们连接编码器和解码器，并给出它的层结构：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402095658527.png",alt:"image-20220402095658527"}}),t._v(" "),a("p",[t._v("seq2seq 由两个 LSTM 层构成，即编码器的 LSTM 和解码器的 LSTM，此时 LSTM 层的"),a("strong",[t._v("隐藏状态是编码器和解码器的桥梁")]),t._v("：")]),t._v(" "),a("ul",[a("li",[t._v("正向传播时，编码器的编码信息通过 LSTM 层的隐藏状态传递给解码器")]),t._v(" "),a("li",[t._v("反向传播时，解码器的梯度通过这个“桥梁”传递给编码器")])]),t._v(" "),a("h3",{attrs:{id:"_2-2-时序数据转换的简单尝试"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-时序数据转换的简单尝试"}},[t._v("#")]),t._v(" 2.2 时序数据转换的简单尝试")]),t._v(" "),a("p",[t._v("首先说明一下我们要处理的问题。这里我们将“加法”视为一个时序转换问题，效果如图：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402100347204.png",alt:"image-20220402100347204"}}),t._v(" "),a("p",[t._v("这种为了评价机器学习而创建的简单问题，称为 “"),a("strong",[t._v("toy problem")]),t._v("”。")]),t._v(" "),a("p",[a("strong",[t._v("seq2seq 对加法逻辑一无所知，它从样本中出现的字符模式，真的可以学习到加法运算的规则吗")]),t._v("？这正是本次实验的看头。在之前我们都把文本以单词为单位进行了分割，但并非必须这样做，这次我们将不以单词为单位，而是以字符为单位进行分割，比如“57 + 5”这样的输入会被处理为 ['5', '7', '+', '5'] 这样的列表。")]),t._v(" "),a("p",[t._v("还有一个问题，不同加法的字符数是不同的，比如“57 + 5”共有 4 个字符，而“628 + 521”共有 7 个字符。如此，在加法问题中，每个样本在时间方向上的大小不同。也就是说，加法问题处理的是"),a("strong",[t._v("可变长度的时序数据")]),t._v("。")]),t._v(" "),a("blockquote",[a("p",[t._v("在使用批数据进行学习时，会一起处理多个样本。此时，（在我们的实现中）需要保证一个批次内各个样本的数据形状是一致的。")])]),t._v(" "),a("p",[a("strong",[t._v("在基于 mini-batch 学习可变长度的时序数据时，最简单的方法是使用填充")]),t._v("（padding）。"),a("strong",[t._v("填充")]),t._v("就是用无效（无意义）数据填入原始数据，从而使数据长度对齐：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402100914788.png",alt:"image-20220402100914788"}}),t._v(" "),a("p",[t._v("制作出如下的数据集并使用：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402101024174.png",alt:"image-20220402101024174"}}),t._v(" "),a("p",[a("strong",[t._v("数据集原本应分成训练用、验证用和测试用 3 份。用训练数据进行学习，用验证数据进行调参，最后再用测试数据评价模型的能力")]),t._v("。而简单起见，这里只分成训练数据和测试数据 2 份，用它们进行模型的训练和评价。")]),t._v(" "),a("h3",{attrs:{id:"_2-3-seq2seq-的实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-seq2seq-的实现"}},[t._v("#")]),t._v(" 2.3 seq2seq 的实现")]),t._v(" "),a("p",[t._v("seq2seq 是组合了两个 RNN 的神经网络。这里我们首先将这两个 RNN 实现为 Encoder 类和 Decoder 类，然后将这两个类组合起来，来实现 seq2seq 类。")]),t._v(" "),a("h4",{attrs:{id:"_2-3-1-encoder-类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-1-encoder-类"}},[t._v("#")]),t._v(" 2.3.1 Encoder 类")]),t._v(" "),a("p",[t._v("Encoder 类接收字符串，将其转化为向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("：")],1),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402101230018.png",alt:"image-20220402101230018"}}),t._v(" "),a("p",[t._v("它的层结构展开后如下图：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402103036213.png",alt:"image-20220402103036213"}}),t._v(" "),a("ul",[a("li",[a("u",[t._v("Encoder 类由 Embedding 层和 LSTM 层组成")]),t._v("，Embedding 层将字符（字符 ID）转化为字符向量，然后将字符向量输入 LSTM 层。")]),t._v(" "),a("li",[t._v("LSTM 层向右（时间方向）输出隐藏状态和记忆单元，向上输出隐藏状 态。这里，因为上方不存在层，所以"),a("u",[t._v("丢弃 LSTM 层向上的输出")]),t._v("。在编码器处理完最后一个字符后，输出 LSTM 层的隐藏状态 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("。然后，这个隐藏状态 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 被传递给解码器。")],1)]),t._v(" "),a("blockquote",[a("p",[t._v("编码器只将 LSTM 的隐藏状态传递给解码器。尽管也可以把 LSTM 的记忆单元传递给解码器，但"),a("strong",[t._v("我们通常不太会把 LSTM 的记忆单元传递给其他层")]),t._v("。这是因为，LSTM 的记忆单元被设计为只给自身使用。")])]),t._v(" "),a("p",[t._v("我们已经将时间方向上进行整体处理的层实现为了 Time LSTM 层和 Time Embedding 层，通过使用他们，我们的编码器将如下图所示：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402103534054.png",alt:"image-20220402103534054"}}),t._v(" "),a("p",[t._v("在初始化时，因为这次并不保持 Time LSTM 层的状态，所以设定 stateful=False。")]),t._v(" "),a("blockquote",[a("p",[t._v("之前的语言模型处理的是只有一个长时序数据的问题，那时我们设定 Time LSTM 层的参数 stateful=True，以在保持隐藏状态的同时，处理长时序数据。而这次是有多个短时序数据的问题。因此，针对每个问题重设 LSTM 的隐藏状态（为 0 向量）。")])]),t._v(" "),a("p",[t._v("简单看一下其 forward 函数：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\txs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lstm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\tself"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hs\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("p",[t._v("在编码器的反向传播中，LSTM 层的最后一个隐藏状态的梯度是 dh，这个 dh 是从解码器传来的梯度：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("backward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dh"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\tdhs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros_like"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\tdhs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dh\n\tdout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lstm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dhs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\tdout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" dout\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("h4",{attrs:{id:"_2-3-2-decoder-类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-2-decoder-类"}},[t._v("#")]),t._v(" 2.3.2 Decoder 类")]),t._v(" "),a("p",[t._v("Decoder 类接收 Encoder 类输出的 h，输出目标字符串：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402103950452.png",alt:"image-20220402103950452"}}),t._v(" "),a("p",[t._v("解码器可以由 RNN 实现，我们这里使用 LSTM：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402124341110.png",alt:"image-20220402124341110"}}),t._v(" "),a("p",[t._v("这里使用了监督数据 "),a("code",[t._v("_62")]),t._v(" 进行学习，此时输入数据是 "),a("code",[t._v("['_', '6', '2',' ']")]),t._v("，对应的输出是 "),a("code",[t._v("['6', '2', ' ', ' ']")]),t._v("。")]),t._v(" "),a("blockquote",[a("p",[t._v("在使用 RNN 进行文本生成时，学习时和生成时的数据输入方法不同：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("在学习时")]),t._v("，因为已经知道正确解，所以可以"),a("strong",[t._v("整体地输入")]),t._v("时序方向上的数据。")]),t._v(" "),a("li",[a("strong",[t._v("在推理时")]),t._v("（生成新字符串时），则"),a("strong",[t._v("只能输入第 1 个通知开始的分隔符")]),t._v("（本次为“_”）。然后，基于输出采样 1 个字符，并将这个采样出来的字符作为下一个输入，如此重复该过程。")])])]),t._v(" "),a("p",[t._v("之前我们进行文本生成时，使用了概率分布来采样，因此生成的文本会随机变动。因为这次的问题是加法，所以我们想消除这种概率性的“波动”，生成“确定性的”答案。为此，"),a("strong",[t._v("这次我们仅选择得分最高的字符")]),t._v("。下面是解码器生成字符串的过程：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402124819905.png",alt:"image-20220402124819905"}}),t._v(" "),a("ul",[a("li",[t._v("argmax 是获取最大值的索引（本例中是字符 ID）的节点。")]),t._v(" "),a("li",[t._v("这次没有使用 Softmax 层，而是从 Affine 层输出的得分中选择了最大值的字符 ID")])]),t._v(" "),a("p",[t._v("如上所述，在解码器中，在学习时和在生成时处理 Softmax 层的方式是不一样的。因此，Softmax with Loss 层交给此后实现的 Seq2seq 类处理。由此画出 Decoder 类的结构：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402125037813.png",alt:"image-20220402125037813"}}),t._v(" "),a("h4",{attrs:{id:"_2-3-3-seq2seq-类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-3-seq2seq-类"}},[t._v("#")]),t._v(" 2.3.3 seq2seq 类")]),t._v(" "),a("p",[t._v("最后来看 Seq2seq 类的实现，这里需要做的只是将 Encoder 类和 Decoder 类连接在一起，然后使用 Time Softmax with Loss 层计算损失而已：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Seq2seq")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BaseModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TimeSoftmaxWithLoss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br")])]),a("h4",{attrs:{id:"_2-3-4-seq2seq-的评价"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-4-seq2seq-的评价"}},[t._v("#")]),t._v(" 2.3.4 seq2seq 的评价")]),t._v(" "),a("p",[t._v("Seq2seq 的学习和基础神经网络的学习具有相同的流程。基础神经网络的学习流程如下：")]),t._v(" "),a("ol",[a("li",[t._v("从训练数据中选择一个 mini-batch")]),t._v(" "),a("li",[t._v("基于 mini-batch 计算梯度")]),t._v(" "),a("li",[t._v("使用梯度更新权重")])]),t._v(" "),a("p",[t._v("这里使用 Trainer 类进行上述操作，另外，seq2seq 针对每个 epoch 求解测试数据（生成字符串），并计算正确率：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Seq2seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noptimizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Adam"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nacc_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_epoch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_epoch"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_grad"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("max_grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    correct_num "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        question"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" correct "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        verbose "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n        correct_num "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" eval_seq2seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" question"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" correct"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                    id_to_char"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" verbose"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_reverse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    acc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("correct_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    acc_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("acc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val acc %.3f%%'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("acc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br")])]),a("ul",[a("li",[t._v("这里采 用正确率（正确回答了多少问题）作为评价指标")])]),t._v(" "),a("p",[t._v("运行后的结果为：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402130811413.png",alt:"image-20220402130811413"}}),t._v(" "),a("ul",[a("li",[t._v("每个 epoch 显示一次结果，每个问题中， Q 代表问题，T 代表正确答案，第三行代表模型给出的答案。")])]),t._v(" "),a("p",[t._v("随着学习的进行，上面的结果会发生什么样的变化：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402130950215.png",alt:"image-20220402130950215"}}),t._v(" "),a("p",[t._v("从结果中可知，seq2seq 最开始没能顺利回答问题。但是，随着学习不断进行，它在慢慢靠近正确答案，然后变得可以正确回答一些问题，直到最后的正确率约为 10%：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402192455313.png",alt:"image-20220402192455313"}}),t._v(" "),a("p",[t._v("为了能更好地学习相同的问题，我们对 seq2seq 做一些改进。")]),t._v(" "),a("h2",{attrs:{id:"_3-seq2seq-的改进"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-seq2seq-的改进"}},[t._v("#")]),t._v(" 3. seq2seq 的改进")]),t._v(" "),a("p",[t._v("我们对之前的 seq2seq 进行改进，以改进学习的进展。")]),t._v(" "),a("h3",{attrs:{id:"_3-1-反转输入数据-reverse"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-反转输入数据-reverse"}},[t._v("#")]),t._v(" 3.1 反转输入数据（Reverse）")]),t._v(" "),a("p",[t._v("第一个改进方案是非常简单的技巧——"),a("strong",[t._v("反转输入数据的顺序")]),t._v("：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402192216350.png",alt:"image-20220402192216350"}}),t._v(" "),a("p",[t._v("在许多情况下，使用这个技巧后，学习进展得更快，最终的精度也有提高。")]),t._v(" "),a("p",[t._v("为了反转输入数据，在读入数据集之后，我们追加下面的代码：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("div",{staticClass:"highlight-lines"},[a("br"),a("br"),a("div",{staticClass:"highlighted"},[t._v(" ")]),a("br")]),a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sequence"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'addition.txt'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\nx_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])]),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("p",[t._v("通过反转输入数据，正确率可以上升多少呢：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402194731266.png",alt:"image-20220402194731266"}}),t._v(" "),a("p",[a("strong",[t._v("仅仅通过反转输入数据，学习的进展就得到了极大改善")]),t._v("！虽然反转数据的效果因任务而异，但是通常都会有好的结果。")]),t._v(" "),a("p",[a("strong",[t._v("为什么反转数据后，学习进展变快，精度提高了呢")]),t._v("？虽然理论上不是很清楚，但是直观上可以认为，反转数据后梯度的传播可以更平滑。比如，考虑将“吾輩 は 猫 で ある” 翻译成“I am a cat”这一问题，单词“吾輩”和单词“I”之间有转换关系，此时，从“吾輩”到“I”的路程必须经过“は”“猫”“で”“ある”这 4 个单词的 LSTM 层。因此，在反向传播时，梯度从“I”抵达“吾輩”，也要受到这个距离的影响。那么，如果反转输入语句，也就是变为“ある で 猫 は 吾輩”，结果会怎样呢？此时，“吾輩”和“I”彼此相邻，梯度可以直接传递。如此，因为通过反转，输入语句的开始部分和对应的转换后的单词之间的距离变近（这样的情况变多），所以梯度的传播变得更容易，学习效率也更高。不过，在反转输入数据后，单词之间的“平均”距离并不会发生改变。")]),t._v(" "),a("h3",{attrs:{id:"_3-2-偷窥-peeky"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-偷窥-peeky"}},[t._v("#")]),t._v(" 3.2 偷窥（Peeky）")]),t._v(" "),a("p",[t._v("编码器将输入语句转换为固定长度的向量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("，这个 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 集中了解码器所需的全部信息，也就是说，它是解码器的唯一信息源，但是如下图所示，当前的 seq2seq 只有最开始时刻的 LSTM 层利用了 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v("，我们能更加充分地利用这个 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 吗？")],1),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402200304363.png",alt:"image-20220402200304363"}}),t._v(" "),a("p",[t._v("为了达成该目标，seq2seq 的第二个改进方案就应运而生了——"),a("strong",[t._v("将这个集中了重要信息的编码器的输出 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 分配给解码器的其他层")],1),t._v("。改进后的解码器如图：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402200411443.png",alt:"image-20220402200411443"}}),t._v(" "),a("ul",[a("li",[t._v("将编码器的输出 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 分配给所有时刻的 LSTM 层和 Affine 层")],1)]),t._v(" "),a("p",[t._v("相较于之前，LSTM 层专用的重要信息 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"h"}})],1)],1)],1),t._v(" 现在在多个层中共享了。")],1),t._v(" "),a("blockquote",[a("p",[t._v("这里的改进是将编码好的信息分配给解码器的其他层，这可以解释为其他层也能“"),a("strong",[t._v("偷窥")]),t._v("”到编码信息。因为“偷窥”的英语是 peek，所以将这个改进了的解码器称为 Peeky Decoder，其相应的 seq2seq 称为 Peeky seq2seq。")])]),t._v(" "),a("p",[t._v("上图的结构中，有两个向量同时被输入到了 LSTM 层和 Affine 层，这实际上表示两个向量的拼接（concatenate）。如果使用 concat 节点拼接两个向量，则正确的计算图可以绘制成下图形式：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402203848285.png",alt:"image-20220402203848285"}})]),t._v(" "),a("p",[t._v("编码器与上一节没有变化。由于其实现并不难，我们省略了 PeekyDecoder 类的实现。")]),t._v(" "),a("p",[t._v("最后我们看一下实现 PeekySeq2seq，这和上一节的 Seq2seq 类基本相同，唯一的区别是 Decoder 层：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("PeekySeq2seq")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Seq2seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" PeekyDecoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("V"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" H"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TimeSoftmaxWithLoss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("params\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grads\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br")])]),a("p",[t._v("至此，准备工作就完成了，现在我们使用这个 PeekySeq2seq 类，再次挑战加法问题：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" PeekySeq2seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" wordvec_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("可以看到结果如图所示：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402205923166.png",alt:"image-20220402205923166"}}),t._v(" "),a("p",[t._v("加上了 Peeky 的 seq2seq 的结果大幅变好。刚过 10 个 epoch 时，正确率已经超过 90%，最终的正确率接近 100%。")]),t._v(" "),a("p",[t._v("从上述实验结果可知，Reverse 和 Peeky 都有很好的效果。借助反转输入语句的 Reverse 和共享编码器信息的 Peeky，我们获得了令人满意的结果！")]),t._v(" "),a("p",[t._v("这里的实验有几个需要注意的地方。因为使用 Peeky 后，网络的权重参数会额外地增加，计算量也会增加，所以这里的实验结果必须考虑到相应地增加的“负担”。另外，"),a("strong",[t._v("seq2seq 的精度会随着超参数的调整而大幅变化")]),t._v("。虽然这里的结果是可靠的，但是在实际问题中，它的效果可能不稳定。")]),t._v(" "),a("h2",{attrs:{id:"_4-seq2seq-的应用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-seq2seq-的应用"}},[t._v("#")]),t._v(" 4. seq2seq 的应用")]),t._v(" "),a("p",[t._v("seq2seq 将某个时序数据转换为另一个时序数据，这个转换时序数据的框架可以应用在各种各样的任务中：")]),t._v(" "),a("ul",[a("li",[t._v("机器翻译：将“一种语言的文本”转换为“另一种语言的文本”")]),t._v(" "),a("li",[t._v("自动摘要：将“一个长文本”转换为“短摘要”")]),t._v(" "),a("li",[t._v("问答系统：将“问题”转换为“答案”")]),t._v(" "),a("li",[t._v("邮件自动回复：将“接收到的邮件文本”转换为“回复文本”")])]),t._v(" "),a("p",[t._v("像这样，seq2seq 可以用于处理成对的时序数据的问题。除了自然语言之外，也可以用于语音、视频等数据。")]),t._v(" "),a("h3",{attrs:{id:"_4-1-聊天机器人"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-聊天机器人"}},[t._v("#")]),t._v(" 4.1 聊天机器人")]),t._v(" "),a("p",[t._v("聊天机器人是人和计算机使用文本进行对话的程序，因为对话是由“对方的发言”和“本方的发言”构成的，"),a("strong",[t._v("可以理解为是将“对方的发言”转换为“本方的发言”的问题")]),t._v("。也就是说，如果有对话文本数据，seq2seq 就可以学习它：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402210807013.png",alt:"image-20220402210807013"}}),t._v(" "),a("p",[t._v("这个机器人还无法泛化，但是，基于对话获取答案或者线索，这一点非常实用，应用范围很广。")]),t._v(" "),a("h3",{attrs:{id:"_4-2-算法学习"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-算法学习"}},[t._v("#")]),t._v(" 4.2 算法学习")]),t._v(" "),a("p",[t._v("本章进行的 seq2seq 实验是加法这样的简单问题，但理论上它也能处理更加高级的问题：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402210929509.png",alt:"image-20220402210929509"}}),t._v(" "),a("p",[t._v("可以直接将源代码输入 seq2seq，让 seq2seq 对源代码与目标答案一起进行学习。不过这类问题并不太好解决，通过改造 seq2seq 的结构，可以期待这样的问题能够被解决。")]),t._v(" "),a("h3",{attrs:{id:"_4-3-自动图像描述"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-自动图像描述"}},[t._v("#")]),t._v(" 4.3 自动图像描述")]),t._v(" "),a("p",[a("strong",[t._v("自动图像描述")]),t._v("将“图像”转换为“文本”：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402211121286.png",alt:"image-20220402211121286"}}),t._v(" "),a("p",[t._v("这时我们熟悉的网络结构，实际上，它和之前的网络的唯一区别在于，编码器从 LSTM 换成了 CNN，而解码器仍使用与之前相同的网络。仅通过这点改变（用 CNN 替代 LSTM），seq2seq 就可以处理图像了。")]),t._v(" "),a("p",[t._v("现在我们看几个基于 seq2seq 的自动图像描述的例子，这时由基于 TensorFlow 的 im2txt 生成的例子：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402211239229.png",alt:"image-20220402211239229"}}),t._v(" "),a("p",[t._v("由图可知，这里得到了很不错的结果。之所以能够达到这样的效果，是因为存在大量的图像和说明文字等训练数据，再加上可以高效学习这些训练数据的 seq2seq 的应用，最终得到了如图所示的出色结果。")]),t._v(" "),a("p",[t._v("下一章我们将继续改进 seq2seq，届时深度学习中最重要的技巧之一 Attention 将会出现。")])])}),[],!1,null,null,null);s.default=e.exports}}]);