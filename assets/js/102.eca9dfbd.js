(window.webpackJsonp=window.webpackJsonp||[]).push([[102],{855:function(t,e,a){"use strict";a.r(e);var s=a(22),i=Object(s.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"_1-contextualized-word-embedding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-contextualized-word-embedding"}},[t._v("#")]),t._v(" 1. Contextualized Word Embedding")]),t._v(" "),a("p",[t._v("很多词存在一词多义的情况，为了应对这种情况，出现了 "),a("mark",[t._v("Contextualized Word Embedding")]),t._v("，它期待说：")]),t._v(" "),a("ul",[a("li",[t._v("Each word token has its own embedding (even though it has the same word type)")]),t._v(" "),a("li",[t._v("The embeddings of word tokens also depend on its context.")])]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215201656060.png",alt:"image-20221215201656060"}})]),t._v(" "),a("ul",[a("li",[t._v("比如说，下面两个句子中的 bank 的含义可能比较相近，他们的 Embedding 也会很相近；但是右边上下的两个句子的 bank 的含义可能就不太一样了，这时他们的 Embedding 也会差距很大。")])]),t._v(" "),a("h2",{attrs:{id:"_2-elmo"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-elmo"}},[t._v("#")]),t._v(" 2. ELMo")]),t._v(" "),a("p",[t._v("ELMo 是一个 RNN-based 的 Language Model。什么是 RNN-based language model：它是从大量 sentences 中训练出来的。比如你有一句话“潮水 退了 就 知道 谁 没穿 裤子”，那么就要教给它如果看到一个 begin token <BOS>，那就要输出“潮水”：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215202145192.png",alt:"image-20221215202145192"}})]),t._v(" "),a("p",[t._v("接下来再输入“潮水”这个 token，你就要输出“退了”这个 token：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215202244126.png",alt:"image-20221215202244126"}})]),t._v(" "),a("p",[t._v("看到“潮水”和“退了”这两个 token，就要输出“就”….. 然后一直重复下去：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215202345374.png",alt:"image-20221215202345374"}})]),t._v(" "),a("p",[t._v("这样用很多 sentences 学完之后，你就有 Contextualized Word Embedding 了！你可以直接把 RNN 的 hidden layer 拿出来，说它就是当前输入的那个 token 的 Contextualized Word Embedding，比如下图输入“退了”之后，标红的那一部分：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215202631556.png",alt:"image-20221215202631556"}}),t._v(" "),a("p",[t._v("为什么说它是 Contextualized 呢？因为如果你给它不同的 context 的句子，那它输出的 embedding 是不同的。")]),t._v(" "),a("p",[t._v("以上就是 ELMo 的基本概念。但上面这种方式只考虑了每个 token 的前文，没有考虑到后文，这时只需要再训练一个反向的 RNN 就好了：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215202951749.png",alt:"image-20221215202951749"}})]),t._v(" "),a("p",[t._v("现在我们为了得到“退了”这个 token 的 embedding，就要把两个方向的 RNN 的 hidden layer 都拿出来：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215203137422.png",alt:"image-20221215203137422"}})]),t._v(" "),a("p",[t._v("现在很多模型都是 Deep 的，ELMo 也是 deep 的，它有很多层的 RNN：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215203239976.png",alt:"image-20221215203239976"}}),t._v(" "),a("p",[t._v("但现在遇到的问题是，它有这么多层 RNN，每一层都有一个 embedding，那到底应该用哪一层的呢？ELMo 给出的 solution 就是：“我全都要！”")]),t._v(" "),a("p",[t._v("每次给 ELMo 一个 token，它会得到好多个 contextualized word embedding（每一层 RNN 都会得到），这时怎么办呢？就可以把每一个 embedding 统统加起来一起用，ELMo 具体的做法是 weighted sum：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215203700162.png",alt:"image-20221215203700162"}})]),t._v(" "),a("ul",[a("li",[t._v("黄色和绿色代表同一个 token 的不同 contextualized word embedding，ELMo 通过 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"3B1"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v(" 和 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"3B1"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1),t._v(" 把他们 weighted sum 起来，得到这个 token 的最终的 embedding，也就是蓝色的 vector，这时你就可以那这个蓝色的最终的 embedding 用到 downstream tasks 中。")],1),t._v(" "),a("li",[a("strong",[t._v("这里的 weights "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 是通过 downstream task 中 learn 出来的")],1),t._v("。所以说你要实现先决定好要做哪个 task（比如 QA），然后再把这些参数与接下来的 task 一起 learn 出来。")])]),t._v(" "),a("p",[t._v("所以这里不同的 downstream tasks 用到的 weights "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 是不一样的。")],1),t._v(" "),a("p",[t._v("原论文展示了一下训练结果：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215204531266.png",alt:"image-20221215204531266"}})]),t._v(" "),a("ul",[a("li",[t._v("横轴代表不同的 task，纵轴是不同层的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 的大小。可以看出 SQuAD 这个 task 就特别需要重视 LSTM 1 中的 embedding。")],1)])],1)}),[],!1,null,null,null);e.default=i.exports}}]);