(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{814:function(t,s,a){"use strict";a.r(s);var n=a(22),r=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("blockquote",[a("p",[t._v("参考自："),a("a",{attrs:{href:"https://mp.weixin.qq.com/s/L4iIs_aVGEhAz5BRGTwhyw",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dropout技术一览：可视化解释以及在DNN/CNN/RNN中的应用"),a("OutboundLink")],1)])]),t._v(" "),a("h2",{attrs:{id:"_1-动机"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-动机"}},[t._v("#")]),t._v(" 1. 动机")]),t._v(" "),a("p",[t._v("在深度机器学习中训练一个模型的主要挑战之一是协同适应。这意味着神经元是相互依赖的。他们对彼此的影响相当大，相对于他们的输入还不够独立。我们也经常发现一些神经元具有比其他神经元更重要的预测能力的情况。换句话说，我们会过度依赖于个别的神经元的输出。这些影响必须避免，权重必须具有一定的分布，以防止过拟合。某些神经元的协同适应和高预测能力可以通过不同的正则化方法进行调节。其中最常用的是 "),a("strong",[t._v("Dropout")]),t._v("。然而，dropout 方法的全部功能很少被使用。取决于它是 DNN，一个 CNN 或一个 RNN，不同的 dropout 方法可以被应用。在实践中，我们只(或几乎)使用一个。我认为这是一个可怕的陷阱。所以在本文中，我们将从数学和可视化上深入到 dropouts 的世界中去理解：")]),t._v(" "),a("ul",[a("li",[t._v("标准的 Dropout 方法")]),t._v(" "),a("li",[t._v("标准 Dropout 的变体")]),t._v(" "),a("li",[t._v("用在 CNNs 上的 dropout 方法")]),t._v(" "),a("li",[t._v("用在 RNNs 上的 dropout 方法")]),t._v(" "),a("li",[t._v("其他的 dropout 应用(蒙特卡洛和压缩)")])]),t._v(" "),a("h2",{attrs:{id:"_2-符号"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-符号"}},[t._v("#")]),t._v(" 2. 符号")]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215220131203.png",alt:"image-20221215220131203"}}),t._v(" "),a("h2",{attrs:{id:"_3-各种各样的-dropout"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-各种各样的-dropout"}},[t._v("#")]),t._v(" 3. 各种各样的 Dropout")]),t._v(" "),a("h3",{attrs:{id:"_3-1-standard-dropout"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-standard-dropout"}},[t._v("#")]),t._v(" 3.1 Standard dropout")]),t._v(" "),a("p",[t._v("最常用的 dropout 方法是 Hinton 等人在 2012 年推出的 "),a("mark",[t._v("Standard dropout")]),t._v("。通常简单地称为 “Dropout”，在本文中我们将称之为 Standard dropout。")]),t._v(" "),a("img",{staticStyle:{zoom:"75%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215220811105.png",alt:"image-20221215220811105"}}),t._v(" "),a("p",[t._v("为了防止"),a("strong",[t._v("训练")]),t._v("阶段的过拟合，"),a("strong",[t._v("随机去掉神经元")]),t._v("。在一个密集的(或全连接的)网络中，对于每一层，我们给出了一个dropout的概率 p。在每次迭代中，每个神经元被去掉的概率为 p。Hinton等人的论文建议，输入层的dropout概率为“p=0.2”，隐藏层的dropout概率为“p=0.5”。显然，我们对输出层感兴趣，输出层是我们的预测结果。所以我们"),a("strong",[t._v("不会在输出层应用 dropout")]),t._v("。")]),t._v(" "),a("p",[t._v("Dropout 具体使用的方法就是让被丢弃的值归 0，所以只需要创建一个元素值为 0 或 1 的 mask "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1),t._v(" 与 feature vector 进行 element-wise product 即可。当某个 feature 归 0 后，与任何参数相乘都是 0，这也就相当于这部分网络不起作用了。")],1),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215221005036.png",alt:"image-20221215221005036"}})]),t._v(" "),a("p",[t._v("数学上，我们说每个神经元的丢弃概率遵循概率 p 的"),a("strong",[t._v("伯努利")]),t._v("分布。因此，我们用一个 mask 对神经元向量(层)进行了一个元素级的操作，"),a("strong",[t._v("其中每个元素都是遵循伯努利分布")]),t._v("的随机变量，取值 0 或 1。")]),t._v(" "),a("p",[a("strong",[t._v("在 testing 或 inference 阶段，没有 dropout，所有的神经元都是活跃的")]),t._v("。为了补偿与训练阶段相比较的额外信息，我们用出现的概率来衡加权权重。所以神经元没有被忽略的概率，是“1 - p”。")]),t._v(" "),a("blockquote",[a("p",[t._v("一般我们会将dropout理解为“一种低成本的集成策略”，这是对的。具体过程大概可以这样理解。")]),t._v(" "),a("p",[t._v("经过上述置零操作后，我们可以认为 0 的那部分是被丢弃了，丢失了一部分信息。然而虽然信息丢失了，但生活还得继续呀，不对，是训练还得继续，所以就逼着模型用剩下的信息去拟合目标了。然而每次 dropout 是随机的，我们就不能侧重于某些节点了。所以总的来说就是——每次逼着模型用少量的特征学习，每次被学习的特征有不同，那么就是说，每个特征都应该对模型的预测有所贡献（而不是侧重部分特征，导致过拟合）。")]),t._v(" "),a("p",[t._v("最后预测的时候，就不 dropout 了，所以就等价于所有局部特征的平均（这次终于用上所有的信息了），理论上效果就变好了，过拟合也不严重了（因为风险平摊到了每个特征而不是部分特征上面）。")])]),t._v(" "),a("p",[t._v("Standard Dropout 的实现伪代码如下：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dropout")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    x: feature vector\n    p: 某个神经元被丢弃的概率，0 < p <=1\n    """')]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" 训练阶段"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        retain_prob "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" p\n        sample "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("binomial"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("retain_prob"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 生成一个 0-1 分布的向量，形状与 x 一样")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*=")]),t._v(" sample\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# testing or inference 阶段")]),t._v("\n         x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*=")]),t._v(" retain_prob\n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br")])]),a("details",{staticClass:"custom-block details"},[a("summary",[t._v("pytorch 使用 Dropout 的示例")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n\n\ninput_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v("   \nhidden_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),t._v("   \nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("    \n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 三层神经网络")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("NeuralNet")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" input_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("NeuralNet"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入层到影藏层")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ReLU"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 影藏层到输出层")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dropout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dropout训练")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n   \n\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" NeuralNet"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hidden_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br")])])]),t._v(" "),a("h3",{attrs:{id:"_3-2-dropconnect"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-dropconnect"}},[t._v("#")]),t._v(" 3.2 DropConnect")]),t._v(" "),a("p",[t._v("也许你已经熟悉标准的 Dropout 方法。但也有很多变化。要对密集网络的前向传递进行正则，可以在神经元上应用dropout。L. Wan等人介绍的 "),a("mark",[t._v("DropConnect")]),t._v(" 没有直接在神经元上应用 dropout，而是"),a("strong",[t._v("在连接这些神经元的权重和偏置上进行屏蔽与激活")]),t._v("。")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215223303351.png",alt:"image-20221215223303351"}}),t._v(" "),a("ul",[a("li",[t._v("Standard dropout 从特征的角度进行 drop，在反向传播时依然对所有权重都进行更新；")]),t._v(" "),a("li",[t._v("DropConnect 对权重进行 drop，在反向传播过程中使用相应的 mask 屏蔽相应的权重，因此只对部分权重进行更新。")])]),t._v(" "),a("p",[t._v("DropConnect "),a("strong",[t._v("只能用于全连接层")]),t._v("，原理如下：")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221215224307235.png",alt:"image-20221215224307235"}})]),t._v(" "),a("p",[t._v("从上面的图中可以看到，一个含有 DropConnect 的 network 的运算过程如下：")]),t._v(" "),a("ul",[a("li",[t._v("输入 input "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1)],1)],1)],1),t._v(" "),a("li",[t._v("特征提取器从 input x 中提取 feature v")]),t._v(" "),a("li",[a("strong",[t._v("随机产生一个 0/1 的矩阵 mask，记为 M")])]),t._v(" "),a("li",[a("strong",[t._v("使用 Mask 对权重 W 进行相乘得到 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"W"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1)],1)],1),t._v("，从而屏蔽掉某些权重")],1)]),t._v(" "),a("li",[t._v("使用 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"W"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1)],1)],1),t._v(" 与 feature v 进行矩阵相乘，得到 vector u，u 就是 activation function 的输入数据")],1),t._v(" "),a("li",[t._v("经过 activation function 和 softmax 后得到预测值")])]),t._v(" "),a("p",[t._v("加粗的步骤是 DropConnect 的特别之处，由此可见，DropConnect 就是在训练阶段通过对网络权重进行 mask 操作实现网络正则化。")]),t._v(" "),a("p",[t._v("DropConnect 的 inference 阶段中，需要对每个权重从一个 Gaussian 分布中进行 sample，算法如下（看不懂就算了）：")]),t._v(" "),a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221217232525942.png",alt:"image-20221217232525942"}}),t._v(" "),a("p",[t._v("正是由于需要多次 sample，因此速度较慢，"),a("strong",[t._v("一般不会在产业中应用")]),t._v("，这也是为何我们基本上没用过 DropConnect 的原因。")]),t._v(" "),a("h3",{attrs:{id:"_3-3-standout"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-standout"}},[t._v("#")]),t._v(" 3.3 Standout")]),t._v(" "),a("p",[t._v("L. J. Ba 和 B. Frey 介绍的 "),a("mark",[t._v("Standout")]),t._v(" 旨在通过选择神经元来自适应地而不是随机地省略来改进标准的 dropout。 这是通过在控制神经网络结构的神经网络上叠加一个二元信念网络来实现的。 对于原始神经网络中的每个权重 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"W"}})],1)],1)],1),t._v("，Standout 在二元信念网络中添加一个相应的权重参数 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"W"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1)],1)],1),t._v("，并借助这个网络来生成一个 Bernoulli mask（我们将根据它们遵循的分布来命名这些 mask，这样会更简单）。在训练阶段，layer 的 output 由下式计算得出：")],1),t._v(" "),a("p"),a("p",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[a("mjx-math",{staticClass:" MJX-TEX",attrs:{display:"true"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"f"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-TeXAtom",[a("mjx-mtext",{staticClass:"mjx-b"},[a("mjx-c",{attrs:{c:"W"}})],1)],1),a("mjx-TeXAtom",[a("mjx-mtext",{staticClass:"mjx-b"},[a("mjx-c",{attrs:{c:"x"}})],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"2218"}})],1),a("mjx-TeXAtom",{attrs:{space:"3"}},[a("mjx-mtext",{staticClass:"mjx-b"},[a("mjx-c",{attrs:{c:"m"}})],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mtext",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"A0"}})],1),a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"i"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"223C"}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"B"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"r"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"o"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"u"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"l"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"l"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"i"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"g"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-msub",[a("mjx-TeXAtom",[a("mjx-mtext",{staticClass:"mjx-b"},[a("mjx-c",{attrs:{c:"W"}})],1)],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),a("mjx-TeXAtom",[a("mjx-mtext",{staticClass:"mjx-b"},[a("mjx-c",{attrs:{c:"x"}})],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1),a("p"),t._v(" "),a("ul",[a("li",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"W"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1)],1)],1),t._v(" 代表信念网络对该层的权重，g() 代表信念网络的激活函数。")],1)]),t._v(" "),a("p",[t._v("虽然可以应用单独的学习算法来学习信念网络权重，但在实践中，作者发现这导致信念网络权重变得近似等于相应神经网络权重的仿射函数。 因此，确定信念网络权重的有效方法是将其设置为：")]),t._v(" "),a("p"),a("p",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[a("mjx-math",{staticClass:" MJX-TEX",attrs:{display:"true"}},[a("mjx-msub",[a("mjx-TeXAtom",[a("mjx-mtext",{staticClass:"mjx-b"},[a("mjx-c",{attrs:{c:"W"}})],1)],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"s"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"3B1"}})],1),a("mjx-TeXAtom",[a("mjx-mtext",{staticClass:"mjx-b"},[a("mjx-c",{attrs:{c:"W"}})],1)],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"3B2"}})],1)],1)],1)],1),a("p"),t._v(" "),a("ul",[a("li",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B1"}})],1)],1)],1),t._v(" 和 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B2"}})],1)],1)],1),t._v(" 是常数")],1)]),t._v(" "),a("p",[t._v("由此可以看到，权重 W 越大，神经元被丢弃的概率就越大。这有力地限制了某些神经元可能具有的高预测能力。")]),t._v(" "),a("center",[a("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218001412819.png",alt:"image-20221218001412819"}})]),t._v(" "),a("h3",{attrs:{id:"_3-4-gaussian-dropout"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-gaussian-dropout"}},[t._v("#")]),t._v(" 3.4 Gaussian Dropout")])],1)}),[],!1,null,null,null);s.default=r.exports}}]);