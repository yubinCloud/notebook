(window.webpackJsonp=window.webpackJsonp||[]).push([[82],{835:function(t,a,s){"use strict";s.r(a);var i=s(22),m=Object(i.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"_1-pointer-network"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-pointer-network"}},[t._v("#")]),t._v(" 1. Pointer Network")]),t._v(" "),s("p",[t._v("Pointer Network 的提出是源自于这样一个问题：给了一堆 data point，需要能够自动从中找出将哪些点连起来就可以把其余的点都包起来。如下图所示：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204213150357.png",alt:"image-20221204213150357"}})]),t._v(" "),s("p",[t._v("如果让 Neural Network 硬 train 一发的话，NN 的 input 是一个 data point 的 coordinate "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"x"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"y"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("，输出是所要连接起来的点，如下图所示：")],1),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204213345156.png",alt:"image-20221204213345156"}})]),t._v(" "),s("p",[t._v("这个 NN 的 input 是一个 sequence，output 是另一个 sequence，那这似乎可以用 seq2seq 来解，如下所示：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204214619128.png",alt:"image-20221204214619128"}})]),t._v(" "),s("ul",[s("li",[t._v("黄色向量代表一个 distribution，结果是从这个 distribution 中采样或 argmax 出一个结果。")])]),t._v(" "),s("p",[t._v("但这个方法是行不通的，原因在于，input 的 data points 的个数是会变化的，而 Decoder 无法处理这种变化的情况，它输出的 distribution 是固定长度的。")]),t._v(" "),s("p",[t._v("怎么办呢？可以用 Attention 机制来对它做一下改造，让 Network 可以动态地决定它输出的 set 可以有多大。这是怎么做的呢？")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"60%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204220621458.png",alt:"image-20221204220621458"}})]),t._v(" "),s("ul",[s("li",[t._v("类似于 Self-Attention，每一个输入的 data point 首先得到 query，然后每个 query 与一个 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"x"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"y"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 对应的 key "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"z"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1)],1)],1),t._v(" 进行计算得到一排 attention weights 作为 distribution，接下来与以往 self-attention 不一样，只需要从这个 weights 组成的 distribution 中做 argmax 就可以得到 output 的第一项。")],1),t._v(" "),s("li",[t._v("接下来再把 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"x"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"y"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 所对应的 key "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"z"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v(" 做上面的过程，得到 output 的第二项，之后不断重复。这个过程直到 “END” 拥有最大的 attention weights 才会结束。")],1),t._v(" "),s("li",[t._v("现在的"),s("strong",[t._v("好处")]),t._v("就是：What decoder can output depends on the input.")])]),t._v(" "),s("p",[t._v("所以总结一下，传统的带有注意力机制的 seq2seq 模型的运行过程是这样的，先使用 encoder 部分对输入序列进行编码，然后对编码后的向量做 attention，最后使用 decoder 部分对 attention 后的向量进行解码从而得到预测结果。但是作为 Pointer Networks，得到预测结果的方式便是输出一个概率分布，也即所谓的指针。换句话说，"),s("strong",[t._v("传统带有注意力机制的 seq2seq 模型输出的是针对输出词汇表的一个概率分布，而 Pointer Networks 输出的则是针对输入文本序列的概率分布")]),t._v("。")]),t._v(" "),s("p",[t._v("其实我们可以发现，因为输出元素来自输入元素的特点，"),s("strong",[t._v("Pointer Networks 特别适合用来直接复制输入序列中的某些元素给输出序列")]),t._v("。而事实证明，后来的许多文章也确实是以这种方式使用 Pointer Networks 的。")]),t._v(" "),s("h2",{attrs:{id:"_2-applications"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-applications"}},[t._v("#")]),t._v(" 2. Applications")]),t._v(" "),s("p",[t._v("Pointer Network 的主要好处有：")]),t._v(" "),s("ol",[s("li",[t._v("提供了一种新视角去理解 Attention，把 Attention 作为一种求分布的手段。")]),t._v(" "),s("li",[t._v("对于输出字典长度不固定问题提供了一种新的解决方案。")]),t._v(" "),s("li",[t._v("将输入作为输出的一种补充手段，让输出部分可以更好的引入输入部分的信息。")])]),t._v(" "),s("h3",{attrs:{id:"_2-1-applications-summarization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-applications-summarization"}},[t._v("#")]),t._v(" 2.1 Applications - Summarization")]),t._v(" "),s("p",[t._v("Pointer Network 还是蛮适合用在 Summarization 的任务中。")]),t._v(" "),s("blockquote",[s("p",[t._v("Summarization is the task of condensing a piece of text to a shorter version that contains the main information from the original.")]),t._v(" "),s("p",[t._v("—— See A, Liu P J, Manning C D. Get To The Point: Summarization with Pointer-Generator Networks[J]. 2017:1073-1083.")])]),t._v(" "),s("p",[t._v("尽管这个任务可以用 seq2seq 来做，input 是一个 document，output 是 summary，但是今天在做 summary 的时候，很多 summary 里面放的词汇都是人名地名等这些专有名词，这往往做不好。")]),t._v(" "),s("p",[t._v("换一种思路，在做 summary 的时候，你可以想象 summary 与 document 的关系就是，从 document 中取出一些重要的词汇，然后接起来就是 summary 了。所以在产生 summary 的时候，让 machine 直接从 document 挑字出来组成 summary 就会得到不错的效果。")]),t._v(" "),s("p",[t._v("传统 sumarization 的做法如下（"),s("a",{attrs:{href:"https://arxiv.org/abs/1704.04368",target:"_blank",rel:"noopener noreferrer"}},[t._v("paper"),s("OutboundLink")],1),t._v("）：")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"75%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204222238318.png",alt:"image-20221204222238318"}})]),t._v(" "),s("p",[t._v("可以使用 Pointer network 对它进行改进。")]),t._v(" "),s("blockquote",[s("p",[t._v("可以参考文章 https://blog.csdn.net/qq_44766883/article/details/111995364")])]),t._v(" "),s("h3",{attrs:{id:"_2-2-more-applications"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-more-applications"}},[t._v("#")]),t._v(" 2.2 More Applications")]),t._v(" "),s("center",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204222617464.png",alt:"image-20221204222617464"}})])],1)}),[],!1,null,null,null);a.default=m.exports}}]);