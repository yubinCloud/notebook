---
title: 分布式系统的挑战
date: 2023-02-04 00:04:15
permalink: /pages/DDIA/note/consistency-and-consensus/
categories:
  - 开发
  - 软件设计与架构
  - 数据密集型应用系统设计
tags:
  - 
---

> 本章的线性一致性是在铺垫了多副本、网络问题、时钟问题后的一个综合探讨。首先探讨了线性一致的内涵：让系统表现得好像只有一个数据副本。然后讨论如何实现线性一致性，以及背后所做出的的取舍考量。其间花了一些笔墨探讨 CAP，可以看出作者很不喜欢 CAP 的模糊性。

由于分布式系统可能存在各种问题，本章我们将会讨论一些用于构建**具有容错性**分布式系统的**算法**和**协议**。

构建一个容错系统最好的方法是：**找到一些基本抽象，可以对上提供某些承诺，应用层可以依赖这些承诺来构建系统，而不必关心底层细节**。之前介绍的事务就是一种抽象机制并对外提供一些承诺，从而简化应用层。

本章将继续讨论一些可以减轻应用层负担的分布式系统中的基本抽象。例如，分布式系统最重要的抽象之一就是**共识**：让所有的节点在某件事情上达成一致。

> 为什么共识协议如此重要呢？他和真实系统的连接点在于哪里？答曰，**操作日志**。而大部分数据系统都可以抽象为一系列数据操作的依次施加，即状态机模型。而共识协议可以让多机对某个确定的操作序列达成共识，进而对系统的任意状态达成共识。

在讨论共识之前，我们需要探索下分布式系统中我们可以提供的保证和抽象有哪些，并了解系统能力的边界，即哪些可行，哪些不可行。

> 分布式系统领域针对这些主题的研究已经持续了数十载，因此积累了很多材料，但我们只能进行简要介绍其皮毛。由于篇幅所限，我们不会详细探究其严谨的模型描述和详细证明，相反，我们只会给一些其背后的直觉（informal intuitions）。如果你感兴趣，章节末尾的参考文献应该可以提供一些足够深入的细节。

## 1. 一致性保证

在“数据复制”一章中，我们知道了，在相同时刻，由于时间差的问题，多副本之间可能存在不一致性。无论我们使用什么数据副本模型（单主、多主和无主），这种数据的不一致性都有可能会发生。大部分多副本数据库会提供最终一致性的保证，即所有的副本最终会收敛到相同的值。但最终一致性是一个很弱的保证，对应用开发者很不友好。因为它表现出和单线程程序中的变量不一样的行为：赋值之后仍可能读到旧值。

> 在使用只提供弱保证的数据库时，我们需要**时刻记得其限制**，而不能偶尔自己增加额外假设，否则，会产生非常致命且难以察觉的 BUG。

本章将探究更强的一致性模型，更强的保证可以让上层应用的逻辑更简单，但也会牺牲性能或可用性。因此需要我们去进行取舍。

本章涉及到很多主题，乍看起来很宽泛，但其内里是互相勾连的：

1. 首先，我们从常用的最强的一致性模型：线性一致性（linearizability）开始，探究其优缺点。
2. 接着，我们会考察分布式系统中时间的顺序问题，尤其是关于因果关系（causality）和全序问题（total ordering）。
3. 最后，在第三部分，我们会探索如何原子性的提交一个分布式事务，最终导出共识问题的解决方法。

## 2. 线性一致性

<mark>线性一致性</mark>（linearizability）的基本思想：**让一个数据系统看起来好像只有一个数据副本，且所有的操作都是原子的**。这样就可以表现出只让时间线推进，而不让时间线倒流。有了这个保证，应用程序就不需要关系系统内部的多个副本了。换句话说，线性一致性是一种数据**新鲜度的保证**。

> 线性一致性还有很多其他称谓：原子一致性（atomic consistency）、强一致性（strong consistency）、即时一致性（immediate consistency），或者外部一致性（external consistency）。

### 2.1 非线性一致性的例子

我们看一个非线性一致性系统的例子：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205012453.png" alt="20230205012453" style="zoom:75%;" /></center>

上图显示了一个非线性一致性的体育网站。Alice 和 Bob 在一间屋子里，分别通过手机来查看 2014 年国际足联世界杯的总决赛的结果。在最终比分出来后，Alice 刷新了网页，并且看到了发布的赢家信息，并且将该结果告诉了 Bob。Bob 有点难以置信，重新刷了一下网页，但是他的请求被打到了一个滞后的数据库副本上，该副本显示比赛仍在进行。

如果 Alice 和 Bob 同时（也就是并发）刷新网页，可能还不会对出现不同结果有太多惊讶，毕竟他们也不知道谁的查询请求先到（因为并发）。但，上述例子中，Bob 是在 Alice 告知他结果后刷新的网页，因此他才会期待至少能看到和 Alice 一样新的结果。**该例子中 Bob 的请求返回了一个过期的结果，这便是违反了线性一致性**。

### 2.2 如何让系统满足线性一致性？

线性一致性背后的思想很简单：让系统表现得好像只有一个数据副本。为了理解这个概念，先看更多的例子。

下图显示了三个客户端并发访问提供线性一致性的数据库的同一个键。在分布式系统论文中，x 被称为“**寄存器**”（register）。在实践中，x 可以是一个键值存储中的键值对、关系型数据中的一行或者文档数据中的一个文档。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205012654.png" alt="20230205012654" style="zoom:95%;" /></center>

上图只显示了客户端角度数据读写视图：

+ Client C 有一个 write 操作，与 write 前后没有交集的 read 返回的结果是确定的
+ 当 read 与 write 存在交集时，读取的结果就是不确定的

由于当 read 与 write 存在交集时，查询结果可能在新值与旧值之间来回跳变，这不符合线性一致性的要求，因此需要增加约束：<font color=blue>一旦某个读操作返回了新值，之后的所有的读操作都必须返回新值</font>：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205013137.png" alt="20230205013137" style="zoom:95%;" /></center>

+ 如上图，Client A 的第二次 read 返回新值 1 后，Client B 的第二次 read 也必须返回新值 1。

> 在多副本数据库中，如果要解决线性一致性，就要满足一旦某个客户端读取到新值，则其之后的读请求一定能读到该新值，而不是还可能看到旧值。这很难，由于上一章讲的时钟问题，我们甚至很难对多个客户端定义“先后”。此外，**这种线性一致性的特性类似于薛定谔的猫**，本来可能有多个状态，但一旦有个一个客户端进行了一次观察，就迅速的坍缩到了一个状态，其他后来者，也只能看到这一个状态。从另外一个角度理解，是读取请求塑造（seal）了并发请求的多状态边界。

我们将这个时序图提炼一下，将所有请求生效时间都压缩到一个点，一次请求的操作在这个时间点才执行并瞬间完成：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205013428.png" alt="20230205013428" style="zoom:95%;" /></center>

+ `cas(x, v-old, v-new) ⇒ r` 表示一个 CAS 请求，只有当寄存器 x 的值为 v-old 时，才会被更新为 v-new。r 是返回值，指示是否更新成功。

如果将所有生效时间点连成一条线，线性一致性要求所有操作标记组**成序列是永远向前的**，即满足数**据新鲜度要求**：<u>一旦我们写入或者读取到某值，所有稍后的读请求都能看到该值，直到有人再次将其改写</u>。

这就是线性一致性背后的一些直觉。

除此之外，上面这个图有几个有意思的点：

+ Client B 的最后一次 read 读到了旧值，因此不满足线性一致性
+ **这个模型对事务隔离性没做任何要求**，因为按照快照隔离的要求，若 client C 的前两次 read 是一个事务的话，那应该读取到同样的结果，也就是可重复读。

::: note 线性一致性 vs 可串行化
线性一致性和可串行化容易发生混淆。

+ **线性一致性**：这个概念时由于多副本而引出的，是读写寄存器（**单个对象**）的最新值保证；
+ **可串行化**：这个概念是由于一个事务有多个操作并可以假设像独占系统一样来引出的，它是事务的一个隔离属性，其中每个事务也往往涉及到读写**多个对象**。

需要注意的是，在可串行化中，如果某种串行顺序和实际执行顺序不一致也没事，只要是串行执行就行。举个例子，如果 A、B、C 三个事务并发执行，真实顺序是 A、B、C，但如果对应用层表现为 CAB 的执行顺序（可能由于多机时间戳不同步），也可以叫可串行化，但 CAB 的执行顺序在某个对象上可能不满足线性一致性。

一个数据库可以同时提供可串行化和线性一致性保证，我们称之为**严格可串行化**（strict serializability）或者单副本可串行化（strong one-copy serializability）。使用两阶段锁或者真正串行化执行实现的可串行化，通常都是线性一致的。*但实际上这个级别的数据库通常性能会不太好，所以实际上我们还是选择放松要求*。

然而，**基于快照隔离的串行化通常不是线性一致的**。为了避免读写互相阻塞，所有的读取都会基于某个一致性的快照，则该快照之后的写入不会反映到读请求上，因此，快照读不满足线性一致性。
:::

### 2.3 依赖线性一致性的例子

下面讲了一些需要线性一致性的例子。

#### 2.3.1 加锁和主节点选举

主从复制的额系统需要确保只有一个主节点，否则就会脑裂。**选举主节点的常见方法是使用锁**：每个节点在启动时都试图去获取锁，最终只有一个节点会成功并且变为主节点。

不论使用什么方式实现锁，都**必须满足线性一致性**：所有节点必须就某节点拥有锁达成一致，否则这样的锁服务是不能用的。

提供协调者服务的系统，如 Apache Zookeeper 和 ectd 等通常用来实现分布式锁和主节点选取，他们通常使用共识算法来实现线性一致性操作，并且能够进行容错。

#### 2.3.2 约束和唯一性保证

**唯一性约束**在数据库中很常见：

+ userId 要求具有唯一性
+ 文件的绝对路径要求在一个文件系统中具有唯一性

如果你想要在数据写入时**维持这些约束**，你需要线性一致性。

> 这个情形和锁的语义非常类似：当一个用户注册时，可以认为他获得了一个和所注册的用户名关联的“锁”。这个操作很像原子的 CAS（compare-and-set）：如果该用户名没有被使用，就将其分配给该用户。

类似的约束还有：

+ 保证银行账户余额不出现负值
+ 航班座位不能超卖

这些约束都要求所有节点在**单个最新值**（账户余额、股票水位、座位预定）上**达成一致**。

> 当然，在真实场景下，有时这些约束课可以被适当放宽（比如，如果机票座位被超订了，可以将其中一个用户移到其他航班，并给与适当补偿）。在这种情况下，可能不需要严格的线性一致性。

#### 2.3.3 多渠道的时序依赖

在图 9-1 中我们可以注意到一个细节：如果 Alice 没有说出决赛结果，Bob 就不会知道他看到的是过时的结果。如果 Bob 没有从 Alice 那里事先知道结果，他可能就会过几秒再刷新一次页面，最终会看到最终分数。也就是说，因为存在一个**额外的通信渠道**，导致我们注意到了系统不满足线性一致性。

这说明，**一个 Web client 如果观测到系统不满足线性一致性，通常就是有多个通信渠道来获取信息**。如果没有线性一致性的保证，那么多个通信渠道的信息就会发生不一致。如果你可以控制所有的通信渠道，就可以使用类似“读你所写”的技术来解决这种竞态条件。

::: details 一个示例

比如下面这个例子：我们有一个可以让用户上传照片的网站，有个后台进程会将照片进行压缩以支持快速加载，架构图如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205015850.png" alt="20230205015850" style="zoom:75%;" /></center>

图片调整服务（image resizer）需要显式的指定任务，任务指令是通过消息队列由 web 服务器发给图片调整服务。但由于消息队列是针对短小消息（1kb 以下）而设计的，而图片通常有数 M，因此不能直接将图片发送到消息队列。而是，首先将图片写入文件存储服务（File Storage Service），然后将包含该文件路径的调整请求发送到消息队列中。

如果文件存储服务是线性一致的，则这个系统能正常运作。但如果他不是，则可能会存在竞态条件：消息队列可能会比文件存储服务内部多副本同步要快。在这种情况下，当图片调整服务去文件存储服务中捞照片时，就会发现一个旧照片、或者照片不存在。如果调整服务看到的是旧照片，却以为是新的，然后把它调整了并且存回了存储服务，就会出现永久的不一致。

出现这种情况是因为在 web 服务器和图片调整服务中间存在两条不同的通信渠道（communication channels）：存储系统和消息队列。如果没有线性一致性提供的新鲜度保证，两条通信渠道就有可能发生竞态条件（race condition）。
:::

### 2.4 实现了线性一致的系统

我们已经看了一些依赖线性一致性的例子，接下来让我们思考下如何实现一个提供线性一致语义的系统。

线性一致性的本质是在说：**系统表现得像只有一个数据副本，且所有施加于其上的操作都会原子性（瞬间）的完成**。那么，我们最简单的实现方式就是真的只用一个数据副本。但其问题在于，不能容错：一旦该副本挂了，轻则长时间（重启之前）不可用、重则数据丢失。

最常用的让系统进行容错的方式就是多副本。让我们回顾下几种多副本模型，然后逐一考察下其是否能够做成可线性化的：

+ **单主模型**（主从复制模型）：部分支持线性一致。当数据读取是从主节点或者同步更新的从节点上读取时，就可以满足线性一致性。
+ **共识算法**：线性一致。有一些共识算法，看起来与单主模型类似。但这些共识协议有一些阻止脑裂和过期副本的手段。由于这些额外细节，共识算法可以实现安全的线性一致性存储。Zookeeper 和 etcd 就是用的这种手段。
+ **多主模型**：不可线性一致。因为它可以同时在多个节点上处理写入，并且异步同步写入数据。
+ **无主模型**：可能不能线性一致。这完全取决于 quorum 的配置，以及如何定义强一致性，它可能并不保证线性一致。即使对于严格的法定策略，非线性一致的现象也可能出现。

#### 2.4.1 线性一致和 quorum

从直觉出发，在 Dynamo 风格的系统重使用严格的 Quorum 读写应该会满足线性一致性。但在具有不确定延迟的网络中，仍然可能会出现竞态条件。如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205021636.png" alt="20230205021636" style="zoom:75%;" /></center>

在图 9-6 中，x 的初始值是 0。然后一个客户端想将 x 更新为 1，然后将该写请求发送到所有三个副本（n=3, w=3）。与此同时，客户端 A 使用 r = 2 的配置进行 Quorum 读，并且看到了新值 1。稍后，客户端 B 也是用 r = 2 的配置在另外两个节点进行 Quorum 读，但却读到了旧值 0。

Quorum 的配置是严格满足 w+r>n 的，然而这个读写序列却不是线性一致的：**B 的读取请求开始于 A 的读请求结束之后，却读到了比 A 旧的值**。

当然，有趣的是，我们可以通过牺牲部分性能来让 Dynamo 风格的 Quorum 读写变成线性一致的：

1. 每个读请求必须进行同步的读取修复。
2. 发送任意写请求之前要先读取最新值。

> 但由于性能原因 Riak 并没有采用同步的读取修复；Cassandra 倒是会同步读取修复，但在多个请求并发写入同一个 key 时，由于采用了后者胜的策略（考虑时钟，会导致接受顺序不是真正事件发生顺序），仍然不能保持线性一致性。此外，这种方式只能实现线性一致的读写操作，而不能实现线性一致的 CAS 操作。只有共识协议才能实现线性一致的 CAS。

总结来说，**最好认为基于无主模型的 Dynamo 风格的系统不提供线性一致性保证**。

### 2.5 线性一致性的代价

线性一致性的代价，如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205021906.png" alt="20230205021906" style="zoom:75%;" /></center>

对于上图的系统，当两个数据中心之间发生网络中断时，

+ 如果要提供线性一致性，则两个数据中心都不可用；
+ 如果不提供线性一致性，那两个数据中心都是可用的，

因此，<font color=blue>网络中断迫使在可线性化和可用性之间做出选择</font>。

#### 2.5.1 CAP 理论

上面的问题不只是在主从复制和多主复制上才有的问题，**无论如何实现，任何想要提供线性一致性的系统都会面临上述取舍问题**。该问题也不止局限于跨数据中心部署，即使是在一个数据中心之内，任何通过不可靠网络连接的系统都会有该问题。其背后的**取舍考量**如下：

+ 如果应用层要求系统提供线性一致性，此时如果某些数据副本由于网络问题和系统其他部分断开了连接，则这些数据副本就不再能够正常地处理请求：要么等待网络恢复、要么进行报错。但这都意味着系统不可用。
+ 如果应用不要求系统的线性一致，则即使多副本间遇到连接问题，每个副本可以独立的进行写入。从而，即使出现了网络故障，系统仍然能够保持可用，但其行为不是线性一致的。

总而言之，<font color=red>如果系统不提供线性一致性，就可以对网络故障更加鲁棒</font>。这种思路通常被称为 <mark>CAP 定理</mark>。

CAP 最初被提出只是一个为了激发数据库取舍讨论的模糊的取舍参考，而非被精确定义的定理，Martin 还专门写过一篇[文章](https://www.notion.so/Chapter-9-Consistency-and-Consensus-f80d66bdfb7b4d1281722914239a563a)来探讨这件事。在当时，很多分布式数据库还在着眼于基于共享存储的一组机器上提供线性一致性语义。<u>CAP 的提出，鼓励工程师们在 share-nothing 等更广阔的设计领域进行架构探索，以找出更加适合大规模可扩展 web 服务架构</u>。 在新世纪的最初十年里，CAP 的提出见证并推动了当时数据库设计思潮从强一致系统转向弱一致系统（也被称为 NoSQL 架构）。

CAP 定理的形式化定义适用范围很窄：仅包含一种一致性模型（即线性一致性）和一种故障类型（网络分区，或者说节点存活，但互不连通）。它没有进一步说明任何关于网络延迟、宕机节点、以及其他的一些取舍考量。因此，**尽管 CAP 在历史上很有影响力，但他在设计系统时缺乏实际有效指导力**，所以它现在更多是代表历史上曾经的一个关注热点而已。

::: note CAP 理论
CAP 有时候被表述为，在做系统设计时，一致性（consistency）、可用性（Availability）、分区容错性（Partition tolerance），只能三选二。然而，这种说法极具误导性，因为网络分区是一种故障类型，而不是一种可以取舍的选项：不管你喜欢还是不喜欢，只要是分布式系统，它都在那。

**在网络正常连通时，系统可以同时提供一致性（线性一致性）和完全的可用性**。当网络故障发生时，你必须在线性一致性和完全可用性之间二选一。因此，对于 CAP 更好的一个表述可能是：<font color=blue>**当网络出现分区时，一致性和可用性只能二选其一**</font>。一个可靠的网络，可以减少其上的系统该选择的次数，但无论如何，分布式系统中，该选择是无法避免的。
:::

在有关 CAP 的讨论，有几种关于可用性的大相径庭的定义，且将 CAP 升格为定理并给出证明中的提到的形式化的可用性并非通常意义中所说的可用性。很多所谓“高可用”的系统通常并不符合 CAP 定理中关于可用性的独特（idiosyncratic）定义。总而言之，**CAP 有很多容易误解和模糊不清的概念**，并不能帮助我们更好的理解系统，因此**最好不用 CAP 来描述一个系统**。

#### 2.5.2 线性一致性和网络延迟

尽管线性一致性是一个非常有用的保证，但令人惊讶的是在工程实践中，很少有系统支持真正的线性一致。

> 甚而，**即使在现代多核 CPU 体系下的 RAM 也不是线性一致的**：如果一个核上的某个线程往某个内存地址中写了一个值，稍后另外核的一个线程读取该地址，并不一定能读到刚才的值。这是因为每个 CPU 都有自己的缓存（memory cache）和缓冲区（store buffer）。一般缓存通常说的是读取，而缓冲区通常针对写入。线程的内存访问会首先落到缓存里，所有对于缓存的更新会**异步同步**到主存中。缓存访问的速度要（ns 级别）比内存访问（百ns 级别）快几个数量级，由于可以用来弥合寄存器和主存的访问鸿沟，因此是现代 CPU 架构高性能的基石。但一份数据存了多个副本（比如主存中一个，一些 CPU 缓存中各有一个），且是异步更新的，导致线性一致性被破坏。

为什么会做此取舍？此处牺牲线性一致性的真正原因在于——性能，而不是容错。当然，在单机多线程编程中，可以使用一些手段（比如锁）来强制同步相应变量到主存，从而允许用户在关心一致性超过性能的地方，自行进行取舍。

**很多分布式系统选择不提供线性一致性的原因也在于此：是为了提升系统性能而非进行容错**。在任何时候，提供线性一致性都会严重拖慢系统。而非在网络故障发生时，才需要对线性一致性进行牺牲。

> 也就说，尽管 CAP 说了一致性和可用性只能二选一，但我们选择放松一致性的真正原因确实性能而非可用性。

我们能找到一种更高效的实现来让存储服务提供线性一致吗？遗憾的是，暂时没有。Attiya 和 Welch 证明了，**如果你想要保证线性一致，读写请求的响应时间是正比于网络延迟的**。提供线性一致性保证可能没有更快的算法，但是**我们稍微放松一致性，就可以设计出一个更快的系统**。这种取舍在对延迟敏感的系统非常重要。

## 3. 顺序保证
