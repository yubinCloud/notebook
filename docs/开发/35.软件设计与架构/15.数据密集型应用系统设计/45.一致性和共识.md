---
title: 一致性和共识
date: 2023-02-04 00:04:15
permalink: /pages/DDIA/note/consistency-and-consensus/
categories:
  - 开发
  - 软件设计与架构
  - 数据密集型应用系统设计
tags:
  - 
---

> 本章的线性一致性是在铺垫了多副本、网络问题、时钟问题后的一个综合探讨。首先探讨了线性一致的内涵：让系统表现得好像只有一个数据副本。然后讨论如何实现线性一致性，以及背后所做出的的取舍考量。其间花了一些笔墨探讨 CAP，可以看出作者很不喜欢 CAP 的模糊性。

由于分布式系统可能存在各种问题，本章我们将会讨论一些用于构建**具有容错性**分布式系统的**算法**和**协议**。

构建一个容错系统最好的方法是：**找到一些基本抽象，可以对上提供某些承诺，应用层可以依赖这些承诺来构建系统，而不必关心底层细节**。之前介绍的事务就是一种抽象机制并对外提供一些承诺，从而简化应用层。

本章将继续讨论一些可以减轻应用层负担的分布式系统中的基本抽象。例如，分布式系统最重要的抽象之一就是**共识**：让所有的节点在某件事情上达成一致。

> 为什么共识协议如此重要呢？他和真实系统的连接点在于哪里？答曰，**操作日志**。而大部分数据系统都可以抽象为一系列数据操作的依次施加，即状态机模型。而共识协议可以让多机对某个确定的操作序列达成共识，进而对系统的任意状态达成共识。

在讨论共识之前，我们需要探索下分布式系统中我们可以提供的保证和抽象有哪些，并了解系统能力的边界，即哪些可行，哪些不可行。

> 分布式系统领域针对这些主题的研究已经持续了数十载，因此积累了很多材料，但我们只能进行简要介绍其皮毛。由于篇幅所限，我们不会详细探究其严谨的模型描述和详细证明，相反，我们只会给一些其背后的直觉（informal intuitions）。如果你感兴趣，章节末尾的参考文献应该可以提供一些足够深入的细节。

## 1. 一致性保证

在“数据复制”一章中，我们知道了，在相同时刻，由于时间差的问题，多副本之间可能存在不一致性。无论我们使用什么数据副本模型（单主、多主和无主），这种数据的不一致性都有可能会发生。大部分多副本数据库会提供最终一致性的保证，即所有的副本最终会收敛到相同的值。但最终一致性是一个很弱的保证，对应用开发者很不友好。因为它表现出和单线程程序中的变量不一样的行为：赋值之后仍可能读到旧值。

> 在使用只提供弱保证的数据库时，我们需要**时刻记得其限制**，而不能偶尔自己增加额外假设，否则，会产生非常致命且难以察觉的 BUG。

本章将探究更强的一致性模型，更强的保证可以让上层应用的逻辑更简单，但也会牺牲性能或可用性。因此需要我们去进行取舍。

本章涉及到很多主题，乍看起来很宽泛，但其内里是互相勾连的：

1. 首先，我们从常用的最强的一致性模型：线性一致性（linearizability）开始，探究其优缺点。
2. 接着，我们会考察分布式系统中时间的顺序问题，尤其是关于因果关系（causality）和全序问题（total ordering）。
3. 最后，在第三部分，我们会探索如何原子性的提交一个分布式事务，最终导出共识问题的解决方法。

## 2. 线性一致性

<mark>线性一致性</mark>（linearizability）的基本思想：**让一个数据系统看起来好像只有一个数据副本，且所有的操作都是原子的**。这样就可以表现出只让时间线推进，而不让时间线倒流。有了这个保证，应用程序就不需要关系系统内部的多个副本了。换句话说，线性一致性是一种数据**新鲜度的保证**。

> 线性一致性还有很多其他称谓：原子一致性（atomic consistency）、强一致性（strong consistency）、即时一致性（immediate consistency），或者外部一致性（external consistency）。

### 2.1 非线性一致性的例子

我们看一个非线性一致性系统的例子：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205012453.png" alt="20230205012453" style="zoom:75%;" /></center>

上图显示了一个非线性一致性的体育网站。Alice 和 Bob 在一间屋子里，分别通过手机来查看 2014 年国际足联世界杯的总决赛的结果。在最终比分出来后，Alice 刷新了网页，并且看到了发布的赢家信息，并且将该结果告诉了 Bob。Bob 有点难以置信，重新刷了一下网页，但是他的请求被打到了一个滞后的数据库副本上，该副本显示比赛仍在进行。

如果 Alice 和 Bob 同时（也就是并发）刷新网页，可能还不会对出现不同结果有太多惊讶，毕竟他们也不知道谁的查询请求先到（因为并发）。但，上述例子中，Bob 是在 Alice 告知他结果后刷新的网页，因此他才会期待至少能看到和 Alice 一样新的结果。**该例子中 Bob 的请求返回了一个过期的结果，这便是违反了线性一致性**。

### 2.2 如何让系统满足线性一致性？

线性一致性背后的思想很简单：让系统表现得好像只有一个数据副本。为了理解这个概念，先看更多的例子。

下图显示了三个客户端并发访问提供线性一致性的数据库的同一个键。在分布式系统论文中，x 被称为“**寄存器**”（register）。在实践中，x 可以是一个键值存储中的键值对、关系型数据中的一行或者文档数据中的一个文档。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205012654.png" alt="20230205012654" style="zoom:95%;" /></center>

上图只显示了客户端角度数据读写视图：

+ Client C 有一个 write 操作，与 write 前后没有交集的 read 返回的结果是确定的
+ 当 read 与 write 存在交集时，读取的结果就是不确定的

由于当 read 与 write 存在交集时，查询结果可能在新值与旧值之间来回跳变，这不符合线性一致性的要求，因此需要增加约束：<font color=blue>一旦某个读操作返回了新值，之后的所有的读操作都必须返回新值</font>：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205013137.png" alt="20230205013137" style="zoom:95%;" /></center>

+ 如上图，Client A 的第二次 read 返回新值 1 后，Client B 的第二次 read 也必须返回新值 1。

> 在多副本数据库中，如果要解决线性一致性，就要满足一旦某个客户端读取到新值，则其之后的读请求一定能读到该新值，而不是还可能看到旧值。这很难，由于上一章讲的时钟问题，我们甚至很难对多个客户端定义“先后”。此外，**这种线性一致性的特性类似于薛定谔的猫**，本来可能有多个状态，但一旦有个一个客户端进行了一次观察，就迅速的坍缩到了一个状态，其他后来者，也只能看到这一个状态。从另外一个角度理解，是读取请求塑造（seal）了并发请求的多状态边界。

我们将这个时序图提炼一下，将所有请求生效时间都压缩到一个点，一次请求的操作在这个时间点才执行并瞬间完成：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205013428.png" alt="20230205013428" style="zoom:95%;" /></center>

+ `cas(x, v-old, v-new) ⇒ r` 表示一个 CAS 请求，只有当寄存器 x 的值为 v-old 时，才会被更新为 v-new。r 是返回值，指示是否更新成功。

如果将所有生效时间点连成一条线，线性一致性要求所有操作标记组**成序列是永远向前的**，即满足数**据新鲜度要求**：<u>一旦我们写入或者读取到某值，所有稍后的读请求都能看到该值，直到有人再次将其改写</u>。

这就是线性一致性背后的一些直觉。

除此之外，上面这个图有几个有意思的点：

+ Client B 的最后一次 read 读到了旧值，因此不满足线性一致性
+ **这个模型对事务隔离性没做任何要求**，因为按照快照隔离的要求，若 client C 的前两次 read 是一个事务的话，那应该读取到同样的结果，也就是可重复读。

::: note 线性一致性 vs 可串行化
线性一致性和可串行化容易发生混淆。

+ **线性一致性**：这个概念时由于多副本而引出的，是读写寄存器（**单个对象**）的最新值保证；
+ **可串行化**：这个概念是由于一个事务有多个操作并可以假设像独占系统一样来引出的，它是事务的一个隔离属性，其中每个事务也往往涉及到读写**多个对象**。

需要注意的是，在可串行化中，如果某种串行顺序和实际执行顺序不一致也没事，只要是串行执行就行。举个例子，如果 A、B、C 三个事务并发执行，真实顺序是 A、B、C，但如果对应用层表现为 CAB 的执行顺序（可能由于多机时间戳不同步），也可以叫可串行化，但 CAB 的执行顺序在某个对象上可能不满足线性一致性。

一个数据库可以同时提供可串行化和线性一致性保证，我们称之为**严格可串行化**（strict serializability）或者单副本可串行化（strong one-copy serializability）。使用两阶段锁或者真正串行化执行实现的可串行化，通常都是线性一致的。*但实际上这个级别的数据库通常性能会不太好，所以实际上我们还是选择放松要求*。

然而，**基于快照隔离的串行化通常不是线性一致的**。为了避免读写互相阻塞，所有的读取都会基于某个一致性的快照，则该快照之后的写入不会反映到读请求上，因此，快照读不满足线性一致性。
:::

### 2.3 依赖线性一致性的例子

下面讲了一些需要线性一致性的例子。

#### 2.3.1 加锁和主节点选举

主从复制的额系统需要确保只有一个主节点，否则就会脑裂。**选举主节点的常见方法是使用锁**：每个节点在启动时都试图去获取锁，最终只有一个节点会成功并且变为主节点。

不论使用什么方式实现锁，都**必须满足线性一致性**：所有节点必须就某节点拥有锁达成一致，否则这样的锁服务是不能用的。

提供协调者服务的系统，如 Apache Zookeeper 和 ectd 等通常用来实现分布式锁和主节点选取，他们通常使用共识算法来实现线性一致性操作，并且能够进行容错。

#### 2.3.2 约束和唯一性保证

**唯一性约束**在数据库中很常见：

+ userId 要求具有唯一性
+ 文件的绝对路径要求在一个文件系统中具有唯一性

如果你想要在数据写入时**维持这些约束**，你需要线性一致性。

> 这个情形和锁的语义非常类似：当一个用户注册时，可以认为他获得了一个和所注册的用户名关联的“锁”。这个操作很像原子的 CAS（compare-and-set）：如果该用户名没有被使用，就将其分配给该用户。

类似的约束还有：

+ 保证银行账户余额不出现负值
+ 航班座位不能超卖

这些约束都要求所有节点在**单个最新值**（账户余额、股票水位、座位预定）上**达成一致**。

> 当然，在真实场景下，有时这些约束课可以被适当放宽（比如，如果机票座位被超订了，可以将其中一个用户移到其他航班，并给与适当补偿）。在这种情况下，可能不需要严格的线性一致性。

#### 2.3.3 多渠道的时序依赖

在图 9-1 中我们可以注意到一个细节：如果 Alice 没有说出决赛结果，Bob 就不会知道他看到的是过时的结果。如果 Bob 没有从 Alice 那里事先知道结果，他可能就会过几秒再刷新一次页面，最终会看到最终分数。也就是说，因为存在一个**额外的通信渠道**，导致我们注意到了系统不满足线性一致性。

这说明，**一个 Web client 如果观测到系统不满足线性一致性，通常就是有多个通信渠道来获取信息**。如果没有线性一致性的保证，那么多个通信渠道的信息就会发生不一致。如果你可以控制所有的通信渠道，就可以使用类似“读你所写”的技术来解决这种竞态条件。

::: details 一个示例

比如下面这个例子：我们有一个可以让用户上传照片的网站，有个后台进程会将照片进行压缩以支持快速加载，架构图如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205015850.png" alt="20230205015850" style="zoom:75%;" /></center>

图片调整服务（image resizer）需要显式的指定任务，任务指令是通过消息队列由 web 服务器发给图片调整服务。但由于消息队列是针对短小消息（1kb 以下）而设计的，而图片通常有数 M，因此不能直接将图片发送到消息队列。而是，首先将图片写入文件存储服务（File Storage Service），然后将包含该文件路径的调整请求发送到消息队列中。

如果文件存储服务是线性一致的，则这个系统能正常运作。但如果他不是，则可能会存在竞态条件：消息队列可能会比文件存储服务内部多副本同步要快。在这种情况下，当图片调整服务去文件存储服务中捞照片时，就会发现一个旧照片、或者照片不存在。如果调整服务看到的是旧照片，却以为是新的，然后把它调整了并且存回了存储服务，就会出现永久的不一致。

出现这种情况是因为在 web 服务器和图片调整服务中间存在两条不同的通信渠道（communication channels）：存储系统和消息队列。如果没有线性一致性提供的新鲜度保证，两条通信渠道就有可能发生竞态条件（race condition）。
:::

### 2.4 实现了线性一致的系统

我们已经看了一些依赖线性一致性的例子，接下来让我们思考下如何实现一个提供线性一致语义的系统。

线性一致性的本质是在说：**系统表现得像只有一个数据副本，且所有施加于其上的操作都会原子性（瞬间）的完成**。那么，我们最简单的实现方式就是真的只用一个数据副本。但其问题在于，不能容错：一旦该副本挂了，轻则长时间（重启之前）不可用、重则数据丢失。

最常用的让系统进行容错的方式就是多副本。让我们回顾下几种多副本模型，然后逐一考察下其是否能够做成可线性化的：

+ **单主模型**（主从复制模型）：部分支持线性一致。当数据读取是从主节点或者同步更新的从节点上读取时，就可以满足线性一致性。
+ **共识算法**：线性一致。有一些共识算法，看起来与单主模型类似。但这些共识协议有一些阻止脑裂和过期副本的手段。由于这些额外细节，共识算法可以实现安全的线性一致性存储。Zookeeper 和 etcd 就是用的这种手段。
+ **多主模型**：不可线性一致。因为它可以同时在多个节点上处理写入，并且异步同步写入数据。
+ **无主模型**：可能不能线性一致。这完全取决于 quorum 的配置，以及如何定义强一致性，它可能并不保证线性一致。即使对于严格的法定策略，非线性一致的现象也可能出现。

#### 2.4.1 线性一致和 quorum

从直觉出发，在 Dynamo 风格的系统重使用严格的 Quorum 读写应该会满足线性一致性。但在具有不确定延迟的网络中，仍然可能会出现竞态条件。如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205021636.png" alt="20230205021636" style="zoom:75%;" /></center>

在图 9-6 中，x 的初始值是 0。然后一个客户端想将 x 更新为 1，然后将该写请求发送到所有三个副本（n=3, w=3）。与此同时，客户端 A 使用 r = 2 的配置进行 Quorum 读，并且看到了新值 1。稍后，客户端 B 也是用 r = 2 的配置在另外两个节点进行 Quorum 读，但却读到了旧值 0。

Quorum 的配置是严格满足 w+r>n 的，然而这个读写序列却不是线性一致的：**B 的读取请求开始于 A 的读请求结束之后，却读到了比 A 旧的值**。

当然，有趣的是，我们可以通过牺牲部分性能来让 Dynamo 风格的 Quorum 读写变成线性一致的：

1. 每个读请求必须进行同步的读取修复。
2. 发送任意写请求之前要先读取最新值。

> 但由于性能原因 Riak 并没有采用同步的读取修复；Cassandra 倒是会同步读取修复，但在多个请求并发写入同一个 key 时，由于采用了后者胜的策略（考虑时钟，会导致接受顺序不是真正事件发生顺序），仍然不能保持线性一致性。此外，这种方式只能实现线性一致的读写操作，而不能实现线性一致的 CAS 操作。只有共识协议才能实现线性一致的 CAS。

总结来说，**最好认为基于无主模型的 Dynamo 风格的系统不提供线性一致性保证**。

### 2.5 线性一致性的代价

线性一致性的代价，如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205021906.png" alt="20230205021906" style="zoom:75%;" /></center>

对于上图的系统，当两个数据中心之间发生网络中断时，

+ 如果要提供线性一致性，则两个数据中心都不可用；
+ 如果不提供线性一致性，那两个数据中心都是可用的，

因此，<font color=blue>网络中断迫使在可线性化和可用性之间做出选择</font>。

#### 2.5.1 CAP 理论

上面的问题不只是在主从复制和多主复制上才有的问题，**无论如何实现，任何想要提供线性一致性的系统都会面临上述取舍问题**。该问题也不止局限于跨数据中心部署，即使是在一个数据中心之内，任何通过不可靠网络连接的系统都会有该问题。其背后的**取舍考量**如下：

+ 如果应用层要求系统提供线性一致性，此时如果某些数据副本由于网络问题和系统其他部分断开了连接，则这些数据副本就不再能够正常地处理请求：要么等待网络恢复、要么进行报错。但这都意味着系统不可用。
+ 如果应用不要求系统的线性一致，则即使多副本间遇到连接问题，每个副本可以独立的进行写入。从而，即使出现了网络故障，系统仍然能够保持可用，但其行为不是线性一致的。

总而言之，<font color=red>如果系统不提供线性一致性，就可以对网络故障更加鲁棒</font>。这种思路通常被称为 <mark>CAP 定理</mark>。

CAP 最初被提出只是一个为了激发数据库取舍讨论的模糊的取舍参考，而非被精确定义的定理，Martin 还专门写过一篇[文章](https://www.notion.so/Chapter-9-Consistency-and-Consensus-f80d66bdfb7b4d1281722914239a563a)来探讨这件事。在当时，很多分布式数据库还在着眼于基于共享存储的一组机器上提供线性一致性语义。<u>CAP 的提出，鼓励工程师们在 share-nothing 等更广阔的设计领域进行架构探索，以找出更加适合大规模可扩展 web 服务架构</u>。 在新世纪的最初十年里，CAP 的提出见证并推动了当时数据库设计思潮从强一致系统转向弱一致系统（也被称为 NoSQL 架构）。

CAP 定理的形式化定义适用范围很窄：仅包含一种一致性模型（即线性一致性）和一种故障类型（网络分区，或者说节点存活，但互不连通）。它没有进一步说明任何关于网络延迟、宕机节点、以及其他的一些取舍考量。因此，**尽管 CAP 在历史上很有影响力，但他在设计系统时缺乏实际有效指导力**，所以它现在更多是代表历史上曾经的一个关注热点而已。

::: note CAP 理论
CAP 有时候被表述为，在做系统设计时，一致性（consistency）、可用性（Availability）、分区容错性（Partition tolerance），只能三选二。然而，这种说法极具误导性，因为网络分区是一种故障类型，而不是一种可以取舍的选项：不管你喜欢还是不喜欢，只要是分布式系统，它都在那。

**在网络正常连通时，系统可以同时提供一致性（线性一致性）和完全的可用性**。当网络故障发生时，你必须在线性一致性和完全可用性之间二选一。因此，对于 CAP 更好的一个表述可能是：<font color=blue>**当网络出现分区时，一致性和可用性只能二选其一**</font>。一个可靠的网络，可以减少其上的系统该选择的次数，但无论如何，分布式系统中，该选择是无法避免的。
:::

在有关 CAP 的讨论，有几种关于可用性的大相径庭的定义，且将 CAP 升格为定理并给出证明中的提到的形式化的可用性并非通常意义中所说的可用性。很多所谓“高可用”的系统通常并不符合 CAP 定理中关于可用性的独特（idiosyncratic）定义。总而言之，**CAP 有很多容易误解和模糊不清的概念**，并不能帮助我们更好的理解系统，因此**最好不用 CAP 来描述一个系统**。

#### 2.5.2 线性一致性和网络延迟

尽管线性一致性是一个非常有用的保证，但令人惊讶的是在工程实践中，很少有系统支持真正的线性一致。

> 甚而，**即使在现代多核 CPU 体系下的 RAM 也不是线性一致的**：如果一个核上的某个线程往某个内存地址中写了一个值，稍后另外核的一个线程读取该地址，并不一定能读到刚才的值。这是因为每个 CPU 都有自己的缓存（memory cache）和缓冲区（store buffer）。一般缓存通常说的是读取，而缓冲区通常针对写入。线程的内存访问会首先落到缓存里，所有对于缓存的更新会**异步同步**到主存中。缓存访问的速度要（ns 级别）比内存访问（百ns 级别）快几个数量级，由于可以用来弥合寄存器和主存的访问鸿沟，因此是现代 CPU 架构高性能的基石。但一份数据存了多个副本（比如主存中一个，一些 CPU 缓存中各有一个），且是异步更新的，导致线性一致性被破坏。

为什么会做此取舍？此处牺牲线性一致性的真正原因在于——性能，而不是容错。当然，在单机多线程编程中，可以使用一些手段（比如锁）来强制同步相应变量到主存，从而允许用户在关心一致性超过性能的地方，自行进行取舍。

**很多分布式系统选择不提供线性一致性的原因也在于此：是为了提升系统性能而非进行容错**。在任何时候，提供线性一致性都会严重拖慢系统。而非在网络故障发生时，才需要对线性一致性进行牺牲。

> 也就说，尽管 CAP 说了一致性和可用性只能二选一，但我们选择放松一致性的真正原因确实性能而非可用性。

我们能找到一种更高效的实现来让存储服务提供线性一致吗？遗憾的是，暂时没有。Attiya 和 Welch 证明了，**如果你想要保证线性一致，读写请求的响应时间是正比于网络延迟的**。提供线性一致性保证可能没有更快的算法，但是**我们稍微放松一致性，就可以设计出一个更快的系统**。这种取舍在对延迟敏感的系统非常重要。

## 3. 顺序保证

线性一致性的定义暗含着：所有的操作会形成一个确定的执行顺序。**顺序**是本书不断强调的一个主题，它是一个很重要的基础概念。先回忆一下本书提到的有关顺序的上下文：

+ [数据复制](/pages/DDIA/note/replication/) 一章中，提到主从复制中就需要由主副本来确定复制日志的**写入顺序**，然后所有从副本都要遵从这个顺序。
+ [事务](/pages/DDIA/note/transaction/) 一章中，“可串行化”就是保证所有并发的事务像以某种顺序一样串行执行。
+ [分布式系统的挑战](/pages/DDIA/note/the-trouble-with-distrubuted-systems/) 中的时钟也是试图对无序的真实世界引入某种顺序，以解决诸如哪个写入更靠后之类的问题。

顺序性（ordering）、线性一致性（linearizability）和共识协议（consensus）三个概念间有很深的联系，尽管这几个概念比较偏理论和抽象，但理解他们有助于来厘清系统的功能边界——哪些可以做，哪些做不了。

### 3.1 顺序和因果（Ordering and Causality）

#### 3.1.1 什么是因果一致性？

**顺序可以维持因果性**。下面给出了一些体现“因果关系”重要性的例子：

+ 在一致性前缀读中，我们提到：问题和答案之间存在*因果*依赖（casual dependency）。我们应该先看到问题，才能看到答案。
+ 在数据复制的多主模型中，由于网络延迟可能出现对一个对象的写入操作居于更新操作后面的问题。这里*因果*意味着一个对象必须先被创建，然后才能去更新。
+ 在检测并发写时，判断”A发生在B之前“就是一种因果关系，这说明 B 依赖于 A
+ 事务从一致性快照中读取，这里的“一致性”就意味着因果关系一直，它对此后发生的事件不可见
+ 事务之间写倾斜的例子，图7-8中，Alice 申请调班成功是因为事务以为 Bob 仍在值班。
+ 违反线性一致性，导致 A 看到新值而后 B 看到旧值也是违背了因果关系。

**因果将顺序施加于事件**，也就是谁应该先发生，谁应该后发生：

1. 先有因，后有果
2. 先有消息发送，然后该消息被收到
3. 先有问题，后有答案

如果一个系统遵循因果约束，则我们称其为<mark>因果一致的</mark>（causally consistent）。比如，快照隔离就可以提供因果一致性：当从数据库读取数据的时候，如果你能读到某个时间点的数据，就一定能读到其之前的数据（当然，要在该数据还没有被删除的情况下）。

#### 3.1.2 因果序非全序

+ <mark>全序</mark>（total order）意味着任意两个元素之间都可以进行比较大小。比如整数的比大小。
+ <mark>偏序</mark>（partially order）意味着一个元素可以和一部分元素进行比较大小，但和另一部分是不可比的。比如数学集合的包含关系。

全序和偏序的差异也体现在数据库的一致性模型中：

+ **线性一致性**：由于它对外表现像所有操作都发生于单副本上，并且会原子性的完成。这意味着，任意两个操作，总是可以确定他们发生的先后关系。**所有操作是全序的**。
+ **因果一致性**：如果我们无从判定两个操作的先后关系，则称之为**并发的**。如果两个事件因果相关，则其一定有序。因此，**因果性定义了一种偏序关系**，而非全序的。

根据上述解释，在线性一致性的数据存储服务中，是不存在并发操作的：因为必然存在一个时间线能将所有操作进行排序。

> 理解全序和偏序、线性一致性和因果一致性的一个关键模型是**有向图**。在该图中，点代表事件，有向边代表因果关系，并且从因事件指向果事件，很自然的，因果性满足传递性。如果该图中有一条单一的路径能串起所有点，且不存在环，则该系统是线性一致的。可以看出，**因果关系是一种局部特性**（也即偏序关系），定义在两个点之间（如果两个点之间存在着一条单向途径，则这两点有因果关系）；而**线性关系是一种全局特性**（也即全序关系），定义在整个图上。

#### 3.1.3 线性一致性强于因果一致性

**任何线性一致性的系统都将正确地保证因果一致性**。

线性一致性能够保证因果一致性，这一结论使得线性化系统更加简单易懂且富有吸引力，但这会显著降低性能和可用性，尤其是网络延迟严重的情况下。

好消息是存在折中路线。一个系统可以不必承担线性一致性所带来的性能损耗，而仍然是因果一致的。这种情况下，CAP 理论是不适用的。事实上，**因果一致性是系统在保证有网络延迟而不降低性能、在有网络故障而仍然可用的情况下，能够提供的最强一致性模型**。

在大多数情况下，我们以为我们需要线性一致模型，其实我们真实需要因果一致模型，而后者允许我们实现性能更好的系统。基于这个观察，研究人员在探寻新型数据库的设计，让系统既可以提供因果关系保证，也可以提供（堪比最终一致性系统的）高性能和高可用性。这些研究都比较新，还存在很多挑战，也没有进行落地。但无疑，是分布式系统在将来一个很有前景发展方向。

::: note 实践
真实系统中，在所有的事件集中，只有部分事件是有因果依赖的，这些事件需要在执行时保证因果顺序执行；而其他的大部分事件是没有因果依赖的，因此可以乱序、按需执行以保证性能。但**这件事情的难点在于，因果关系是应用层定义的。而我们在系统层，就很难识别**。可能需要提供某种接口，可以让应用层显示指定因果，但一来不确定这种接口是否能做的足够宽泛；二来，这种因果追踪的额外代价是非常大的。
:::

#### 3.1.4 捕获因果依赖

我们不会事无巨细的去探究非线性化系统如何保持因果关系的每个细节，仅就其中的一些关键点进行探讨。

为了保证因果一致性，**我们需要知道哪些操作存在着因果关系**。为了确定因果依赖，我们需要某种手段来描述系统中节点的“知识”（knowledge）。如果某个节点在收到 Y 的写入请求时已经看到了值 X，则 X 和 Y 间可能会存在着因果关系。就如在调查公司的欺诈案时，CEO 常被问到，“你在做出 Y 决定时知道 X 吗”？

确定哪些操作先于哪些些操作发生的方法类似于我们在“并发写入检测”一节讨论的技术。那一节针对无主模型讨论了如何检测针对单个 Key 的并发写入，以防止更新丢失问题。因果一致性所需更多：需要**在整个数据库范围内追踪所有 Key 间操作的因果依赖，而非仅仅单个 Key 上**。**版本向量**（version vectors）常用于此道。

为了解决确定因果顺序，数据库需要知道应用读取数据的**版本信息**。为此，数据库需要跟踪一个事务读取了哪些数据的哪些版本。

### 3.2 序列号定序

虽然因果关系很重要，但在实践中，**追踪所有的因果依赖非常不切实际**。面对大量的读写，我们也无从得知之后的写入和先前有没有关系，和哪些有关系。**显式的追踪所有读集合**所带来的开销会非常大。

不过，我们有一种更简单的手段：**使用序列号（sequence numbers）或者时间戳（timestamps）来给事件定序**。

我们不一定适用物理时间戳（如日历时钟），而是可以用逻辑时钟。最简单的，可以用一个计数器来递增地为每个操作安排一个序列号。

> 实际中尽管使用逻辑时间戳，但也会采用一些算法与物理时间戳进行关联，否则出现错误时，难以知道事件发生时的上下文，如 TSO 算法。

此种序列号和时间戳通常都非常紧凑，只占几个字节，但却能提供一种全序关系。通过给每个操作关联一个序列号，就能比较任何两个操作的先后关系。进一步，我们可以保证我们产生序列号的方式满足因果关系：如果操作 A 发生在 B 之前，则 A 获取到的序列号比 B 小。并发的（无法比较谁先谁后）操作获取到的序列号顺序不确定。

**序列号本质上是一种全序**，通过这种方式可以追踪因果关系，但也施加了一个比因果关系更为严格的全序，同时实现的性能也并不是特别好。

下面介绍了一些产生序列号的方式。

#### 3.2.1 非因果序列生成器

如果系统中没有唯一的单主节点，那么为每个操作产生一个序列号就变得不那么简单直观了。常见的方式有如下：

1. **每个节点独立地生成不相交的序列集**。如，你的系统中有两个节点，一个节点只产生奇数序号，另一个节点只产生偶数序号。更通用一些，我们可以在生成的序号中保留一些位来编码对节点的标识，从而让不同的节点永远不会产生相同的序号。
2. **可以为每个操作关联一个日历时钟**。这些时间戳不是有序的（因为回拨），但如果有足够的精度，就可以让任意两个操作关联的时间戳不同，依次也可以达到全序的目的。此种方法有时候会被用在解决冲突使用后者胜的策略。
3. **每次可以批量产生一组序列号**。比如，在请求序列号时，节点 A 可以一次性声明占用 1 ~ 1000 的序列号，节点 B 会一次占用 1001~2000 的序列号。则本地的操作可以从拿到的这批序列号中直接分配，仅在快耗尽时再去请求一批。这种方法常被用在 TSO（timestamp oracle，单点授时）的优化中。

这三种方案都要比使用单点计数器生成序列号要性能好、扩展性更强，且能为系统中的每个操作产生全局唯一的、近似递增的序列号。但他们都存在着同样的问题：**产生的序列号不是因果一致的**。

#### 3.2.2 Lamport 时间戳

虽然上面的几种方式产生的序列号不满足因果一致性，但却有一种相对简洁的方式可以做到—— <mark>Lamport 时间戳</mark>。它是由 Lesilie Lamport 在 1978 年提出的，*是分布式领域被引用最多的论文之一*。

下图展示了 Lamport 时间戳的使用方法。在该系统中，每个节点有一个唯一的 id 和一个记录处理过多少个操作的计数器，Lamport 时间戳是上述两者组成的二元组：<font color=blue>(counter, node ID)</font>。不同的节点可能会有相同的 counter 值，但通过引入 node ID，可以使所有时间戳都是**全局唯一的**。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230205231806.png" alt="20230205021906" style="zoom:75%;" /></center>

Lamport 时间戳不依赖于物理时钟，但可以提供全序保证，对于任意两个 Lamport 时间戳：

1. 具有较大 counter 的时间戳较大
2. counter 相同，具有较大 node ID 的时间戳较大

让 Lamport 时间戳能够满足因果一致性的核心点在于：**每个节点和客户端都会让 counter 追踪当前所看到（包括本机的和通信的）的最大值**。当节点看到请求或者回复中携带的 counter 值比自己大，就会立即用其值设置本地 counter。

> 系统中所有的事件（event），和交互方（client，server）都要被纳入 Lamport Clock 体系内，才能追踪系统内的所有因果关系。因为只有通过通信才能有因果性，并通过通信来维持 counter 最大值。

只要最大的 counter 值通过每个操作被传播，就能保证 Lamport 时间戳满足因果一致。因为每次因果依赖的交互都会推高时间戳。

有时候我们会将 Lamport 时间戳和之前提到的版本向量混淆。虽然看起来相似，但其根本目的却是不同：

+ 版本向量能够用于检测操作的并发和因果依赖
+ Lamport 时间戳只是用于确定全序

对于 2，虽然 Lamport 时间戳能够追踪因果关系，即具有因果关系中的 happens-before 关系。但是反过来，并不能通过两个 Lamport 时间戳的大小来判断其是有因果关系、还是并发的。但相对于版本向量，Lamport 时间戳占用空间小，更为紧凑。

#### 3.2.3 时间戳定序还不够

尽管 Lamport 时间戳能够给出一种能够追踪因果关系的全序时间戳生成算法，但并不足以解决分布式系统中所面临的的很多基本问题。

时间戳排序的一个问题是：**只有在收集到系统中所有的操作信息之后，才能真正确定所有操作的全序**。而为了获得这些信息，系统就需要检查每个节点，询问他们在做什么，一旦某个节点突然出现故障而无法连接，那么方法就无法正常运转。显然，这不是我们所期望的容错系统。

总而言之，为了实现像用户名唯一性约束这样的目标，仅仅对操作进行全序排列还是不够的，还需要知道这些操作是否发生、何时确定等。假如能够在创建用户名时，已经确定知道了没有其他节点正在执行相同用户名的创建，你大可以直接安全返回创建成功。

想要知道什么时候全序关系已经确定，就需要之后的“**全序广播**“。

### 3.3 全序广播

在一个分布式系统中，**让所有节点就所有操作的某个确定的全局序列达成一致**是相当棘手的。前一节讨论了使用序列号来定序，但相比单主模型这种方法容错能力很弱鸡（在使用时间戳定序的系统中，如果你想实现唯一性约束，就不能容忍任何故障）。

单主模型通过在所有节点中选出一个主，尔后在该节点上利用某个 CPU 对所有操作进行定序，从而确定一个唯一的全局序列。但使用单主模型的系统会面临两个问题：

1. 当系统负载超过单机可以处理的尺度，如何进行扩容。
2. 当主节点宕机时如何进行故障转移（failover）。

在分布式系统的语境下，该问题也被称为<mark>全序广播</mark>（total order broadcast）或者<mark>原子广播</mark>（atomic broadcast）。

> **顺序保证的范围**。
>
> 多分区的数据库，对于每个分区使用单主模型，能够维持每个分区的操作全局有序，但并不能提供跨分区的一致性保证（比如一致性快照，外键约束）。当然，跨分区的全序保证也是可以提供的，只不过需要进行额外的协调。

**全序广播是一种多个节点间交换消息的协议**。它要求系统满足两个安全性质：

1. **可靠交付**。如果一个节点收到了消息，则系统内所有的相关节点都要收到该消息。
2. **全序交付**。每个节点接收到消息的<u><font color=red>顺序一致</font></u>。

一个正确的全序广播算法需要保证上述两条性质在任何情况下都能够满足，包括网络故障和节点宕机。但当然，如果网络出现故障时，消息肯定不能送达所有节点；但算法可以进行重试，直到网络最终恢复（当然，恢复后也要保证所有消息送达的顺序一致）。

#### 3.3.1 使用全序广播

像 Zookeeper 和 etcd 等共识服务都实现了全序广播算法。从这些共识服务我们能感觉到，全序广播和共识协议具有很强的关联性，我们会在本章稍后一探究竟。

**全序广播正是数据库复制所需要的**：如果我们让“消息”代表数据库中的写请求，且每个副本以相同的顺序处理相同的输入集，则每个副本必然会保持一致（除却暂时的临时同步延迟外）。该准则也被称为：<mark>状态机复制</mark>（state machine replication），在第 11 章的时候我们将继续该主题。

类似的，**全序广播也可以用于实现可串行化的事务**：如之前物理上串行提到的，消息在此具象为作为存储过程执行的一个确定性的事务，如果所有节点按同样的顺序处理这些消息，则数据中的所有分区和副本最终都会在数据上保持一致。

需要注意到，全序广播的一个重要性质是：<font color=blue>当收到消息时，其顺序已经确定</font>。这是因为，节点不能将后收到的消息，插入之前的已经收到的消息序列。这让**全序广播要强于时间戳排序**（timestamp order）。

还可以从另外一个角度来理解全序广播——用来写日志（比如复制日志、事务日志或者写前日志）：**传递消息就像追加日志**。由于所有节点都会按照同样的顺序发送消息，则所有节点在读取日志的时候也会得到同样的消息序列。

**全序广播也可以用来实现提供防护令牌功能**（fencing token，参见防护令牌，即关联了 id 的锁）的锁服务。每个上锁请求可以作为消息追加到日志中，依据其追加到日志中的顺序，所有请求可以被自然地编号。由于这个序列号是单调递增的，便可以充当防护令牌。**在 Zookeeper 中，这个序列号便是 zxid**。

#### 3.3.2 使用全序广播实现线性一致性存储

在线性一致系统中，所有操作存在着一个全局序列。这是否意味着全序广播就是线性一致性？不尽然，但他们间有很深的联系。

+ 全序广播是异步的：系统保证以同样的顺序交付消息，但并不保证消息的交付时刻（即，有的消息接收者间可能存在着滞后）
+ 线性一致性是一种新鲜度保证：读取一定能看到最新成功的写

不过，**你可以基于全序广播来构建线性一致的存储系统**。如，可以用于保证用户名的唯一性约束。

对于该问题，可以这样实现。对每一个可能的用户名，我们使用一个支持 CAS 操作的线性寄存器，初始值为 null（表示该用户名没有被占用）。当用户想使用某个用户名创建账户时，使用 CAS 操作，在寄存器旧值为 null 时，将其值设置为该用户 account-id。由于寄存器的原子性，最后只有一个用户会成功。

**使用全序广播系统作为日志追加服务，便可以实现这样一个支持可线性化 CAS 操作的“寄存器”**：

1. 向服务中追加一个带有某用户名的消息条目，表明你想使用该用户名。
2. （由于全序广播是异步的）不断读取日志，直到能够读到刚才你追加的消息条目。
3. 检查所有想要使用该用户名的消息，这时你可能会得到多条消息，如果你当初写下的消息在第一条，则你是成功的。此时，你可以“确认”（持久化，比如追加日志，比如写入数据库）占有该用户名的信息，然后给客户端返回成功。如果第一条消息不是你的，则终止请求。

> 这里其实隐藏了一些细节，即我们会将追加消息请求发送给全序广播系统，全序广播系统会真正将消息按之前提到的两条保证的方式（可靠送达和全序送达）同步到每个节点。因此，对于每个节点来说，会首先发起消息追加请求，然后之后某个时刻，可以等到真正同步回来的消息。如果觉得绕，可以带入 Raft 的付之日志来类比。

由于所有日志条目都会以同样的顺序送达每个节点，若有并发写入，则所有节点都能依靠日志顺序就谁“先来后到”达成一致。当有同名冲突时，可以选择第一条作为赢家，并舍弃其后的冲突请求。可以使用类似的方式，基于日志来实现涉及到多对象的事务的可串行化。

尽管该方式能够提供线性化的写入，却不能保证线性化的读取。如果你从一个异步同步日志的节点读取日志，就有可能读到陈旧的数据（更精确一点说，上述过程能够提供<mark>顺序一致性</mark>，sequential consistency，有时也被称为<mark>时间线一致性</mark>，timeline consistency，比线性一致性稍弱 ）。在此基础上，如果想让读取也变得可线性化，有几种做法：

+ 让读取也走日志，即通过追加消息的方式将读取顺序化，然后当读取请求所在节点收到这条读取日志时才去真正的去读。则消息在日志中的位置定义了整个时间序列中读取真正发生的时间点。（etcd 中的法定读取就是用的类似的做法）
+ 如果日志服务允许查询最新日志的位置，则可以在请求到来时，获取当时最新位置，然后不断查询日志看是否已经跟到最新位置。如果跟到了，就进行读取。（这是 Zookeeper 中 sync() 操作的背后的原理）
+ 可以将读取路由到写入发生的那个节点，或者与写入严格同步的节点，以保证能够读到最新的内容。（这种技术用于**链式复制**，chain replication 中）

#### 3.3.3 使用线性一致存储实现全序广播

上节展示了如何使用全序广播来实现 CAS 操作的线性化存储。我们也可以反过来，设有一个线性一致的存储系统，看如何基于其实现全序广播。

最简单的方法，假设我们有一个整数寄存器，并提供 increment-and-get 原子操作，当然提供 CAS 原子操作也是可以的，只是稍微没那么直观。

算法相当简单：对于每一个发给全序广播系统的消息，使用整数寄存器 increment-and-get 操作关联一个序列号；然后将消息发送给所有节点（重试任何丢失的消息）。每个节点接收到消息后利用序列号顺序对外交付消息。这种机制很像 TCP，但并不是描述通信双方，而是一个分布式系统。

注意到，和 Lamport 时间戳不同，**从线性化的寄存器中获取的数字是连续的，非跳跃的**（可以用来防止有中间的消息丢失）。如此一来，当某节点交付了消息 4 后，收到了消息 6，但不能立即交付，而需要等待消息 5 的到来。但在 Lamport 时间戳系统中则非如此——这（是否连续）也是全序广播和时间戳顺序的核心不同。

实现一个支持 increment-and-get 原子操作的线性化整数寄存器有多难？如果所有组件都不会出错，则很简单：你可以直接用单台机器上的一个变量。但如果连接到该节点的网络会出问题怎么办？这台机器机器宕机后，宕机前变量的值又该怎么恢复？理论上说，只要你对线性序列生成器思考的足够完备，就会不可避免的得到一个共识算法（consensus algorithm）。

这并不是巧合：可以证明，**一个线性的 CAS 寄存器和全序广播都等价于共识协议**（equivalent to consensus）。也即，如果你解决了其中任意一个问题，就可以通过某种手段将其转化为另一个问题的解决方案。这真是一个惊人的性质！

终于，说到了共识，这是本章余下将要全力探讨的问题。

> 上面铺垫了一大通来引出了共识协议。

## 4. 分布式事务和共识协议
