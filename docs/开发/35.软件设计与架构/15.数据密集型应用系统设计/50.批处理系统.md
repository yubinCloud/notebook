---
title: 批处理系统
date: 2023-03-04 17:19:00
permalink: /pages/DDIA/note/batch-processing/
categories:
  - 开发
  - 软件设计与架构
  - 数据密集型应用系统设计
tags:
  - 
---

> 带有太强个人色彩的系统难以大获成功。一旦最初设计基本完成且足够鲁棒时，由于经验迥然、观点各异的人的加入，真正的测试才刚刚开始。
>
> —— Donald Knuth

本书前面主要讨论的是“请求应答”风格的数据处理，由于 Web 服务和日期增长的基于 HTTP/REST 的 API，让请求/应答风格的交互如此普遍。但这并非是唯一的系统构建方式。我们对下面三种类型的系统进行考察：

+ **服务**（在线系统，**online systems**）：server 接收 client 发来的请求，并试图尽快处理然后应答。**响应时间**通常是衡量一个服务性能的最主要指标，且**可用性**通常很重要。
+ **批处理系统**（离线系统，**offline systems**）：一个批处理系统接收大量数据作为输入，然后在这批数据上跑 job，进而产生一些数据作为输出。**吞吐量**（throughput，处理单位数据量所耗费的时间）通常是衡量批处理任务最主要指标。
+ **流式系统**（近实时系统，**near-real-time systems**）：流式处理介于在线处理和离线处理（批处理）之间。流式任务通常会在事件产生不久后就对其进行处理，与之相对，一个批处理任务通常会攒够一定尺寸的输入数据才会进行处理。这种区别让流式处理系统比同样功能的批处理系统具有更低的延迟。

批处理是我们寻求构建可靠的、可扩展的、可维护的应用的重要组成部分。例如 MapReduce，一个发表于 2004 年的批处理算法，使得“谷歌具有超乎寻常可扩展能力”。该算法随后被多个开源数据系统所实现，包括 Hadoop，CounchDB 和 MongoDB。

本章将会介绍 MapReduce 和其他几种批处理算法和框架，并探讨下他们如何用于现代数据系统中。作为引入，首先会看下标准 Unix 工具如何用于数据处理，进而将这些哲学和经验运用到大规模、异构的分布式数据系统中。

## 1. 使用 Unix 工具进行批处理

从一个简单的例子开始。设你有一个 web 服务器，并且当有请求进来时，服务器就会向日志文件中追加一行日志：

```cpp
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X
10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115
Safari/537.36"
```

注：上面文本其实是一行，只是为了阅读性，拆成了多行。

这行日志信息量很大。为了便于理解，你可能首先需要了解其格式：

```cpp
$remote_addr - $remote_user [$time_local] "$request"
$status $body_bytes_sent "$http_referer" "$http_user_agent"
```

### 1.1 简单的日志分析

这里我们只使用基本的 Unix 命令来做一个工具：获取网站上访问频次最高的五个页面。在 Unix Shell 中输入如下：

```sh
cat /var/log/nginx/access.log | #(1)
      awk '{print $7}' |        #(2)
      sort             |        #(3)
      uniq -c          |        #(4)
      sort -r -n       |        #(5)
      head -n 5                 #(6)
```

每一行作用如下：

1. 读取给定日志文件
2. 将每一行按空格分成多个字段，然后取出第七个，即我们关心的 URL 字段。在上面的例子中，即：`/css/typography.css`
3. 按字符序对所有 url 进行排序。如果某个 url 出现了 n 次，则排序后他们会连着出现 n 次。
4. `uniq` 命令会将输入中相邻的重复行过滤掉。`-c` 选项告诉命令输出一个计数：对于每个 URL，输出其重复的次数。
5. 第二个 `sort` 命令会按每行起始数字进行排序（`-n`），即按请求次数多少进行排序。`-r` 的意思是按出现次数降序排序，不加该参数默认是升序的。
6. 最后，`head` 命令会只输出前 5 行，丢弃其他多余输入。

对日志文件执行这一系列命令，会得到类似如下结果：

```cpp
4189 /favicon.ico
3631 /2013/05/24/improving-security-of-ssh-private-keys.html
2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
1369 /
 915 /css/typography.css
```

如果你熟悉 Unix 工具链，会发现他们非常强大，这个组合可以在数秒内处理上 G 的日志文件。

本书中没有余力去详细讨论所有 Unix 工具使用细节，但他们都很值得一学。你可以在短短几分钟内，通过灵活组合 awk, sed, grep, sort, uniq, 和 xargs 等命令，应对很多数据分析需求，并且性能都相当不错。

### 1.2 链式命令 vs 专用程序

除了链式组合 Unix 命令，你也可以写一个简单的小程序来达到同样的目的。如，使用 Ruby，会有类似如下代码：

```ruby
counts = Hash.new(0)    # (1)

File.open('/var/log/nginx/access.log') do |file| 
  file.each do |line|
    url = line.split[6] # (2)
    counts[url] += 1    # (3)
  end
end

top5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5] # (4)
top5.each{|count, url| puts "#{count} #{url}" }                   # (5)
```

标号对应代码功能如下：

1. `counts` 是一个哈希表，为每个出现过的 URL 保存一个计数器，计数器初始值为 0。
2. 对于每行日志，提取第六个字段作为 URL（ ruby 的数组下标从 0 开始）。
3. 对当前行包含的 URL 的计数器增加 1 。
4. 对哈希表中的 URL 按计数值降序排序，取前五个结果。
5. 打印这五个结果。

该程序虽不如使用 Unix 管道组合的命令行简洁，但可读性也很好，喜欢使用哪种方式是一个偏好问题。然而，两者上除了表面上的语法区别，在执行流程上差别也很大。在你分析海量数据时，这一点变的尤为明显。

### 1.3 排序 vs 内存聚合

Ruby 脚本在内存中保存了 URL 的**哈希表**，记录每个 URL 到其出现次数的映射。Unix 管道例子中并没有这样一个哈希表。作为替代，它将所有 URL 进行**排序**，从而让所有相同的 URL 聚集到一块，从而对 URL 出现次数进行统计。

那种方法更好一些呢？这取决于你有多少个不同的 URL：

+ 如果工作集足够小，则基于内存的哈希表能够很好地工作——即使在笔记本电脑上。
+ 但如果任务的工作集大于可用内存，则排序方式更有优势，因为能够充分利用磁盘空间。因为可以使用磁盘上的归并排序。

GNU 核心工具包中的 sort 命令，会自动的处理超过内存大小的数据集，将一些数据外溢（spill）到磁盘上；此外，该工具还可以充分利用多核 CPU 进行并发地排序。这意味着，我们之前例子中的对日志处理的 Unix 命令行能够轻松应对大数据集，而不会耗尽内存（OOM）。不过，性能瓶颈会转移到从磁盘读取输入文件的 IO 上。

### 1.4 Unix 哲学

我们能够通过简单的组合 Unix 工具来进行复杂的日志文件处理并非巧合：这正是 Unix 的核心设计思想之一，且该思想在今天也仍然非常重要。让我来深入的探究一下其背后哲学，以看看有什么可以借鉴的。

Doug McIlroy，Unix 管道（pipe）的发明人，在 1964 年是这样描述管道的：“我们需要一种像软管一样可以将不同程序连接到一块的方法——当数据准备好以其他方式处理时，只需要接上就行。 IO 也应该以这种方式工作”。管道的类比到今天仍然存在，并且成了 Unix 哲学的一部分。**Unix 哲学**是一组在 Unix 用户和开发者中很流行的设计原则，在 1978 年被表述为：

1. **每一个程序专注干一件小事**。在想做一个新任务时，新造一个轮子，而非向已有的程序中增加新的“功能”。
2. **每个程序的输出成为其他程序（即便下一个程序还没有确定）的输入**。不要在输出中混入无关信息（比如在数据中混入日志信息），避免使用严格的列式数据（数据要面向行，以行为最小粒度？）或者二进制数据格式。不要使用**交互式输入**。
3. **尽快的设计和构建软件**，即便复杂如操作系统，也最好在几周内完成（译注：这里翻译稍微有些歧义，即到底是尽快迭代还是尽早让用户试用，当然他们最终思想差不多，即构造最小可用模型，试用-迭代）。对于丑陋部分，不要犹豫，立即推倒重构。
4. 优先使用工具来减轻编程任务，即使你不得不额外花费时间去构建工具，并且预期在使用完成后会将其中一些工具扔掉。

这些手段——尽可能自动化、快速原型验证、小步增量迭代、易于实验测试，将大型工程拆解成一组易于管理的模块——听起来非常像今天的**敏捷开发**和 **DevOps 运动**。令人惊讶的是，很多软件工程的核心思想在四十年间并没有太多变化。

Unix 中的 sort 工具是一个程序只干好一个事情的非常典型的案例。相对大多数编程语言的标准库函数（即使在能明显提升性能时，也不能利用磁盘空间、不能利用多核性能），它给出了一个更好的实现。不过，单独使用 sort 威力还不是那么大。只有在与其他 Unix 工具（如 uniq）组合时，sort 才会变的相当强大。

使用 Unix Shell 如 bash 让我们能够轻易的将这些工具**组合**以应对数据处理任务。即使大部分的工具都是由不同人编写的，也可以很容易的组合到一块。我们不禁会问，Unix 做了什么让其有如此强的组合能力？

#### 1.4.1 统一的接口

如果你希望一个程序的输出可以作为其他程序的输入，就意味着这些程序需要使用同样的数据格式，也就是这些程序需要同样的输入输出接口。

在 Unix 中，这种接口是**文件**（更准确来说是 file descriptor）。其本质上是一种**有序的字节序列**。这种接口是如此简约，以至于很多不同的实体都可以共用该接口：文件系统中真实的文件、与其他程序的通信渠道（Unix socket，标准输入 stdin，标准输出 stdout）、设备驱动（如 /dev/audio 或者 /dev/lp0）、以 socket 表示的 TCP 连接等等。现在回过头去看，在当时能让这么多不同的东西使用统一的接口是一种非常了不起的设计，唯其如此，这些不同的东西才能进行任意可插拔的组合。

> 另外一个非常成功的接口设计是：URL 和 HTTP，互联网的基石。一个 URL 能够唯一的定位网络中的一个资源。

通常来说，大部分（并不是所有）的 Unix 程序将这些字节序列看做是 ASCII 文本。**大部分程序将其输入文件视为由 `\n` 分隔的一系列记录**。但相对来说，大部分程序对于每一行记录的解析却是相对模糊和非统一的。Unix 工具通常使用空格或者 tab 作为分隔符将一行分解成多个字段，但有时也会用 CSV（逗号分割）、管道分割等其他编码。

尽管很多设计并不完美，在数十年后的今天，Unix 统一的接口设计仍然堪称伟大。没有多少软件模块可以像 Unix 工具这样进行任意交互和组合。

即使两个数据库的数据具有**相同的数据模型**（Schema），也很难快速的将数据在两个数据库间导来导去。多个系统间缺少有效整合使得数据变得[巴尔干化](https://zh.wikipedia.org/zh-hans/巴尔干化)（一种相对贬义的地缘政治学术语，指较大国家分裂成互相敌对的一系列小国的过程）。

#### 1.4.2 逻辑和接线（数据流组织）分离

**Unix 工具的另外一个显著特征是其对于标准输入（stdin）和标准输出（stdout）的使用**。**管道**（pipe）能让你将一个程序的标准输出（即编码实现该程序时，程序视角的 stdout）重定向到另外一个程序的标准输入（仅需要一个比较小的缓冲区足矣，并不需要将所有的中间数据流写入磁盘）。

程序在需要时当然可以直接读写文件，但若不关心具体的文件路径，而**仅面向标准输入和标准输出进行编程，可以在 Unix 环境下和其他工具进行更好地协同**。这种设定，让 shell 用户可以按任意方式对多个程序的输入输出进行组织（也即**接线**，wire up）；每个程序既不需要关心输入来自何处，也不需要知道输出去往何方（我们有时也将这种设计称为**松耦合**，**延迟绑定**或者**控制反转**）。将程序逻辑和数据流组织分离，能让我们轻松地组合小工具，形成复杂的大系统。**只要你的工具是从 stdin 读取数据，并将处理结果写入到 stdout，就能作为一环嵌入到 Unix 的数据处理流水线中**。

不过，只是用 stdin 和 stdout 编程是有很多限制的。比如难以让程序使用多个输入或者产生多个输出。

#### 1.4.3 透明性和实验性

Unix 工具生态如此成功的另外一个原因是，可以很方便让用户查看系统运行的状态：

+ Unix 命令的输入文件通常被当做是**不可变的**。这意味着，你可以使用不同命令行参数，针对同样的输入跑很多次，而不用担心会损坏输入文件。
+ 你可以在多个命令组成的处理流水线的任意环节停下来，将该环节的输出打到 `less` 工具中，以查看输出格式是否满足预期。这种可以对运行环节**随意切片**查看运行状态的能力对调试非常友好。
+ 你可以将一个流水线环节的输出**写入到文件**中，并将该文件作为流水线下一个环节的输入。这样即使你中断了流水线的执行，之后想重启时就不用重新跑流水线所有环节。

因此，虽然相比关系型数据库中的查询优化，Unix 工具非常粗陋、简单，但却非常好用，尤其是在做简单的实验场景下。

然而，**Unix 工具最大的局限在于只能运行在单机上**——这也是大数据时代人们引入 Hadoop 的进行数据处理的原因——单机尺度已经无法处理如此巨量的数据。