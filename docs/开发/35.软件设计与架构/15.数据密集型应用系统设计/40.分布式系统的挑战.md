---
title: 分布式系统的挑战
date: 2023-02-02 00:00:37
permalink: /pages/DDIA/note/the-trouble-with-distrubuted-systems/
categories:
  - 开发
  - 软件设计与架构
  - 数据密集型应用系统设计
tags:
  - 
---

前面几章所考虑的情况：机器宕机、网络延迟都相对较理想。但在实际的大型分布式系统中，情况会更加悲观，出错方式更加复杂。

构建分布式系统和单机软件完全不同。尽管我们总期望能构建处理任何可能故障的系统，但**在实践中，一切都是权衡**。因此，我们首先需要知道可能遇到哪些问题，才能进而选择：**是否要在目标场景下解决这些问题、还是为了降低系统复杂度忽略这些问题**。

本章将会探讨计算机网络的痼疾、时钟和时间、以什么程度避免上述问题等等。所有上述问题的原因都隐藏的很深，本章会探索如何理解分布式系统当前所处状态、如何定位分布式系统问题原因所在。

## 1. 故障和部分失败

单机系统通常具有一种很好地特性：要么正常运行、要么出错崩溃，而不会处于一种中间状态。但在构建分布式系统时，系统行为边界变得模糊起来。在分布式系统中，有很多我们习以为常的假设都不复存在，各种各样的异常问题都会出现。其中最令人难受的是：<mark>部分失败</mark>（partial failure），即系统的一部分正常工作，另一部分却以某种诡异的方式出错。这些问题，多数都是由于连接不同主机的异步网络所引入的。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230202021316.png" alt="20230202021316" style="zoom:75%;" /></center>

### 1.1 云计算与超算

关于如何构建大规模计算系统，在选择上有如下一个光谱：

+ 在光谱的一侧，是<mark>高性能计算</mark>（HPC，high-performance computing）。使用上千个 CPU 构建的超级计算机，用于计算密集型工作，如天气预报、分子动力学模拟。
+ 在光谱的另一侧，是<mark>云计算</mark>。云计算不是一个严谨的术语，而是一个偏口语化的形象指代。通常指将通用的廉价的计算资源，通过计算机网络收集起来进行池化，然后按需分配给多租户，并按实际用量进行计费。
+ 传统的企业**自建的数据中心**位于光谱中间。

不同构建计算系统的哲学，有着不同的处理错误的方式。对于高性能计算（也称超算）来说，只要其中部分出错后只要停止集群运行，等故障修复后在恢复最近的快照继续运行即可。超算更像是一个单节点系统而非分布式系统，它将局部失效升级为了整体失效。

但本章的重点在于基于互联网的服务系统，**它与单机应用有着很多不同之处**：

+ <u>在线离线</u>。互联网多为在线服务，这种场景下重启往往是不可接受的；但在高性能计算的离线任务中，影响相对较小。
+ <u>专用通用</u>。超算多用专用硬件构建而成，组件本身很可靠；而云服务多由通用机器组网而成，具有规模化堆数量的特点，经济但故障率高。
+ <u>组网方式</u>。大型数据中心通常基于 IP 和以太网，采用 Clos 拓扑组网以对分带宽；而超算常用特定拓扑结构来提高计算性能。
+ <u>故障常态化</u>。系统越大，其中局部组件失效的概率越大。在上千个节点组成的系统中，**可以认为任何时刻，总有组件存在故障**。
+ <u>容错</u>。当部分节点故障时，如果系统仍能作为一个整体而正常工作，将会对运维十分友好。
+ <u>本地异地</u>。多地部署的大型系统多通过互联网通信，更易出错；超算的多个节点都靠的很近。

为了让分布式系统能够工作，就必须假设故障一定会存在，并在设计层面考虑各种出错处理。即，**我们要基于不可靠的组件构建一个可靠系统**。因此，**面向容错进行设计**是对分布式系统软件的基本要求，为此，我们首先要了解分布式系统中的常见问题，并依此设计、编写、测试你的系统。

::: note 在实践中
但**在实践中，任何设计都是取舍（trade-off），容错是有代价（昂贵、损失性能、系统复杂度提升等）的**。因此，充分了解你的系统应用场景，才能做出合理的容错实现，过犹不及。
:::

### 1.2 基于不可靠的组件构建可靠的系统

但在工程领域，这种思想并不少见。如：

+ 纠错码能够容忍信道中偶尔一两个比特的误传。
+ IP 层不可靠，但 TCP 层却基于 IP 层提供了相对可靠的传输保证。

不过，**所有容错都是有限度的**。如纠错码也没办法处理信号干扰造成的大量信息丢失，TCP 可以解决 IP 层的丢包、重复和乱序问题，但没办法对上层隐藏解决这些问题带来的通信时延。但，**通过处理一些常见的基本错误，可以简化上层的设计**。

## 2. 不可靠的网络

要明确的是，本书讨论的系统范畴是 <mark>share-nothing 系统</mark>：**所有及其不共享资源（如内存、磁盘），通信的唯一途径就是网络**。这种构建方式是最经济，也是复杂度最高的。

现在的网络多是**异步封包网络**（asynchronous packet networks），也就是说，一个机器向其他机器发送数据包时，不提供任何保证：你不知道数据包什么时候到、甚至不知道它是否能够到。下图展示了一次网络交互可能遇到的异常情况：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230202021316.png" alt="20230202021316" style="zoom:75%;" /></center>

在异步网络中，当你发送出一个请求，并在一段时间内没有收到应答，任何事情都有可能发生：由于没有收到任何信息，你无从得知具体原因是什么。甚至，你都不知道你的请求是否已被送达处理。应对这种情况的惯常做法是——<mark>超时</mark>（timeout）。即，**设定一个时限，到点后，我们便认为这个请求废了**。但在实际上，该请求可能只是还在排队、可能稍后到到达远端节点、甚至可能最终还会收到应答。

### 2.1 实践中的网络故障

很多经验表明：**即使在专门管理的数据中心，网络问题也相当普遍**。

> 一项研究关于中型数据中心的研究表明，平均每月有 12 次网络故障，其中一半是单机失联，另一半是机架整个失联。另一项关于组件故障率的研究，包括 TOR 交换机、聚合交换机和负载均衡器。发现，增加网络冗余并不能如预期般减少故障，因为这不能避免**造成网络中断的最主要原因——人为故障**（如配置错误）。

尽管云服务相比自建数据中心会更加稳定，但没人能够真正逃脱网络故障：

+ 如交换机软件升级引发的拓扑重置，会导致期间网络延迟超过一分钟
+ :shark: 鲨鱼可能会咬断海底光缆
+ 有些奇葩的网口会只传送单向流量（即使一个方向通信正常，你也不能假设对向通信没问题

虽然上述情况可能都比较极端，但你的软件如果处理不好，当网络问题一旦发生，你将面临各种**难以定位**莫名其妙的问题，并且可能会导致服务停止和数据丢失。

处理网络错误并不一定要容错，**如果网络问题很少发生，直接让系统出现问题时停止运行并打印提示信息就可以**。但要保证，在网络恢复之后，服务也能够恢复，并且不会造成意外损失。为此，你需要使用混沌测试工具来主动模拟各种网络异常，在交付前确保你的软件有足够的鲁棒性。

### 2.2 故障检测

在很多系统里，我们需要**自动检测故障节点**，并据此做出一些决策，比如流量的负载均衡、主节点的更换等。

但不幸的是，你很难准确的判断一个远端节点是否发生了故障。在下面的一些特定场景，你可以通过一些旁路信号来获取一些信息，来判断确实发生了故障：

+ **操作系统通知**。如果你能触达服务所在机器，但发现没有进程在监听预期端口（比如对应服务进程挂了），操作系统会通过发送 RST 或 FIN 包来关闭 TCP 连接。但是如果对端节点在处理你的请求时整个宕机了，就很难得知你请求的具体处理进度。
+ **daemon 脚本通知**。可以通过一些 daemon 脚本，在本机服务进程死掉之后，主动通知其他节点。来避免其他节点通过发送请求超时来判断此节点宕机。当然这前提是，服务进程挂了，但所在节点没挂。
+ **数据链路层面**。如果你是管理员，并且能访问到你数据中心的网络交换机，可以在数据链路层判断远端机器是否宕机。当然如果你访问不到交换机，那这种方法就不太行。
+ **路由层 IP 不可达**。如果路由器发现你要发送请求的 IP 地址不可达，它会直接回你一个 ICMP 不可达包。但路由器也并不能真正判断是否该机器不可用了。

尽管有上述手段可以快速检测远端节点是否宕机，但你并不能依赖它们。因为，即使 TCP 层已经收到某个请求的 ACK，但对端服务仍有可能在应用层面没有处理完该请求就宕机了。因此，**如果你想确定某个请求确实成功了，只能在应用层进行显式确认**。

当然，如果对端出错，你可能会很快收到一个错误，但你并不能指望在任何情况下都能很快得到错误回复——可能过了一段时间我们仍然没有得到任何回复。因此，**在应用代码里，必须设置一个合理的超时时限和重试次数**。直到，你确认没有再重试的必要——即不管远端节点是否存活，我在重试几次后，都认为它不可用了（或者暂时不可用）。

### 2.3 超时和无界延迟

如上所述，<font color=blue>超时是应用层唯一能动用的检测网络故障的手段</font>。但另一个问题是：**超时间隔要设置多久呢**？总的来说：

+ 不能太长：过长会浪费很多时间在等待上。
+ 不能太短：太短会造成误判，误将网络抖动也视为远端节点失败。

过早将一个正常节点视为故障会有诸多问题：

1. **多次执行**。如果节点已经成功执行了某动作，但却被认为故障，在另一个节点进行重试，可能会导致次动作被执行两次（如发了两次邮件）。
2. **恶性循环**。如果系统本就处于高负载状态，此时还频繁错误的在其他节点上重试，可能会造成恶性循环，重试过多导致系统负载加重，系统负载加重反过来造成通信延迟增加，从而造成更多误判。

实际中网络都不会提供网络通信延迟的保证，尤其是当前网络环境下的异步网络。并且，大多数服务也很难保证在所有请求的处理时间都不超过某个上界。

#### 2.3.1 数据包的排队

在计算机网络中，数据报有很多环节可能造成排队：

1. 去程网络排队。如果多个节点试图将数据包同时发给一个目的端，则交换机得将他们排队以逐个送达目的端。
2. 目的机器排队。当数据包到达目的端时，如果目标机器 CPU 负载很高，操作系统会将进来的数据包进行排队，直到有时间片分给他们。
3. 虚拟机排队。在虚拟化环境中，由于多个虚拟机共用物理机，因此经常会整体让出 CPU 一段时间的情况。在让出 CPU 等待期间，是不能处理任何外部请求的，又会进一步给网络请求的排队时延增加变数。
4. TCP 流量控制。会剪枝发送方的发送频率，因此可能在本机排队。

一个基本现象是：网络流量越满，单个请求延迟抖动越大。

#### 2.3.2 超时间隔的设置

1. **静态设置**。使用实验统计的方式，在检测过久和故障误报之间做一个权衡。

2. **动态调整**。通过类似时间窗口的方式，不断监测过去一段时间内的请求时延和抖动情况，来获取请求时延的分布情况，进而动态调整超时间隔。<mark>Phi 累积故障检测算法</mark>（ The Φ Accrual Failure Detector）便是这样一种算法，Akka and Cassandra 中都用到了此种算法，它的工作原理和 TCP 重传间隔的动态调整类似。

### 2.4 同步网络和异步网络

> 这里计算机网络课程中都学了，不再展开

+ 同步网络：固定电话网、电路网络
+ 异步网络：分组交换网络

我们在设计分布式系统时，不能对网络传输的时延和稳定性有任何假设。我们必须要假定我们面对的网络会发生网络拥塞、会产生排队、会有无界延迟，在这种情况下，没有放之四海而皆准的超时间隔 。针对不同的具体情况，需要通过经验或者实验来确定它。

在异步网络中，可以认为是**资源的动态分配导致了时延的不稳定性**。这种不稳定的延迟并非什么不可变的自然法则，而仅是一种代价和收益权衡的结果罢了。

## 3. 不可靠的时钟
