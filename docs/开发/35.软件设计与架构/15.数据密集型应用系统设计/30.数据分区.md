---
title: 数据分区
date: 2023-01-15 15:08:00
permalink: /pages/DDIA/note/partition/
categories:
  - 开发
  - 软件设计与架构
  - 数据密集型应用系统设计
tags:
  - 
---

上一章主要讲复制，本章转向分片。这是两个相对正交但勾连的两个概念：

+ **分片**（Partition）：解决数据集尺度与单机容量、负载不匹配的问题，分片之后可以利用多机容量和负载。
+ **复制**（Replication）：系统机器一多，单机故障概率便增大，为了防止数据丢失以及服务高可用，需要做多副本。

> 分片，Partition，有很多别称。通用的有 Shard；具体到实际系统，HBase 中叫 Region，Bigtable 中叫 tablet，等等。**本质上是对数据集的一种逻辑划分**，后面行文，分片和分区可能混用，且有时为名词，有时为动词。

通常来说，数据系统在分布式系统中会有三级划分：数据集（如 Database、Bucket）——分片（Partition）——数据条目（Row、KV）。

本章思路：先介绍数据集**切分的方法**，并讨论索引和分片的配合；然后将会讨论分片**再平衡**（rebalancing），集群节点增删会引起数据再平衡；最后，会探讨数据库如何将请求**路由**到相应的分片并执行。

## 1. 分片和复制

**分片通常和复制结合使用**：

+ <u>每个分片在多个节点上都存有副本</u>，更泛化一点：多个容错阈；
+ <u>每个机器含有多个分片</u>，但通常不会有一个分片的两个副本放到一个机器上。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230121182324415.png" alt="image-20230121182324415" style="zoom:95%;" /></center>

> 由于分区方式和复制策略相对正交，本章会暂时忽略复制策略（在上章讲过），专注分析分区方式。

## 2. KV 数据的分片

**键值对是数据的一种最通用、泛化的表示**，其他种类数据库都可以转化为键值对表示：

1. 关系型数据库：primary key → row
2. 文档型数据库：document id → document
3. 图数据库：vertex id → vertex props, edge id → edge props

因此，接下来主要针对键值对集合的分区方式，则其他数据库在构建存储层时，可以首先转化为 KV 对，然后进行分区。

<mark>分片</mark>（Partition）的**本质是对数据集合的划分**。但**在实践中**，可以细分为两个步骤：

1. 对数据集进行**逻辑划分**
2. 将逻辑分片**调度**到物理节点

 因此，在分片时，有一些基本要求：

+ 分片过程中，要保证**每个分片的数据量尽量均匀**，否则会有<mark>数据偏斜</mark>（**skew**），甚而形成<mark>数据热点</mark>。
+ **分片后，需要保存路由信息**，给一个 KV 条目，能知道去**哪个**机器上去查；稍差一些，可以知道去**哪几个**机器上去找；最差的，如果需要去所有机器逐一查询，但性能一般不可接受。

**这两条是互相依赖和制约的**。比如说，假设分片数目确定，为了分片均匀，每来一条数据，我们可以等概率随机选择一个分片；但在查询每个数据条目时，就得去所有机器上都查一遍。

保存所有数据条目路由信息，有三种常用的策略：

1. 通过某种固定规则，比如哈希，算出一个位置。
2. 使用内存，保存所有数据条目到机器的映射。
3. 结合两种，首先通过规则算出到逻辑分片的映射，然后通过内存保存逻辑分片到物理节点的映射。

本节主要讨论根据 data Item 算出 partition，常见的有两种方式：基于 Key Range 分区，基于 Key Hash 分区。

### 2.1 基于 Key Range 分区

<mark>基于 Key Range 分区</mark>：每个分区分配一段连续的 key 或者 key 区间范围（以最小值和最大值来指示）。

如，百科全书系列，通常是按照名词的字母序来分册的，每个分册可理解为该系列的一个分区：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230121195937136.png" alt="image-20230121195937136" style="zoom:80%;" /></center>

由于键并不一定在定义域内均匀分布，因此简单按照定义域等分，并不能将数据等分。因此，需要按照数据的分布，**动态调整分区的界限**，保证分区间数据大致均匀。这个调整的过程，既可以手动完成 ，也可以自动进行。

> 动态调整的过程一般是先设只有一个分区，随着数据的到来再不断划分。

Key Range 分区的优缺点：

+ 优点：**快速的范围查询**（range query）。
+ 缺点：数据分散不均匀，且**容易造成热点**。可能需要动态的调整的分区边界，以维护分片的相对均匀。

错误使用示例：以传感器数据存储为例，以时间戳为 Key，按天的粒度进行分区，那么所有最新写入就都被路由到最后一个分区节点，造成严重的写入倾斜，不能充分利用所有机器的写入带宽。一个解决办法是**分级**或者**混合**，使用拼接主键，如使用传感器名称+时间戳作为主键，则可以将同时写入的多个传感器的数据分散到多机上去。

### 2.2 基于 Key Hash 分区

<mark>基于 Key Hash 分区</mark>：使用 key 的 hash function 值来分区。

选择 hash function 的依据是：使得数据散列尽量均匀。即给定一个 key，经 hash 后可以等概率在一个区间内产生一个值。

::: note hash function 选择的注意点
+ hash function 的选择**不需要考虑加密**方面的强弱，因此选择简单的 function 就可以。
  + 如，Cassandra 和 MongoDB 使用 MD5，Voldemort 使用 Fowler-Noll-Vo 函数。
+ **许多编程语言内置的 hash function 可能不适合分区**。比如 Java 的 Object.hashCode，因为同一个 key 在不同的进程中可能返回不同的 hash value。
:::

hash function 将 key 的定义域映射到了均匀的散列值域：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230121202257888.png" alt="image-20230121202257888" style="zoom:80%;" />

Key Hash 分区的优缺点：

+ 优点：均匀散列能力
+ 缺点：丧失了良好的区间查询特性

一种折中方式：使用**组合索引**的方式，先散列，再顺序。如使用主键进行散列得到分区，在每个分区内使用其他列顺序存储。如在社交网络上，首先按 user_id 进行散列分区，再使用 update_time 对用户事件进行顺序排序，则可以通过 (user_id, update_timestamp) 高效查询某个用户一段事件的事件。

### 2.3 一致性哈希

还有一种常提的哈希方法叫做[一致性哈希](https://zh.m.wikipedia.org/zh-hans/一致哈希)。其特点是，会考虑逻辑分片和物理拓扑，将数据和物理节点按同样的哈希函数进行哈希，来决定如何将哈希分片路由到不同机器上。它可以避免在内存中维护逻辑分片到物理节点的映射，而是每次计算出来。即**用一套算法同时解决了我们在最初提出的逻辑分片和物理路由的两个问题**。 比较经典的数据系统，[Amazon Dynamo](https://www.qtmuniao.com/2020/06/13/dynamo/) 就用了这种方式。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230121203212350.png" alt="image-20230121203212350" style="zoom:80%;" /></center>

如果不使用一致性哈希，我们需要在元数据节点中，维护逻辑分片到物理节点的映射。则在某些物理节点宕机后，需要调整该映射并手动进行数据迁移，而不能像一致性哈希一样，半自动的增量式迁移。

但正如后面“分区再平衡”一节将要介绍的，这种特殊的分区方法对于数据库<u>实际效果并不好</u>，所有目前已很少使用。

::: warning 小结
小结一下，两种分区方式区别在于，一个使用应用相关值（ `Key` ）分区，一个使用应用无关值（`Hash(key)`）分区，前者支持高效范围查询，后者可以均摊负载。但可使用多个字段，组合使用两种方式，使用一个字段进行分区，使用另一个字段在分区内进行排序，兼取两者优点。
:::

### 2.4 负载偏斜和热点消除

基于 Key Hash 的分区方法可以减轻热点，但无法做到完全避免。一个极端情况是：所有的 read-write 操作都是针对同一个 key，则最终的所有请求都被路由到同一个分区了。

如在社交网络中的大 V，其发布的信息，天然会引起同一个键（假设键是用户 id）大量数据的写入，因为可能会有针对该用户信息的大量评论和互动。

**大多数的系统至今仍然无法自动消除这种高度倾斜的负载，而只能通过应用层来减轻倾斜程度**。一种简单的方式是给大 V 用户制造“分身”，也就是在用户主键开始或结尾处添加一个随机数，从而让这个原本对一个 key 的写操作分不到多个不同的 key 上。但这无疑需要应用层做额外的工作，之后的任何读取都必须从所有分身上读取数据再合并返回，也就是说，请求时需要进行拆分，返回时需要进行合并。

也许以后某天，数据库可以自动检测负载倾斜的问题并处理。但目前，仍然需要开发者自己结合应用来综合权衡。