---
title: 分布式设计模式：弹力设计篇
date: 2023-02-06 00:11:36
permalink: /pages/43898a/
categories:
  - 开发
  - 软件设计与架构
  - 专栏：左耳听风
tags:
  - 

---

> 参考：
>
> + [41 弹力设计篇之“认识故障和弹力设计” | 极客时间](https://time.geekbang.org/column/article/3912)
> + [42 弹力设计篇之“隔离设计” | 极客时间](https://time.geekbang.org/column/article/3917)
> + [43 弹力设计篇之“异步通讯设计” | 极客时间](https://time.geekbang.org/column/article/3926)
> + [44 弹力设计篇之“幂等性设计” | 极客时间](https://time.geekbang.org/column/article/4050)

接下来的几章将主要谈一下分布式系统中一些比较关键的设计模式，其中包括容错、性能、管理等几个方面：

+ **容错设计又叫弹力设计**，其中着眼于分布式系统的各种“容忍”能力，包括容错能力（服务隔离、异步调用、请求幂等性）、可伸缩性（有 / 无状态的服务）、一致性（补偿事务、重试）、应对大流量的能力（熔断、降级）。可以看到，在确保系统正确性的前提下，系统的可用性是弹力设计保障的重点。
+ **管理篇**会讲述一些管理分布式系统架构的一些设计模式，比如网关方面的，边车模式，还有一些刚刚开始流行的，如 Service Mesh 相关的设计模式。
+ **性能设计篇**会讲述一些缓存、CQRS、索引表、优先级队列、业务分片等相关的架构模式。

这一章先讲弹力设计部分。

## 1. 认识故障和弹力设计

### 1.1 系统的可用性测量

对于分布式系统的容错设计，在英文中又叫 **Resiliency**（弹力）。意思是，系统在不健康、不顺，甚至出错的情况下有能力 hold 得住，挺得住，还有能在这种逆境下力挽狂澜的能力。

容错主要是为了可用性，那么，我们是怎样计算一个系统的可用性的呢？下面是一个工业界里使用的一个公式：

$$Availability = \frac{MTTF}{MTTF + MTTR}$$

其中：

+ **MTTF** 是 Mean Time To Failure，平均故障前的时间，即系统平均能够正常运行多长时间才发生一次故障。系统的可靠性越高，MTTF 越长。（注意：从字面上来说，看上去有 Failure 的字样，但其实是正常运行的时间。）
+ **MTTR** 是 Mean Time To Recovery，平均修复时间，即从故障出现到故障修复的这段时间，这段时间越短越好。

这个公式就是计算系统可用性的，也就是我们常说的，多少个 9，如下表所示。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230212133317845.png" alt="image-20230212133317845" style="zoom:67%;" /></center>

根据上面的这个公式，为了提高可用性，我们要么提高系统的无故障时间，要么减少系统的故障恢复时间。

SLA 的几个 9 就是能持续提供可用服务的级别。

### 1.2 故障原因

在工业界中，会把服务不可用的因素分成两种：一种是有计划的，一种是无计划的。

+ **无计划的**：

  + 系统级故障，包括主机、操作系统、中间件、数据库、网络、电源以及外围设备。
  + 数据和中介的故障，包括人员误操作、硬盘故障、数据乱了。
  + 还有自然灾害、人为破坏，以及供电问题等。

+ **有计划的**：

  + 日常任务：备份，容量规划，用户和安全管理，后台批处理应用。
  + 运维相关：数据库维护、应用维护、中间件维护、操作系统维护、网络维护。
  + 升级相关：数据库、应用、中间件、操作系统、网络，包括硬件升级。

### 1.3 故障不可避免

对于大规模的分布式系统，出现故障基本上就是常态，甚至还有些你根本就不知道会出问题的地方。如果你在云平台上，或者使用了“微服务”，面对大量的 IoT 设备以及不受控制的用户流量，那么系统故障会更为复杂和变态。因为上面这些因素增加了整个系统的复杂度。

所以，要充分地意识到下面两个事：

+ 故障是正常的，而且是常见的。
+ 故障是不可预测突发的，而且相当难缠。

这告诉我们，**不要尝试着去避免故障，而是要把处理故障的代码当成正常的功能做在架构里写在代码里**。这就是为什么我们把这个设计叫做弹力（Resiliency）。

+ 一方面，在好的情况下，这个事对于我们的用户和内部运维来说是完全透明的，系统自动修复不需要人的干预。
+ 另一方面，如果修复不了，系统能够做自我保护，而不让事态变糟糕。

这就是所谓的“弹力”——能上能下。这让我想到三国杀里赵云的技能——“**能进能退乃真正法器**”。

### 1.3 小结

这一节主要讲了弹力设计的真正目的，并对系统可用性的衡量指标和故障的各种原因有所了解。

## 2. 隔离设计

隔离设计对应的单词是 Bulkheads，中文翻译为隔板，原意是船舱中防止漏水蔓延的隔板。在软件设计中，也是为了防止故障蔓延，来使用隔板对架构隔离故障。

在分布式软件架构中，我们需要对系统进行分离，一般来说有两种方式：

+ 以服务的种类来做分离
+ 以用户来做分离

### 2.1 按服务的种类来做分离

下面这个图中，说明了按服务种类来做分离的情况：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230213102139.png" alt="20230213102139" style="zoom:75%;" /></center>

上图中，我们将系统分成了用户、商品、社区三个板块。这三个块分别使用**不同的域名、服务器和数据库**，做到从接入层到应用层再到数据层三层完全隔离。这样一来，**在物理上来说，一个板块的故障就不会影响到另一板块**。

> 在亚马逊，每个服务都有自己的一个数据库，每个数据库中都保存着和这个业务相关的数据和相应的处理状态。而每个服务从一开始就准备好了对外暴露。同时，这也是微服务所推荐的架构方式。

然而任何架构都有其好和不好的地方，上面这种架构虽然在系统隔离上做得比较好，但是也存在以下一些**问题**：

+ 如果我们需要同时获得多个板块的数据，那么就需要调用多个服务，这会**降低性能**。注意，这里性能降低指的是响应时间，而不是吞吐量（相反，在这种架构下，吞吐量可以得到提高）。
  + 对于这样的问题，一般来说，我们需要小心地设计用户交互，**最好不要让用户在一个页面上获得所有的数据**。对于目前的手机端来说，因为手机屏幕尺寸比较小，所以，也不可能在一个屏幕页上展示太多的内容。
+ 如果有大数据平台，就需要把这些数据都抽取到一个数据仓库中进行计算，这也增加了数据合并的复杂度。对于这个问题，我们需要一个框架或是一个中间件来对数据进行相应的抽取。
+ 如果我们的业务逻辑或是业务流程需要跨板块的话，那么一个板块的故障也会导致整个流程走不下去，同样会导致整体业务故障。
  + 对于这个问题，一方面，我们需要保证这个业务流程中各个子系统的高可用性，并且在业务流程上做成 **Step-by-Step** 的方式，这样用户交互的每一步都可以保存，以便故障恢复后可以继续执行，而不是从头执行。
+ 如果需要有跨板块的交互也会变得有点复杂。对此我们需要一个类似于 Pub/Sub 的高可用、且可以持久化的消息订阅通知中间件来打通各个板块的数据和信息交换。
+ 最后还会有在多个板块中分布式事务的问题。对此，我们需要“二阶段提交”这样的方案。

可见，隔离了的系统在具体的业务场景中还是有很多问题的，是需要我们小心和处理的。对此，我们不可掉以轻心。根据我的经验，这样的系统通常会引入大量的异步处理模型。

### 2.2 按用户的请求来做分离

下图是一个按用户请求来做分离的图示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230213102824.png" alt="20230213102824" style="zoom:75%;" /></center>

在这个图中，可以看到，我们将用户分成不同的组，并把后端的**同一个服务根据这些不同的组分成不同的实例**。让同一个服务对于不同的用户进行冗余和隔离，这样一来，当服务实例挂掉时，只会影响其中一部分用户，而不会导致所有的用户无法访问。

这种分离和上面按功能的分离可以融合。说白了，这就是所谓的“<mark>多租户</mark>”模式。对于一些比较大的客户，我们可以为他们设置专门独立的服务实例，或是服务集群与其他客户隔离开来，对于一些比较小的用户来说，可以让他们共享一个服务实例，这样可以节省相关的资源。

对于“多租户”的架构来说，会引入一些系统设计的复杂度。一方面，如果完全隔离，资源使用上会比较浪费，如果共享，又会导致程序设计的一些复杂度。通常来说多租户的做法有三种：

1. 完全独立的设计。每个租户有自己完全独立的服务和数据。
2. 独立的数据分区，共享的服务。多租户的服务是共享的，但数据是分开隔离的。
3. 共享的服务，共享的数据分区。每个租户的数据和服务都是共享的。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230213103156.png" alt="20230213103156" style="zoom:75%;" /></center>

通过上图，可以看到：

+ 如果使用完全独立的方案，在开发实现上和资源隔离度方面会非常好，然而，成本会比较高，计算资源也会有一定的浪费。
+ 如果使用完全共享的方案，在资源利用和成本上会非常好，然而，开发难度非常大，而且数据和资源隔离非常不好。

一般来说，技术方案会使用**折中方案**，也就是中间方案，服务是共享的，数据通过分区来隔离，而对于一些比较重要的租户（需要好的隔离性），则使用完全独立的方式。

然而，在虚拟化技术非常成熟的今天，我们完全可以使用“完全独立”（完全隔离）的方案，通过底层的虚拟化技术（Hypervisor 的技术，如 KVM，或是 Linux Container 的技术，如 Docker）来实现物理资源的共享和成本的节约。

### 2.3 隔离设计的重点

要能做好隔离设计，我们需要有如下的一些设计考量：

1. 我们需要定义好隔离业务的大小和粒度，过大和过小都不好。这需要认真地做业务上的需求和系统分析。
2. 无论是做系统板块还是多租户的隔离，你都需要考虑系统的复杂度、成本、性能、资源使用的问题，找到一个合适的均衡方案，或是分布实施的方案尤其重要，这其中需要你定义好要什么和不要什么。因为，**我们不可能做出一个什么都能满足的系统**。
3. 隔离模式需要配置一些高可用、重试、异步、消息中间件，流控、熔断等设计模式的方式配套使用。
4. 不要忘记了分布式系统中的运维的复杂度的提升，要能驾驭得好的话，还需要很多自动化运维的工具，尤其是使用像容器或是虚拟机这样的虚拟化技术可以帮助我们更方便地管理，和对比资源更好地利用。否则做出来了也管理不好。
5. 最后，你需要一个非常完整的能够看得到所有服务的监控系统，这点非常重要。

### 2.4 小结

这一节介绍了分布式系统设计中两种常见的隔离设计：

+ 按服务种类隔离
+ 按用户隔离（即多租户）

## 3. 异步通讯设计

当把系统进行解耦拆分，所要面对的一个重要问题就是这些系统间的通讯。

通讯一般来说分同步和异步两种。同步通讯就像打电话，需要实时响应，而异步通讯就像发邮件，不需要马上回复。

---

同步调用虽然让系统间只耦合于接口，而且实时性也会比异步调用要高，但是我们也需要知道**同步调用会带来如下几个问题**：

+ 同步调用需要被调用方的吞吐不低于调用方的吞吐。否则会导致被调用方因为性能不足而拖死调用方。换句话说，**整个同步调用链的性能会由最慢的那个服务所决定**。
+ 同步调用会导致调用方一直在等待被调用方完成，如果一层接一层地同步调用下去，所有的参与方会有相同的等待时间。这会**非常消耗调用方的资源**。
+ 同步调用**只能是一对一**的，很难做到一对多。
+ 最不好的是，如果被调用方有问题，那么其调用方就会跟着出问题，于是会出现**故障的多米诺骨牌效应**，故障一下就蔓延开来。

---

所以，异步通讯相对于同步通讯来说，除了可以增加系统的吞吐量之外，最大的一个好处是其可以让服务间的解耦更为彻底，系统的调用方和被调用方可以按照自己的速率而不是步调一致，从而可以更好地保护系统，让系统更有弹力。

### 3.1 异步通讯的三种方式

#### 3.1.1 请求响应式

<font color=blue>发送方（sender）会直接请求接收方（receiver），被请求方接收到请求后，直接返回——“收到请求，正在处理”</font>。

对于返回结果，有两种方法：

+ 一种是发送方时不时地去轮询一下，问一下干没干完。
+ 另一种方式是发送方注册一个回调方法，也就是接收方处理完后回调请求方。

> 这种架构模型在以前的网上支付中比较常见，页面先从商家跳转到支付宝或银行，商家会把回调的 URL 传给支付页面，支付完后，再跳转回商家的 URL。

很明显，这种情况下还是有一定耦合的。是发送方依赖于接收方，并且要把自己的回调发送给接收方，处理完后回调。

#### 3.1.2 通过订阅的方式

<font color=blue>接收方（receiver）会来订阅发送方（sender）的消息，发送方会把相关的消息或数据放到接收方所订阅的队列中，而接收方会从队列中获取数据</font>。

这种方式下，发送方并不关心订阅方的处理结果，它只是告诉订阅方有事要干，收完消息后给个 ACK 就好了，你干成啥样我不关心。这个方式常用于像 MVC（Model-View-Control）这样的设计模式下，如下图所示。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230213104840.png" alt="20230213104840" style="zoom:75%;" /></center>

这就好像下订单的时候，一旦用户支付完成了，就需要把这个事件通知给订单处理以及物流，订单处理变更状态，物流服务需要从仓库服务分配相应的库存并准备配送，后续这些处理的结果无需告诉支付服务。

**为什么要做成这样**？前一小节的调用方式就像函数调用一样，有数据状态的往来，调用方需要保存调用状态，而如果我们把服务的状态给去掉（通过第三方的状态服务来保证），那么服务间的依赖就只有事件了。你知道，分布式系统的服务设计是需要向**无状态服务**（Stateless）努力的，这其中有太多的好处，无状态意味着你可以非常方便地运维。所以，**事件通讯成为了异步通讯中最重要的一个设计模式**。就上面支付的那个例子，商家这边只需要订阅一个支付完成的事件，这个事件带一个订单号，而不需要让支付方知道自己的回调 URL，这样的异步是不是更干净一些？

但是，在这种方式下，接收方需要向发送方订阅事件，所以是**接收方依赖于发送方**。这种方式还是有一定的耦合。

#### 3.1.3 通过 Broker 的方式

所谓 **Broker**，就是一个中间人，<font color=blue>发送方（sender）和接收方（receiver）都互相看不到对方，它们看得到的是一个 Broker，发送方向 Broker 发送消息，接收方向 Broker 订阅消息</font>。如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230213105355.png" alt="20230213105355" style="zoom:75%;" /></center>

这是完全的解耦。**所有的服务都不需要相互依赖，而是依赖于一个中间件 Broker**。这个 Broker 是一个像数据总线一样的东西，所有的服务要接收数据和发送数据都发到这个总线上，这个总线就像协议一样，让服务间的通讯变得标准和可控。

在 Broker 这种模式下，发送方的服务和接收方的服务最大程度地解耦。但是所有人都依赖于一个总线，所以这个总线就需要有如下的特性：

+ 必须是高可用的，因为它成了整个系统的关键；
+ 必须是高性能而且是可以水平扩展的；
+ 必须是可以持久化不丢数据的。

要做到这三条还是比较难的。当然，好在现在开源软件或云平台上 Broker 的软件是非常成熟的，所以节省了我们很多的精力。

### 3.2 事件驱动设计

上述的第二种和第三种方式就是比较著名的<mark>事件驱动架构</mark>（**EDA** – Event Driven Architecture）。正如前面所说，**事件驱动最好是使用 Broker 方式，服务间通过交换消息来完成交流和整个流程的驱动**。

如下图所示，这是一个订单处理流程。下单服务通知订单服务有订单要处理，而订单服务生成订单后发出通知，库存服务和支付服务得到通知后，一边是占住库存，另一边是让用户支付，等待用户支付完成后通知配送服务进行商品配送：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20230213105944.png" alt="20230213105944" style="zoom:75%;" /></center>

每个服务都是“自包含”的。所谓“<mark>自包含</mark>”也就是没有和别人产生依赖。而要把整个流程给串联起来，我们需要一系列的“消息通道（Channel）”。各个服务做完自己的事后，发出相应的事件，而又有一些服务在订阅着某些事件来联动。

**事件驱动方式的好处**至少有五个：

+ 服务间的依赖没有了，服务间是平等的，每个服务都是高度可重用并可被替换的。
+ 服务的开发、测试、运维，以及故障处理都是高度隔离的。
+ 服务间通过事件关联，所以服务间是不会相互 block 的。
+ 在服务间增加一些 Adapter（如日志、认证、版本、限流、降级、熔断等）相当容易。
+ 服务间的吞吐也被解开了，各个服务可以按照自己的处理速度处理。

**事件驱动的架构也会有一些不好的地方**：

+ 业务流程不再那么明显和好管理。整个架构变得比较复杂。解决这个问题需要有一些可视化的工具来呈现整体业务流程。
+ 事件可能会乱序。这会带来非常 Bug 的事。解决这个问题需要很好地管理一个状态机的控制。
+ 事务处理变得复杂。需要使用两阶段提交来做强一致性，或是退缩到最终一致性。

### 3.3 异步通讯的设计重点

首先，我们需要知道，为什么要异步通讯：

+ 异步通讯最重要的是**解耦服务间的依赖**。最佳解耦的方式是通过 Broker 的机制。
+ 解耦的目的是让各个服务的**隔离性更好**，这样不会出现“一倒倒一片”的故障。
+ 异步通讯的架构可以获得**更大的吞吐量**，而且各个服务间的性能不受干扰相对独立。
+ 利用 Broker 或队列的方式还可以达到把抖动的吞吐量变成均匀的吞吐量，这就是所谓的“**削峰**”，这对后端系统是个不错的保护。
+ **服务相对独立**，在部署、扩容和运维上都可以做到独立不受其他服务的干扰。

但我们需要知道这样的方式带来的问题，所以在设计成异步通信的时候需要注意如下事宜：

+ 用于异步通讯的中间件 Broker 成为了关键，需要设计成**高可用不丢消息**的。另外，因为是分布式的，所以可能很难保证消息的顺序，因此**你的设计最好不依赖于消息的顺序**。
+ 异步通讯会导致业务处理流程不那么直观，因为像接力一样，所以在 Broker 上需要有相关的服务消息跟踪机制，否则出现问题后不容易调试。
+ 因为服务间只通过消息交互，所以业务状态最好由一个总控方来管理，这个总控方维护一个业务流程的状态变迁逻辑，以便系统发生故障后知道业务处理到了哪一步，从而可以在故障清除后继续处理。
  + 这样的设计常见于银行的对账程序，银行系统会有大量的外部系统通讯，比如跨行的交易、跨企业的交易，等等。所以，为了保证整体数据的一致性，或是避免漏处理及处理错的交易，需要有对账系统，这其实就是那个总控，这也是为什么银行有的交易是 T+1（隔天结算），就是因为要对个账，确保数据是对的。
+ 消息传递中，可能有的业务逻辑会有像 TCP 协议那样的 send 和 ACK 机制。比如：A 服务发出一个消息之后，开始等待处理方的 ACK，如果等不到的话，就需要做重传。此时，**需要处理方有幂等的处理**，即同一条消息无论收到多少次都只处理一次。

### 3.4 小结

总结一下这一节。

首先，同步调用有四个问题：影响吞吐量、消耗系统资源、只能一对一，以及有多米诺骨牌效应。于是，我们想用异步调用来避免该问题。

异步调用有三种方式：请求响应、直接订阅和中间人订阅。

最后，我介绍了事件驱动设计的特点和异步通讯设计的重点。

## 4. 幂等性设计

