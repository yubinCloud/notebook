---
title: Thrift
permalink: /pages/distributed/bigdata-paper-intro/Thrift/
categories: 
  - 开发
  - 大数据与分布式系统
  - 专栏-大数据经典论文导读
tags: 
  - null
date: 2023-08-25 13:37:00
---

这一讲主要介绍的是 Facebook 在 2007 年发表的 *Thrift: Scalable Cross-Language Services Implementation* 论文，它的背后是 Apache Thrift 这个开源项目。

Thrift 是在大数据处理，以及分布式系统中长期使用的一个开源项目，相比于 Google 的 Protobuf 和 gRPC，Thrift 更早发表。

这一讲将带着你从最简单的 CSV 格式开始，根据需求一步步优化扩展，看看为什么我们需要 Thrift 里像 TCompactProtocol 这样的编码格式，然后我会带你来理解它是如何设计，做到可以跨语言、跨协议、可扩展的。

通过这一讲的学习，相信你在对如何进行高效的数据序列化和反序列化，以及让系统设计的“正交化”这两点上，能有充分的收获。

## 1. 常用的 CSV 和 JSON 格式

在之前 Google 的 MapReduce 论文中，输入输出都是 string，然后由 developer 去决定做数据的 encode 和 decode，比如可以是 JSON，可以是 CSV。

以 CSV 为例，使用这个格式有两个缺点：

1. **数据里面没有告诉我们数据类型是什么**
2. **很多数据使用文本来保存有些浪费空间**

JSON 可以解决第一个缺点，但当数据类型为 `List<Student>` 这种时，每个 object 数据在 JSON 都要存储一份字段名，占用的空间就太大了。

事实上，CSV 也好，JSON 也好，乃至 XML 也好，这些针对结构化数据进行编码主要想解决的问题是提升开发人员的效率，所以重视的是数据的“人类可读性”。因为在小数据量的情况下，程序员的开发效率是核心问题，多浪费一点存储空间算不了什么。**但是在“大数据”的场景下，除了程序员的效率，存储数据本身的“效率”就变得非常重要了**。

## 2. 二进制序列化

想要减少存储所占的空间，最直接的方法就是自定义一个序列化方法，然后按照各个字段实际的格式把数据写进去。

典型的方法就是 **Java 的序列化**，把数据写入到一个 byte 数组中，等需要读数据的时候，我们就再按照这个顺序读出来就好了。一个通过 Java 进行序列化的示例：

```java
import java.io.*;
import java.util.Arrays;
public class Main {
    public static void main(String[] args) throws IOException {
        ByteArrayOutputStream buffer = new ByteArrayOutputStream();
        try (ObjectOutputStream output = new ObjectOutputStream(buffer)) {
            output.writeUTF("597819210");
            output.writeUTF("大数据");
            output.writeInt(4);
            output.writeUTF("https://aws.amazon.com/cn/big-data/what-is-big-data/");
            output.writeInt(1592373781);
        }
        System.out.println(Arrays.toString(buffer.toByteArray()));
    }
}
```

这个方法的确确保了数据都有类型，并且占用的存储空间尽可能小。但这种方法有一个**重大缺点：读写 schema 是隐式包含在代码中的**。每当有一个新的数据结构，那我们就要手写序列化和反序列化的代码。

那么，有没有什么解决办法呢？

## 3. 包含 IDL 并能向前和向后兼容的 Thrift 的 TBinaryProtocol

