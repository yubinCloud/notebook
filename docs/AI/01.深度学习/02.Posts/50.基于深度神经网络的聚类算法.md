---
title: 基于深度神经网络的聚类算法
date: 2022-12-29 12:56:40
permalink: /pages/9f2f15/
categories:
  - AI
  - 深度学习
  - Posts
tags:
  - 
---

> 参考视频：[第40期：基于深度神经网络的聚类算法 —— 郭西风](https://www.bilibili.com/video/BV1H3411t7Vk/)
>
> 更多资料：
>
> + [深度聚类算法研究综述 - 博客园](https://www.cnblogs.com/kailugaji/p/15574267.html)
> + [基于深度神经网络的图像聚类算法研究 - 知网](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDTEMP&filename=1021828356.nh&uniplatform=NZKPT&v=hlOUXPz_daQmnARtQc3G4KvlOmEz3mH9CimThSP9ufN0TQ_UoRp_XOAvO8rbCTsC)
> + [物以类聚人以群分：聚类分析的一些挑战和进展 - 博客园](https://www.cnblogs.com/kailugaji/p/14340602.html)
> + [Deep Clustering | Deep Learning Notes](https://deepnotes.io/deep-clustering)

## 1. Background

### 1.1 什么是聚类？

物以类聚，人以群分：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221229135048148.png" alt="image-20221229135048148" style="zoom:80%;" />

### 1.2 什么是深度聚类？

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221229135149171.png" alt="image-20221229135149171" style="zoom:80%;" />

+ 传统聚类存在的问题主要是，这种手工设计的 feature extractor 得到的 feature 并不一定是好的 feature，而且 extractor 与聚类模型也没有交互
+ 深度聚类在提取 feature 的时候就已经会考虑 clustering 的目的，两个过程会相互促进，从而达到好的效果。

因此深度聚类的**核心思想**：学习到的高质量的特征有助于提升聚类算法的性能，而聚类结果反过来可以引导神经网络学习更好的特征。其流程与有监督深度学习类似，都是同时完成特征学习任务和后续任务（分类、回归）。

**深度聚类的一般范式**：网络损失与聚类损失的组合

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221229135712368.png" alt="image-20221229135712368" style="zoom:80%;" /></center>

$$\color{red}{L = \alpha L_n + \beta L_c}, \alpha \ge 0, \beta > 0$$

### 1.3 从两个视角来看 Deep Clustering

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221229140116041.png" alt="image-20221229140116041" style="zoom:80%;" />

## 2. 从聚类模型看 Deep Clustering

### 2.1 基于 K-Means 的 Deep Clustering

先看一下 K-Means 是怎么做的：

::: theorem review: K-Means
给定 n 个样本 x 和 K 个初始化的聚类中心 $\mu$，K-Means 通过最小化*类内均方误差*实现对样本的划分。对应的损失函数为：

$$L_c(\mu, s) = \sum^n_{i=1} \sum^K_{j=1} s_{ij} ||x_i - \mu_j||^2_2$$

其中，$s_{ij}$ 指示样本 i 是否属于类别 j，是为 1，否为 0.
:::

+ 这里的损失其实就是每个样本距离它最近聚类中心的距离的累加
+ K-means 所要优化的是聚类的中心 $\mu$ 和样本的标签 s

这种方式存在的一个问题是，x 是一个样本 feature，如果 x 在它所在的 feature space 中不容易被区分，那整体的聚类效果将不太好。所以**一个想法是如果 x 经过某种变换，变换到一个新的空间里面，如果在新的 space 里面比较容易做 clustering，那效果就会不错**，而基于神经网络的 deep clustering 其实就是这种思路，这种 feature space 的变换就交给了神经网络。

用 $f_w$ 表示以 w 为参数的神经网络，则基于 K-means 的 deep clustering 的 loss function 如下：

$$L_c(\textbf{w}, \mu, s) = \sum^n_{i=1} \sum^K_{j=1} s_{ij} ||f_{\textbf{w}}(x_i) - \mu_j||^2_2$$

这里需要使用 loss 同时对 $\textbf{w}、\mu、s$ 进行优化更新，实现特征学习与聚类过程的联合训练。

但由于神经网络的映射能力特别强，**直接最小化这个 loss function 有可能得到退化解**：神经网络 $f_w$ 将所有样本 x 都映射到同一个点，此时损失函数为 0，但所有样本都在同一个类里。

因此需要加入额外约束消除退化解，比如加入网络损失 $L_n$<sup>[1-4]</sup>，或显式约束样本在各类均匀分布<sup>[5]</sup>。

优缺点：

+ **优点**：简单直观，与单独使用 KMeans 相比聚类性能有较大幅度提升。
+ **缺点**：继承了 KMeans 受初始化影响大、不能处理簇形状非凸的数据、无法得到全局最优解等

> 参考文献：
>
> [1] YANG B, et al. Towards kmeansfriendly spaces: Simultaneous deep learning and clustering[C]//ICML. 2017: 3861-3870.
> [2] TIAN K, et al. Deepcluster: A general clustering framework based on deep learning[C]//ECML/PKDD. 2017: 809-825.
> [3] ALQAHTANI A, et al. A deep convolutional autoencoder with embedded clustering[C]//ICIP. 2018: 4058-4062.
> [4] MA Q, et al. Learning representations for time series clustering[C]//NeurIPS. 2019: 3776-3786.
> [5] CARON M, et al. Deep clustering for unsupervised learning of visual features[C]//ECCV. 2018: 139-156.

### 2.2 基于谱聚类的深度聚类

review 一下 spectral clustering，它把聚类的过程转换成了一个图分割的问题。

::: theorem Spectral Clustering
给定样本集 x 和聚类个数 K 后，先根据样本之间的距离构建相似性矩阵 A，然后通过最小化下列损失
函数求解谱嵌入特征 Z：

$$L_c(Z) = Tr(Z^TLZ) = \sum_{ij}A_{ij}||z_i - z_j||^2,  s.t. Z^T Z = I$$

其中 $z_i$ 是 Z 的第 i 行，对应第 i 个样本 $x_i$ 的谱嵌入特征。
:::

扩展到 deep clustering 也很自然，谱聚类并没有显式求出样本 $x_i$ 到特征 $z_i$ 的映射 $f$，很自然的想法是使用深度神经网络来实现映射 $f$，使用以下的 loss function 来训练神经网络 $f_{\textbf{w}}$，称为 <mark>SpectralNet</mark>：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221229143141321.png" alt="image-20221229143141321" style="zoom: 67%;" /></center>

它的**核心思想**就是：用神经网络来显式地刻画样本 x 到它谱空间的嵌入。

当然它也存在不少问题：

+ 性能受限于相似性矩阵 A 的质量，Yang等<sup>[1]</sup>使用自编码器的嵌入层特征作为 SpectralNet<sup>[2]</sup> 输入。
+ Huang等<sup>[3]</sup>将 SpectralNet 扩展到多视图场景。
+ Yang等<sup>[4]</sup>扩展了谱嵌入的方式，由原来的最小化在嵌入空间的欧式距离变为最小化样本之间的
  后验概率分布

但是以上的改进的核心思想仍然不变。

优缺点：

+ **优点**：通过显式求解特征映射，可以使用批量训练的策略，提高向大规模数据的可扩展性。同时与基于 K-Means 的深度聚类算法相比，能充分利用数据的拓扑结构，实现对非凸数据的聚类。
+ **缺点**：显式求解的特征映射不能保证是全局最优的。使用同样的相似性矩阵 A，性能更差。

> 参考文献：
>
> [1] YANG X, et al. Deep spectral clustering using dual autoencoder network[C]//CVPR. 2019:4066-4075.
> [2] SHAHAM U, et al. Spectralnet: Spectral clustering using deep neural networks[C]//ICLR. 2018.
> [3] HUANG S, et al. Multi-spectralnet: Spectral clustering using deep neural network for multiview data[J]. IEEE Transactions
> on Computational Social Systems, 2019, 6:749-760.
> [4] YANG L, et al. Deep clustering by gaussian mixture variational autoencoders with graph embedding[C]//ICCV. 2019:6439-6448.

### 2.3 基于子空间聚类的深度聚类

// TODO