---
title: Data Efficient & Parameter-Efficient Tuning
date: 2022-10-29 20:28:58
permalink: /pages/lhy/data-efficient/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅
tags:
  - 
---

## 1. Background: Pre-trained Language Models

### 1.1 什么是 Language Model？

<mark>Neural Language Models</mark>: A neural network that defines the probability over sequences of words.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210149304.png" alt="image-20221029210149304" style="zoom:90%;" />

### 1.2 怎样训练的这些 model？

Given an incomplete sentence, predict the rest of the sentence.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210325093.png" alt="image-20221029210325093" style="zoom:90%;" />

不完整的句子怎么构造呢？根据不完整的句子的构造方式，可以将 Language Model 的训练分成两种：

+ **Autoregressive Language Model**（<mark>ALMs</mark>）: Complete the sentence given its prefix.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210716452.png" alt="image-20221029210716452" style="zoom:80%;" />

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210806588.png" alt="image-20221029210806588" style="zoom:80%;" />

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210806588.png" alt="image-20221029210806588" style="zoom:80%;" />

我们看一下 Transformer-based PLM 长什么样子：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029211126556.png" alt="image-20221029211126556" style="zoom:90%;" /></center>

上图中，“气”这个字经过一系列 layer，得到了它的 embedding，然后把这个 embedding 输入到一个 LM Head 中，可以得到预测下一个 token 的概率。

训练一个 Language Model 的方式就是 self-supervised learning，但它没有一个明确的定义，这里我们说：

<mark>Self-supervised learning</mark>: Predicting any part of the input from any other part.

还存在另外一种 Language Model，即 Masked Language Models（MLMs）：

**Masked Language Models**（<mark>MLMs</mark>）: Use the unmarked words to predict the masked word.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029211842262.png" alt="image-20221029211842262" style="zoom:80%;" />

### 1.3 Pre-trained Language Models（PLMs）

**Pre**-training: Using a large corpora to train a neural language model.

+ Autoregressive pre-trained: GPT 系列（GPT, GPT-2, GPT-3）
+ MLM-based pre-trained: BERT 系列（BERT, RoBERTa, ALBERT）

为什么要这样做呢？We believe that after pre-training, the PLM learns some knowledge, encoded in its hidden representations, that can transfer to downstream tasks.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029214502721.png" alt="image-20221029214502721" style="zoom:72%;" />

**fine-tuning**: Using the pre-trained weights of the PLM to initialize a model for a downstream task.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029214742084.png" alt="image-20221029214742084" style="zoom: 67%;" />

  PLMs has shown great success on a variety of benchmark datasets in NLP. **The next goal is to make PLMs fit in real-life use case**. 但我们将 PLMs 用到现实情况时，却会遇到各种问题。

## 2. The problem of PLMs

### 2.1 Problem 1: <mark>Data scarcity</mark> in downstream tasks

A large amount of labeled data is not easy to obtain for each downstream task. 下面是一个训练 BERT 所用的 dataset：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161440398.png" alt="image-20221030161440398" style="zoom:80%;" />

这里面的数据集最少都是几千级的，但在现实中想要弄到这么多的数据还是非常困难的。

### 2.2 Problem 2: The PLM is too big

The PLM is too big, and they are still getting bigger:

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161617975.png" alt="image-20221030161617975" style="zoom:80%;" />

在实际应用时，这么大的模型，我们需要为每个 downstream 都弄一份 copy，这会特别占据空间：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161959783.png" alt="image-20221030161959783" style="zoom:80%;" />

而且这么多层的模型，计算一次 inference 都需要花费特别多的时间。

因此，模型越来越大的问题可以总结为两个：

+ Inference takes too long.
+ Consume too much space.



## 3. Labeled Data Scarcity -> Data-Efficient Fine-tuning

### 3.1 Prompt Tuning

#### 3.1.1 什么是 Prompt Tuning？

以往在做 natural language inference 时，我们往往会这么做：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165310825.png" alt="image-20221030165310825" style="zoom:70%;" />

但如果 training data 较少的话，这往往是难以做出来的：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165458079.png" alt="image-20221030165458079" style="zoom: 70%;" />

此时一种方法是，都加上一句 “Is is true that” 来表示询问后面这个句子与前面句子的关系，如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165701921.png" alt="image-20221030165701921" style="zoom:70%;" />

这个东西就是 <mark>Prompt Tuning</mark> 的核心概念，也就是设置一些东西告诉 model 我们在做什么。所以说，By converting the data points in the dataset into natural language prompts, the model may be easier to know what it should do.

什么是 Prompt Tuning 呢？<u>Format the downstream task as a language modelling task with predefined</u>
<u>templates into natural language **prompts**</u>.

#### 3.1.2 Prompt Tuning 需要什么

What you need in prompt tuning:

1. A prompt template
2. A PLM
3. A verbalizer

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030171916101.png" alt="image-20221030171916101" style="zoom: 80%;" /></center>

##### 1）A prompt template

A <mark>prompt template</mark>: convert data points into a natural language prompt.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172215732.png" alt="image-20221030172215732" style="zoom:80%;" />

在得到 natural language prompt 后，就可以将它输入到 PLM 中，来预测 [MASK] 的部分是什么。

##### 2）A PLM

A <mark>PLM</mark>: perform language modeling.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172527863.png" alt="image-20221030172527863" style="zoom:80%;" />

##### 3）A verbalizer

A <mark>verbalizer</mark>: A mapping between the label and the vocabulary. For example, which vocabulary should represents the class “entailment”:

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172809882.png" alt="image-20221030172809882" style="zoom:80%;" />

然后在神经网络中，我们就可以这么干了：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172845084.png" alt="image-20221030172845084" style="zoom:80%;" />

#### 3.1.3 Prompt tuning v.s. Standard fine-tuning

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030173244999.png" alt="image-20221030173244999" style="zoom:80%;" /></center>

+ 在 standard fine-tuning 中，我们会丢掉 LM Head 并重新 initialize 一个 Classifier Head；
+ 而在 prompt tuning 中，我们就是要利用 language model 的能力，因此不会丢弃这个 language model 的 head。

Prompt tuning has better performance under data scarcity **because**：

+ It incorporates human knowledge（因为 prompt template 的设计本身就融入了 human knowledge）
+ It introduces no new parameters

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102201439206.png" alt="image-20221102201439206" style="zoom:73%;" /></center>

下面，Lets see how prompts can help us under different level of data scarcity.

### 3.2 Few-shot Learning

<mark>Few-shot Learning</mark>: We have **some** labeled training data.

但 Few-shot Learning 也是一个没有明确定义的词，具体多少是 few-shot，并没有具体的范围，在这里假设 few-shot 是指的 “Some ≈ 10GB training data”。

Good News 是 GPT-3 可以被用于做 few-shot learning，但 bad news 是：GPT-3 is not freely available and contains 175B parameters.

Can we use smaller PLMs and make them to perform well in few-shot learning?

<mark>LM-BFF</mark>: **b**etter **f**ew-shot **f**ine-tuning of **l**anguage **m**odels. 它的核心概念：**prompt** + **demonstration**。

prompt 是指下面这个样子：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102203929560.png" alt="image-20221102203929560" style="zoom:75%;" />

而 <mark>demonstration</mark> 是说，我要让 model 知道，当它看到这样的 prompt 之后，它该去怎么做。所以 demonstration 的做法就是，在 prompt 的部分后面加了两个 demonstration 的句子：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102204222689.png" alt="image-20221102204222689" style="zoom:80%;" />

给他一个正面的 review，当 model 看到正面的 review 之后，它应该知道后面 “It was \_\_\_” 这里应该填 “great”，类似的，当他看到负面的 review 之后，它应该知道后面的 “It was \_\_\_” 这里应该填 “terrible”。 这样的形式就可以更加帮助 language model 在 few-shot learning 上面的表现。

其 performance 如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102210720916.png" alt="image-20221102210720916" style="zoom:67%;" />

### 3.3 Semi-supervised Learning

<mark>Semi-supervised Learning</mark>: We have some labeled training data and a large amount of unlabeled data.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102212710576.png" alt="image-20221102212710576" style="zoom:80%;" />

这里主要看一下：[It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://aclanthology.org/2021.naacl-main.185/?utm_campaign=%E6%AF%8E%E9%80%B1%20NLP%20%E8%AB%96%E6%96%87&utm_medium=email&utm_source=Revue%20newsletter)，因为 GPT-3 的论文就是 [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)，所以前者就像有点在打脸 GPT-3 的样子。

它提出的方法叫做 **Pattern-Exploiting Training** (<mark>PET</mark>)，具体分成了三个步骤：

+ Step 1: Use different prompts and verbalizer to prompt-tune different PLMs on the labeled dataset.

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221104221640334.png" alt="image-20221104221640334" style="zoom:67%;" /></center>

+ Step 2: Predict the unlabeled dataset and combine the predictions from different models.

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221104222639420.png" alt="image-20221104222639420" style="zoom:67%;" /></center>

对同一笔数据，不同 prompt-tune 得到的 model 给出的 prediction 也许是不一样的，我们拿到这些 prediction 后再将他们 combine 到一起，在这里可以只是简单地相加。

+ Step 3: Use a PLM with classifier head to train on the soft-labeled data set.

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221104223312301.png" alt="image-20221104223312301" style="zoom:67%;" /></center>

这一步是在所有的 dataset 进行 fine-tuning，对于 labeled data 则是用的原 label，而对于 unlabeled data，则用的是 soft label，即我们在 step 2 中得到的 label，然后进行 standard fine-tuning，也就是拿掉 LM Head 再加一个 Classifier Head 进行 fine-tuning。

### 3.4 Zero-shot Learning

// TODO