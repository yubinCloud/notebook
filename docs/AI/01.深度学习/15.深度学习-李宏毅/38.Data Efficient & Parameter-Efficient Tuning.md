---
title: Data Efficient & Parameter-Efficient Tuning
date: 2022-10-29 20:28:58
permalink: /pages/lhy/data-efficient/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅
tags:
  - 
---

## 1. Background: Pre-trained Language Models

### 1.1 什么是 Language Model？

<mark>Neural Language Models</mark>: A neural network that defines the probability over sequences of words.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210149304.png" alt="image-20221029210149304" style="zoom:90%;" />

### 1.2 怎样训练的这些 model？

Given an incomplete sentence, predict the rest of the sentence.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210325093.png" alt="image-20221029210325093" style="zoom:90%;" />

不完整的句子怎么构造呢？根据不完整的句子的构造方式，可以将 Language Model 的训练分成两种：

+ **Autoregressive Language Model**（<mark>ALMs</mark>）: Complete the sentence given its prefix.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210716452.png" alt="image-20221029210716452" style="zoom:80%;" />

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210806588.png" alt="image-20221029210806588" style="zoom:80%;" />

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210806588.png" alt="image-20221029210806588" style="zoom:80%;" />

我们看一下 Transformer-based PLM 长什么样子：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029211126556.png" alt="image-20221029211126556" style="zoom:90%;" /></center>

上图中，“气”这个字经过一系列 layer，得到了它的 embedding，然后把这个 embedding 输入到一个 LM Head 中，可以得到预测下一个 token 的概率。

训练一个 Language Model 的方式就是 self-supervised learning，但它没有一个明确的定义，这里我们说：

<mark>Self-supervised learning</mark>: Predicting any part of the input from any other part.

还存在另外一种 Language Model，即 Masked Language Models（MLMs）：

**Masked Language Models**（<mark>MLMs</mark>）: Use the unmarked words to predict the masked word.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029211842262.png" alt="image-20221029211842262" style="zoom:80%;" />

### 1.3 Pre-trained Language Models（PLMs）

**Pre**-training: Using a large corpora to train a neural language model.

+ Autoregressive pre-trained: GPT 系列（GPT, GPT-2, GPT-3）
+ MLM-based pre-trained: BERT 系列（BERT, RoBERTa, ALBERT）

为什么要这样做呢？We believe that after pre-training, the PLM learns some knowledge, encoded in its hidden representations, that can transfer to downstream tasks.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029214502721.png" alt="image-20221029214502721" style="zoom:72%;" />

**fine-tuning**: Using the pre-trained weights of the PLM to initialize a model for a downstream task.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029214742084.png" alt="image-20221029214742084" style="zoom: 67%;" />

  PLMs has shown great success on a variety of benchmark datasets in NLP. **The next goal is to make PLMs fit in real-life use case**. 但我们将 PLMs 用到现实情况时，却会遇到各种问题。

## 2. The problem of PLMs

### 2.1 Problem 1: <mark>Data scarcity</mark> in downstream tasks

A large amount of labeled data is not easy to obtain for each downstream task. 下面是一个训练 BERT 所用的 dataset：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161440398.png" alt="image-20221030161440398" style="zoom:80%;" />

这里面的数据集最少都是几千级的，但在现实中想要弄到这么多的数据还是非常困难的。

### 2.2 Problem 2: The PLM is too big

The PLM is too big, and they are still getting bigger:

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161617975.png" alt="image-20221030161617975" style="zoom:80%;" />

在实际应用时，这么大的模型，我们需要为每个 downstream 都弄一份 copy，这会特别占据空间：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161959783.png" alt="image-20221030161959783" style="zoom:80%;" />

而且这么多层的模型，计算一次 inference 都需要花费特别多的时间。

因此，模型越来越大的问题可以总结为两个：

+ Inference takes too long.
+ Consume too much space.



## 3. Labeled Data Scarcity -> Data-Efficient Fine-tuning

### 3.1 Prompt Tuning

#### 3.1.1 什么是 Prompt Tuning？

以往在做 natural language inference 时，我们往往会这么做：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165310825.png" alt="image-20221030165310825" style="zoom:70%;" />

但如果 training data 较少的话，这往往是难以做出来的：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165458079.png" alt="image-20221030165458079" style="zoom: 70%;" />

此时一种方法是，都加上一句 “Is is true that” 来表示询问后面这个句子与前面句子的关系，如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165701921.png" alt="image-20221030165701921" style="zoom:70%;" />

这个东西就是 <mark>Prompt Tuning</mark> 的核心概念，也就是设置一些东西告诉 model 我们在做什么。所以说，By converting the data points in the dataset into natural language prompts, the model may be easier to know what it should do.

什么是 Prompt Tuning 呢？<u>Format the downstream task as a language modelling task with predefined</u>
<u>templates into natural language **prompts**</u>.

#### 3.1.2 Prompt Tuning 需要什么

What you need in prompt tuning:

1. A prompt template
2. A PLM
3. A verbalizer

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030171916101.png" alt="image-20221030171916101" style="zoom: 80%;" /></center>

##### 1）A prompt template

A <mark>prompt template</mark>: convert data points into a natural language prompt.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172215732.png" alt="image-20221030172215732" style="zoom:80%;" />

在得到 natural language prompt 后，就可以将它输入到 PLM 中，来预测 [MASK] 的部分是什么。

##### 2）A PLM

A <mark>PLM</mark>: perform language modeling.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172527863.png" alt="image-20221030172527863" style="zoom:80%;" />

##### 3）A verbalizer

A <mark>verbalizer</mark>: A mapping between the label and the vocabulary. For example, which vocabulary should represents the class “entailment”:

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172809882.png" alt="image-20221030172809882" style="zoom:80%;" />

然后在神经网络中，我们就可以这么干了：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172845084.png" alt="image-20221030172845084" style="zoom:80%;" />

#### 3.1.3 Prompt tuning v.s. Standard fine-tuning

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030173244999.png" alt="image-20221030173244999" style="zoom:80%;" /></center>

+ 在 standard fine-tuning 中，我们会丢掉 LM Head 并重新 initialize 一个 Classifier Head；
+ 而在 prompt tuning 中，我们就是要利用 language model 的能力，因此不会丢弃这个 language model 的 head。

