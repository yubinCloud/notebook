---
title: ChatGPT 是怎样炼成的
date: 2022-12-15 18:43:18
permalink: /pages/595df8/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅
tags:
  - 
---

截止到 22 年底，OpenAI 官方并未放出 ChatGPT 的论文，但是 blog 中提到：“ChatGPT is a sibling model to InstructGPT”，因此这里主要是按照 InstructGPT 的方式来讲解。

> 对比一下 InstructGPT 和 ChatGPT 的训练步骤，两者真的非常非常相似。

ChatGPT 学习的四个阶段：

1. 学习文字接龙
2. 人类老师引导文字接龙的方向
3. 模仿人类老师的爱好
4. 用强化学习向模拟的老师学习

## 1. ChatGPT 训练的四个阶段

### 阶段 1. 学习文字接龙

比如你给 GPT 一个不完整的句子“你好”，然后让 GPT 接一个可能的字“美”：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227212912401.png" alt="image-20221227212912401" style="zoom:90%;" /></center>

这样就是从网上找大量的语料来让 GPT 学文字接龙就好了。

但一个可能让你疑惑的问题是，不完整的句子“你好”后面可能跟很多种可能呀，不一定就接“美”这个字，其实 GPT 的输出是一个 distribution，它学习的其实是“你好”后面接“高”或者“美”的可能性比较高，而接“吗”的也有不低的几率，而接“星”的几率就比较低了：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227213247760.png" alt="image-20221227213247760" style="zoom:80%;" /></center>

然后 GPT 从这个 distribution 中随机 sample 出一个字出来作为本次的 output，也正因为 GPT 每次产生 token 都是具有随机性的，所以 GPT **每一次的输出都是不同的**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227213523301.png" alt="image-20221227213523301" style="zoom:80%;" /></center>

**学习文字接龙有什么用呢**？这是有很大作用的，光是学习文字接龙，GPT 就可以拿来回答问题：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227214547596.png" alt="image-20221227214547596" style="zoom:80%;" /></center>

+ 通过这么两次对“不完整的句子”的文字接龙，我们就知道答案是“玉山”了。

但实际上 GPT 在实际使用时并没有那么好用，比如你要问他“台湾最高的山是哪座”，这时你把这句话当成“不完整的句子”交给 GPT，也许你期待 GPT 产生“玉山”，但其实 GPT 也有可能产生别的东西，比如 GPT 也许从某个资料中看到过地理考试题，于是它决定给你出个选择题也是有可能的：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227215919365.png" alt="image-20221227215919365" style="zoom:80%;" />

所以问题是：**如何引导 GPT 产生有用的输出呢**？这就交给了下一个阶段。

### 阶段 2：人类老师引导文字接龙的方向

这一步，找人来思考想问 GPT 的问题，并人工提供正确答案，比如如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227223605967.png" alt="image-20221227223605967" style="zoom:80%;" /></center>

这么做其实就是让 GPT 多看有益的问句和答案，不要让 GPT 去网络上看一些有的没的。这个过程不需要穷举所有的问题，我们只是要告诉 GPT 人类的偏好，因为产生这些答案其实是 GPT 本来就可以的，现在只是在激发它本来就有的力量，让它懂得人类的偏好。

> 并不需要特别多人类编写好的问题与答案，只需要数万则就可以。

### 阶段 3：模仿人类老师的喜好

当向 GPT 问出一个问题之后，把 GPT 的不同答案收集起来，然后 OpenAI 雇佣人类去标注处哪些答案是好的答案，哪些答案是不好的答案，这里人类老师只需要给出一个 rank 就可以：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227224321833.png" alt="image-20221227224321833" style="zoom:80%;" />

有了这些资讯以后，接下来就要去训练一个模仿人类老师的 model，这里称为 <mark>Teacher Model</mark>：给它问题和 GPT 的答案，它要输出一个分数。这个 Teacher Model 的学习目标就是去模仿人类老师的评分标准。比如人类老师标注说，当面对问题“台湾最高的山是哪座？”时，答案“玉山”好于“谁来告诉我呀”，那么 Teacher Model 给出的分数应当是好的答案大于不好的答案，如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227231852613.png" alt="image-20221227231852613" style="zoom:80%;" /></center>

### 阶段 4：用 RL 向模拟的老师学习

把问题“世界上最高的山是哪座？”丢给 GPT，然后 GPT 接了一句“世界上最深的海又在哪里？”，我们把这么两句话接起来给 Teacher Model，model 会输出一个分数，会给这句话打一个低分，这里的分数就是 RL 中的 “reward”，而 RL 的目标就是通过调整参数来得到最大的 reward。这个过程如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227234215918.png" alt="image-20221227234215918" style="zoom:80%;" /></center>

通过 RL 技术，就希望说在问出“世界上最高的山是哪座？”时，GPT 能够回答出“喜马拉雅山”，从而让 Teacher Model 能给它一个高的 reward。经过 RL 的训练，这里的 GPT 就是 ChatGPT 了。

### Summary

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227234932482.png" alt="image-20221227234932482" style="zoom:80%;" />

## 2. ChatGPT 不完美

如果你问 ChatGPT 一个正常的问题的话，它大概率已经在训练资料中见过这个题型了，如果你想要考倒它，一个技巧是，你问它一些没用而且简单的问题，很有可能就考倒它了：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221227234824404.png" alt="image-20221227234824404" style="zoom:80%;" /></center>