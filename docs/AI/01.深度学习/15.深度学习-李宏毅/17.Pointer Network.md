---
title: Pointer Network
date: 2022-12-04 21:08:15
permalink: /pages/lhy/pointer-network/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅
tags:
  - 
---

## 1. Pointer Network

Pointer Network 的提出是源自于这样一个问题：给了一堆 data point，需要能够自动从中找出将哪些点连起来就可以把其余的点都包起来。如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204213150357.png" alt="image-20221204213150357" style="zoom:67%;" /></center>

如果让 Neural Network 硬 train 一发的话，NN 的 input 是一个 data point 的 coordinate $(x_1, y_1)$，输出是所要连接起来的点，如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204213345156.png" alt="image-20221204213345156" style="zoom:67%;" /></center>

这个 NN 的 input 是一个 sequence，output 是另一个 sequence，那这似乎可以用 seq2seq 来解，如下所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204214619128.png" alt="image-20221204214619128" style="zoom:67%;" /></center>

+ 黄色向量代表一个 distribution，结果是从这个 distribution 中采样或 argmax 出一个结果。

但这个方法是行不通的，原因在于，input 的 data points 的个数是会变化的，而 Decoder 无法处理这种变化的情况，它输出的 distribution 是固定长度的。

怎么办呢？可以用 Attention 机制来对它做一下改造，让 Network 可以动态地决定它输出的 set 可以有多大。这是怎么做的呢？

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204220621458.png" alt="image-20221204220621458" style="zoom:60%;" /></center>

+ 类似于 Self-Attention，每一个输入的 data point 首先得到 query，然后每个 query 与一个 $(x_0, y_0)$ 对应的 key $z^0$ 进行计算得到一排 attention weights 作为 distribution，接下来与以往 self-attention 不一样，只需要从这个 weights 组成的 distribution 中做 argmax 就可以得到 output 的第一项。
+ 接下来再把 $(x_1, y_1)$ 所对应的 key $z^1$ 做上面的过程，得到 output 的第二项，之后不断重复。这个过程直到 “END” 拥有最大的 attention weights 才会结束。
+ 现在的**好处**就是：What decoder can output depends on the input.

所以总结一下，传统的带有注意力机制的 seq2seq 模型的运行过程是这样的，先使用 encoder 部分对输入序列进行编码，然后对编码后的向量做 attention，最后使用 decoder 部分对 attention 后的向量进行解码从而得到预测结果。但是作为 Pointer Networks，得到预测结果的方式便是输出一个概率分布，也即所谓的指针。换句话说，**传统带有注意力机制的 seq2seq 模型输出的是针对输出词汇表的一个概率分布，而 Pointer Networks 输出的则是针对输入文本序列的概率分布**。

其实我们可以发现，因为输出元素来自输入元素的特点，**Pointer Networks 特别适合用来直接复制输入序列中的某些元素给输出序列**。而事实证明，后来的许多文章也确实是以这种方式使用 Pointer Networks 的。

## 2. Applications

Pointer Network 的主要好处有：

1. 提供了一种新视角去理解 Attention，把 Attention 作为一种求分布的手段。
2. 对于输出字典长度不固定问题提供了一种新的解决方案。
3. 将输入作为输出的一种补充手段，让输出部分可以更好的引入输入部分的信息。

### 2.1 Applications - Summarization

Pointer Network 还是蛮适合用在 Summarization 的任务中。

> Summarization is the task of condensing a piece of text to a shorter version that contains the main information from the original.
>
> —— See A, Liu P J, Manning C D. Get To The Point: Summarization with Pointer-Generator Networks[J]. 2017:1073-1083.

尽管这个任务可以用 seq2seq 来做，input 是一个 document，output 是 summary，但是今天在做 summary 的时候，很多 summary 里面放的词汇都是人名地名等这些专有名词，这往往做不好。

换一种思路，在做 summary 的时候，你可以想象 summary 与 document 的关系就是，从 document 中取出一些重要的词汇，然后接起来就是 summary 了。所以在产生 summary 的时候，让 machine 直接从 document 挑字出来组成 summary 就会得到不错的效果。

传统 sumarization 的做法如下（[paper](https://arxiv.org/abs/1704.04368)）：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204222238318.png" alt="image-20221204222238318" style="zoom: 75%;" /></center>

可以使用 Pointer network 对它进行改进。

> 可以参考文章 https://blog.csdn.net/qq_44766883/article/details/111995364

### 2.2 More Applications

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221204222617464.png" alt="image-20221204222617464" style="zoom:67%;" /></center>