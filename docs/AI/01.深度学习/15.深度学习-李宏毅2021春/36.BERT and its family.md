---
title: BERT and its family
date: 2022-09-10 09:11:43
permalink: /pages/lhy/bert-and-family/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

## 1. What is pre-train model?

### 1.1 Pre-train Model 的历史

以前，Pre-train Model 想要做的是：Represent each token by a embedding vector. 如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910092744562.png" alt="image-20220910092744562" style="zoom:67%;" /></center>

以前所做的往往是 simply a table look-up，也就是静态查表，这有一个问题：The token with the same type has the same embedding.

上面所说的技术就有知名的 <mark>Word2vec</mark> [Mikolov,er al., NIPS’13]、<mark>Glove</mark> [Pennington, et al., EMNLP’14]。

除此之外还有什么技术呢？

如果你考虑的是英文，如果将 English word 作为 token 的话，英文单词实在太多了，这样静态查表总会有找不到的词汇。也许我们可以把 model 改成将英文 character 作为 input，输出就是一个 vector，这就是 <mark>FastText</mark> 做的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910093518145.png" alt="image-20220910093518145" style="zoom:72%;" /></center>

如果考虑中文，那每一个中文其实也可以看做一个 image，然后丢到 CNN 里从而输出对应的 vector，这也许也是可以的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910093735265.png" alt="image-20220910093735265" style="zoom:80%;" /></center>

上面所说的技术不会考虑到 context，比如同一个“狗”出现在“圈养狗”和“单身狗”中意思不是一样的。

### 1.2 Contextualized Word Embedding

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910094424787.png" alt="image-20220910094424787" style="zoom:75%;" /></center>

+ Tree-based model 好像没有特别强，所以目前也用的不多。

### 1.3 Bigger Model

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1365470-20211026224307862-217905674.png" alt="1365470-20211026224307862-217905674" style="zoom: 40%;" /></center>

### 1.4 Smaller Model

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910095704359.png" alt="image-20220910095704359" style="zoom:67%;" /></center>

+ 这里面很出名的是 ALBERT

这些让 BERT 变小的技术是 Network Compression。

还有一些在这些模型的 Network Architecture 上也有很多的突破：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910095915587.png" alt="image-20220910095915587" style="zoom:80%;" /></center>

+ Transformer-XL 解决了 BERT 一次只能读 512 个 token 的问题
+ Reformer 和 Longformer 是为了减少 self-attention 过程中的运算复杂度

## 2. How to fine-tune?

### 2.1 NLP tasks

可以根据 input 和 output 的类型进行一个分类：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910100216657.png" alt="image-20220910100216657" style="zoom:67%;" /></center>

#### 2.1.1 Input

对于 multiple sentences 的情况，假如有两种不同类型的 sentence，可以在两者之间加一个特殊分隔符 `[SEP]`：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910100807708.png" alt="image-20220910100807708" style="zoom:80%;" /></center>

然后把它们丢到 model 里面就可以了。

#### 2.1.2 Output

output 有多个类型，我们逐一看一下。

##### 1）one class

一种做法如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101027092.png" alt="image-20220910101027092" style="zoom:80%;" /></center>

+ 输入的时候加一个特别的 token：`[CLS]`，pretrain 的时候就要告诉 model 当看到 [CLS] 就要产生一个与整个句子有关的 Embedding
+ 然后把这个与整个句子有关的 Embedding 丢到一个 task specific model 中得到一个 class，这个 model 什么样要看任务有多复杂，但很多情况下直接是一个 linear transform 就可以了，或者多叠几层 linear transform。

还有另外一种做法是将整个句子的 Embedding 都进行处理得到一个 class：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101447795.png" alt="image-20220910101447795" style="zoom:80%;" /></center>

##### 2）class for each token

这种就是对每个 token 都给一个 class，做法如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101645179.png" alt="image-20220910101645179" style="zoom:80%;" /></center>

+ 这里的 task specific model 可以是一个 LSTM，也可以是其他的。

##### 3）copy from input

完全从 input 做 copy，这种类型的任务也没有很多，其中最经典的就是 Extraction-based QA，这个任务是：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101915337.png" alt="image-20220910101915337" style="zoom:80%;" /></center>

原始的 BERT 的 paper 里提供了这种任务的解决方法，可以参考论文。

##### 4）general sequence

按照 seq2seq model 的设计，可以这样做：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910102404705.png" alt="image-20220910102404705" style="zoom:80%;" /></center>

但问题是，作为 decoder 的 task specific model 是完全没有 pretrain 过的，所以这样做也许不是最好的。

来看第二种版本：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910102738639.png" alt="image-20220910102738639" style="zoom:70%;" /></center>

+ 这种做法就是输入 input sequence 后再输入一个 [SEP]，这时 Model 会输出与 [SEP] 相对应的 Embedding Vector，把这个 vector 输入到 task specific model 得到输出 $w_3$，然后接着把 $w_3$ 当做 input 得到 $w_4$，一直如此直到得到 `<EOS>`。

### 2.2 How to fine-tune

假设你有一些 task specific data，该怎样去 fine-tune 呢？这里有两种做法。

**第一种做法**，固定住 pre-trained model 从而变成一个 feature extractor，然后我们只 fine-tune 那些 task specific 的部分：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910103323315.png" alt="image-20220910103323315" style="zoom:80%;" /></center>

**第二种做法**，把 pre-trained model 和 task-specific model 合在一起 fine-tune，当成一个巨大的 model 来解这个 down-stream task：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910103554759.png" alt="image-20220910103554759" style="zoom:80%;" /></center>

在以前，巨大 model 在 train 时很容易 overfitting，但第二种做法由于只有 task-specific 的 param 是随机初始化的，pre-trained model 的 param 并不是随机初始化的，所以也许这种方式并没有那么容易 overfitting。在一些文献中指出，<u>第二种做法的 performance 往往比第一种要好一些</u>。

### 2.3 Adaptor

刚刚讲的第二种 fine-tune 方法有一个问题，对每一个 task specific 进行 fine-tune 后，pretrained model 会变得不一样：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910105412511.png" alt="image-20220910105412511" style="zoom:70%;" /></center>

这样每个任务都需要存一个新的 model，而这些 model 往往非常巨大，所以这可能行不通的，因此有了 Adaptor 的概念。也就是说，我们能不能只能调 pretrained model 的一部分就好了，于是我们在 pretrained model 里面加了一个很小的 layer，这些 layer 就叫做 <mark>Adaptor</mark>，下图用 Apt 来简称。这样 fine-tune 时只调整 Pretrained Model 的 Adaptor 部分，这样存储时就只需要存一份 pretrained model 的主体部分，从而减小存储压力：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910105923387.png" alt="image-20220910105923387" style="zoom:70%;" /></center>

这边举一个使用 Adaptor 的例子，当然还有很多其他做法：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910110135189.png" alt="image-20220910110135189" style="zoom:80%;" /></center>

+ pretrain 的时候是没有 Adaptor 的，而是在准备 fine-tune 时才加入并只调整 Adaptor 的参数。

这个例子的表现结果如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910110516588.png" alt="image-20220910110516588" style="zoom:80%;" /></center>

+ Accuracy delta 中的 0 代表当我们 fine-tune 整个 model 时的 performance
+ 蓝色这条线表示越往右边微调参数越多

### 2.4 Weighted Features

我们之前是把 input sequence 后扔进整个 model 后，拿到最后的 Embedding 输给 down-stream task layer 中。但还有一种做法，因为每一层他所抽取的资讯是不一样的，所以一种做法是**把不同层的 feature 给 weighted sum 起来得到一个新的 embedding**，这个 embedding vector 同时综合了多层抽取的资讯，然后再把它丢到 down-stream 中：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910111338745.png" alt="image-20220910111338745" style="zoom:70%;" /></center>

这里的 $w^1、w^2\dots$ 该设为多少呢？它们可以被视为 task specific layer 的参数的一部分从而一起 learn 出来。

## 3. Why Pre-train Models?

GLUE 是检测一个模型了解人类语言的能力：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910135932970.png" alt="image-20220910135932970" style="zoom:75%;" /></center>

+ 黑色的这条线表示人类理解这个数据集的能力，可以看到后面提出的 pretrain model 都至少可以在这个 corpus 上超越人类的 performance 了。

有很多 paper 讨论了为什么 pretrain model 是有效的，这里选其中一篇来讲解。这里选了 arxiv 上 1908.05620 这一篇，分析其中的两个结果

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910140557668.png" alt="image-20220910140557668" style="zoom:75%;" /></center>

+ 虚线代表没有 pretrain 的 model，实线代表有 pretrain 的 model，它们的 model 大小是一样的。
+ 可以看到，有 pretrain 的 model 在 training 时 training loss 下降的非常快。

那只看 training loss 是不是有可能是因为 pretrain model 会 overfitting 呢？它在面对新的数据时 generalize 的能力如何呢？我们来看下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910161032304.png" alt="image-20220910161032304" style="zoom:80%;" /></center>

+ 可以看到，fine-tuning BERT 所处在的 local minima 是一个盆地，而左图是一个峡谷。一般来说，local minima 处在盆地时的泛化能力是要比处在峡谷时要强的。

## 4. How to Pre-train?

这章讲如何得到 pre-train 的模型。

我们希望什么样的 pre-train 的 model 呢？我们希望它能把 token sequence 吃进去，然后把每个 token 变成一个 embedding vector，而且希望这些 embedding vector 是 contextulize 的，即考虑上下文的。

### 4.1 Pre-training by Translation

像这种抽取 contextulize 的 embedding 的方法最早是 <mark>CoVe</mark> 这篇 paper，它是通过 translation 而不是现在常用的 unsupervised learning 的方法得到的这个 model：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911105537473.png" alt="image-20220911105537473" style="zoom:70%;" /></center>

+ 把这个 model 当作是 translation 的 Encoder，最终训练好 Encoder 和 Decoder，这里的 Encoder 就是我们 pretrain 好的 contextulize word embedding 的 model。

这篇 paper 选择 translation 也是有合理性的，因为他会把input sequence 里面的资讯都如实呈现到输出里面，而像 summarize 之类的任务就不行。但这种方式的问题是收集大量的 pair data 作为 training data 是很困难的，于是就有了之后的 unsupervised 的方法，现在也常称为 self-supervised 方法。

### 4.2 Self-supervised Learning

Self-supervised Learning 是 Yann LeCun 在 Twitter 上提出的，其基本概念如下：

<center><img src="C:\Users\yubin\AppData\Roaming\Typora\typora-user-images\image-20220911110848667.png" alt="image-20220911110848667" style="zoom:67%;" /></center>

### 4.3 Predict Next Token

给这个 model 一个 $w_1$ 得到 representation $h_1$，然后用 $h_1$ 来预测下一个 token $w_2$。怎样从 $h_1$ 预测 $w_2$ 呢，如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911111325008.png" alt="image-20220911111325008" style="zoom:67%;" /></center>

这样就用从 $w_{i-1}$ 预测 $w_i$ 这个任务来训练 model：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911111557442.png" alt="image-20220911111557442" style="zoom:63%;" /></center>

注意，在训练时**不可以**让 model 一次把 $w_1 \sim w_4$ 同时读进去，然后让他预测 $h_1 \sim h_4$，否则 model 可能直接把下一个 token 给输出出去，从而学不到什么东西。

以上所讲的 <mark>Predict Next Token</mark> 任务是最早的 unsupervised pretrain 技术，这样得到的 model 就是一个 language model（**LM**）。

这个 model 可以使用 LSTM，也可以使用 Self-attention。使用 LSTM 的 language model 有 ELMo、ULMFiT 等，使用 Self-attention 的有 GPT、Megatron、Turing NLG 等。

使用 Self-Attention 作为 model 来做 predict next token 时，注意要给 self-attention 加一个 constraint，不能让它看到后面的，比如 $w_2$ 位置只能 attend 到 $w_1$ 和 $w_2$，而不能 attend 到 $w_3$ 及 $w_4$，从而防止它看到它不该看的以后的答案：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911112705093.png" alt="image-20220911112705093" style="zoom:67%;" /></center>

### 4.4 Predict Next Token - Bidirectional

刚刚讲的最终得到的 contextualize representation 只考通过 predict next token 得到的，而且是只考虑了 left context，并没有考虑 right context。

<mark>ELMo</mark> 就是同时考虑了 left context 和 right context 用来得到一个 token 的 contextualize representation：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912225943109.png" alt="image-20220912225943109" style="zoom:67%;" /></center>

  

+ 它有一个由左向右的 LSTM，看 $w_1 \sim w_4$  去预测 $w_5$；又有一个由右向左的 LSTM，看 $w_7 \sim w_5$ 来预测 $w_4$。
+ 然后 ELMo 把正向的 LSTM 和逆向的 LSTM 输出的 vector 给 concatenate 起来，当作代表 $w_4$ 的 contextualize representation。

所以说 ELMo 通过两个方向的 LSTM 来考虑了一个 token 的 left context 和 right context。但这样还不够，因为 machine 在看 left context 进行 encoding 时没有看到 right context，即两次 encoding 的过程都只是看到句子的一半而非全部，两个 LSTM 得到的 vector 是没有交互的。

### 4.5 Masking Input

BERT 弥补了刚刚讲的 ELMo 的 encoding 过程没有左右 context 交互的问题，而且 BERT 做的也不再是 predict next token，而是 masking input：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912231915822.png" alt="image-20220912231915822" style="zoom:67%;" /></center>

+ BERT 对 input sequence 的一些 token 给 masking 掉，对 masking 的部分有随机两种做法：一种是替换成一个 special token [MASK]，一种是随机 sample 一个 token 替换到这里。
+ 然后用所 masking 掉的部分输出的 vector 来预测 masking 掉的是哪个 token。

BERT 用的 Model 是 Transformer Encoder 堆叠起来的。

> 其实考古一下的话，Word2Vec 的 CBOW 模型的 training 过程就与 BERT 很像，只不过 BERT 内部结构更加复杂，所考虑的东西也更多了。

原始 BERT 中要 masking 掉哪些位置是随机决定的，但这不一定很好。具体改进的做法有：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912232846634.png" alt="image-20220912231915822" style="zoom:67%;" /></center>

+ 上面的 WWM 是 masking 掉一整词，防止出现要预测“黑 [MASK] 江”这种简单的任务；
+ 下面是指讲 entity 先识别出来，然后 masking 掉 entity，这么做的就是 ERNIE。

还有一种 masking 的方法叫做 <mark>SpanBert</mark>，它就是一次 masking 掉很长的一个范围，原来的 BERT 只是每次随机选一个 token 来 masking 掉。SpanBert 每次盖住多长是有一个几率分布：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912233424453.png" alt="image-20220912233424453" style="zoom:67%;" /></center>

这篇 paper 也对比多不同类型的 masking 方法的效果：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912233711412.png" alt="image-20220912233711412" style="zoom: 80%;" /></center>

+ Geometric Spans 就是 SpanBert 提出的 masking 方法。

 ### 4.6 Span Boundary Objective（SBO）

SpanBert 同时提出了一个新的预训练方法：Span Boundary Objective（<mark>SBO</mark>）。做法如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912234424681.png" alt="image-20220912233424453" style="zoom:67%;" /></center>

+ 先 masking 一个范围，同时有一个 SBO 的 model，它把这个所 masking 部分的左右两边的 token 吃进去，同时给 SBO model 一个数字 2，这个数字表示要预测所 masking 掉 span 里的第 2 个 token $w_5$

这个训练方法也许看上去匪夷所思，其实这种设计所期待的是：一个 span 的左右两边的 token 可以包含它内部整个 span 的资讯。为什么这样呢？以后讲到 coreference 时会有用处。

### 4.7 XLNet

// TODO