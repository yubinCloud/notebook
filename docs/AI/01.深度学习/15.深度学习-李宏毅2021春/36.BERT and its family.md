---
title: BERT and its family
date: 2022-09-10 09:11:43
permalink: /pages/lhy/bert-and-family/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

## 1. What is pre-train model?

### 1.1 Pre-train Model 的历史

以前，Pre-train Model 想要做的是：Represent each token by a embedding vector. 如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910092744562.png" alt="image-20220910092744562" style="zoom:67%;" /></center>

以前所做的往往是 simply a table look-up，也就是静态查表，这有一个问题：The token with the same type has the same embedding.

上面所说的技术就有知名的 <mark>Word2vec</mark> [Mikolov,er al., NIPS’13]、<mark>Glove</mark> [Pennington, et al., EMNLP’14]。

除此之外还有什么技术呢？

如果你考虑的是英文，如果将 English word 作为 token 的话，英文单词实在太多了，这样静态查表总会有找不到的词汇。也许我们可以把 model 改成将英文 character 作为 input，输出就是一个 vector，这就是 <mark>FastText</mark> 做的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910093518145.png" alt="image-20220910093518145" style="zoom:72%;" /></center>

如果考虑中文，那每一个中文其实也可以看做一个 image，然后丢到 CNN 里从而输出对应的 vector，这也许也是可以的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910093735265.png" alt="image-20220910093735265" style="zoom:80%;" /></center>

上面所说的技术不会考虑到 context，比如同一个“狗”出现在“圈养狗”和“单身狗”中意思不是一样的。

### 1.2 Contextualized Word Embedding

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910094424787.png" alt="image-20220910094424787" style="zoom:75%;" /></center>

+ Tree-based model 好像没有特别强，所以目前也用的不多。

### 1.3 Bigger Model

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1365470-20211026224307862-217905674.png" alt="1365470-20211026224307862-217905674" style="zoom: 40%;" /></center>

### 1.4 Smaller Model

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910095704359.png" alt="image-20220910095704359" style="zoom:67%;" /></center>

+ 这里面很出名的是 ALBERT

这些让 BERT 变小的技术是 Network Compression。

还有一些在这些模型的 Network Architecture 上也有很多的突破：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910095915587.png" alt="image-20220910095915587" style="zoom:80%;" /></center>

+ Transformer-XL 解决了 BERT 一次只能读 512 个 token 的问题
+ Reformer 和 Longformer 是为了减少 self-attention 过程中的运算复杂度

## 2. How to fine-tune?

### 2.1 NLP tasks

可以根据 input 和 output 的类型进行一个分类：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910100216657.png" alt="image-20220910100216657" style="zoom:67%;" /></center>

#### 2.1.1 Input

对于 multiple sentences 的情况，假如有两种不同类型的 sentence，可以在两者之间加一个特殊分隔符 `[SEP]`：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910100807708.png" alt="image-20220910100807708" style="zoom:80%;" /></center>

然后把它们丢到 model 里面就可以了。

#### 2.1.2 Output

output 有多个类型，我们逐一看一下。

##### 1）one class

一种做法如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101027092.png" alt="image-20220910101027092" style="zoom:80%;" /></center>

+ 输入的时候加一个特别的 token：`[CLS]`，pretrain 的时候就要告诉 model 当看到 [CLS] 就要产生一个与整个句子有关的 Embedding
+ 然后把这个与整个句子有关的 Embedding 丢到一个 task specific model 中得到一个 class，这个 model 什么样要看任务有多复杂，但很多情况下直接是一个 linear transform 就可以了，或者多叠几层 linear transform。

还有另外一种做法是将整个句子的 Embedding 都进行处理得到一个 class：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101447795.png" alt="image-20220910101447795" style="zoom:80%;" /></center>

##### 2）class for each token

这种就是对每个 token 都给一个 class，做法如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101645179.png" alt="image-20220910101645179" style="zoom:80%;" /></center>

+ 这里的 task specific model 可以是一个 LSTM，也可以是其他的。

##### 3）copy from input

完全从 input 做 copy，这种类型的任务也没有很多，其中最经典的就是 Extraction-based QA，这个任务是：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101915337.png" alt="image-20220910101915337" style="zoom:80%;" /></center>

原始的 BERT 的 paper 里提供了这种任务的解决方法，可以参考论文。

##### 4）general sequence

按照 seq2seq model 的设计，可以这样做：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910102404705.png" alt="image-20220910102404705" style="zoom:80%;" /></center>

但问题是，作为 decoder 的 task specific model 是完全没有 pretrain 过的，所以这样做也许不是最好的。

来看第二种版本：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910102738639.png" alt="image-20220910102738639" style="zoom:80%;" /></center>

+ 这种做法就是输入 input sequence 后再输入一个 [SEP]，这时 Model 会输出与 [SEP] 相对应的 Embedding Vector，把这个 vector 输入到 task specific model 得到输出 $w_3$，然后接着把 $w_3$ 当做 input 得到 $w_4$，一直如此直到得到 `<EOS>`。

### 2.2 How to fine-tune

假设你有一些 task specific data，该怎样去 fine-tune 呢？这里有两种做法。

**第一种做法**，固定住 pre-trained model 从而变成一个 feature extractor，然后我们只 fine-tune 那些 task specific 的部分：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910103323315.png" alt="image-20220910103323315" style="zoom:80%;" /></center>

**第二种做法**，把 pre-trained model 和 task-specific model 合在一起 fine-tune，当成一个巨大的 model 来解这个 down-stream task：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910103554759.png" alt="image-20220910103554759" style="zoom:80%;" /></center>

在以前，巨大 model 在 train 时很容易 overfitting，但第二种做法由于只有 task-specific 的 param 是随机初始化的，pre-trained model 的 param 并不是随机初始化的，所以也许这种方式并没有那么容易 overfitting。在一些文献中指出，<u>第二种做法的 performance 往往比第一种要好一些</u>。

### 2.3 Adaptor

刚刚讲的第二种 fine-tune 方法有一个问题，对每一个 task specific 进行 fine-tune 后，pretrained model 会变得不一样：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910105412511.png" alt="image-20220910105412511" style="zoom:80%;" /></center>

这样每个任务都需要存一个新的 model，而这些 model 往往非常巨大，所以这可能行不通的，因此有了 Adaptor 的概念。也就是说，我们能不能只能调 pretrained model 的一部分就好了，于是我们在 pretrained model 里面加了一个很小的 layer，这些 layer 就叫做 <mark>Adaptor</mark>，下图用 Apt 来简称。这样 fine-tune 时只调整 Pretrained Model 的 Adaptor 部分，这样存储时就只需要存一份 pretrained model 的主体部分，从而减小存储压力：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910105923387.png" alt="image-20220910105923387" style="zoom:80%;" /></center>

这边举一个使用 Adaptor 的例子，当然还有很多其他做法：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910110135189.png" alt="image-20220910110135189" style="zoom:80%;" /></center>

+ pretrain 的时候是没有 Adaptor 的，而是在准备 fine-tune 时才加入并只调整 Adaptor 的参数。

这个例子的表现结果如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910110516588.png" alt="image-20220910110516588" style="zoom:80%;" /></center>

+ Accuracy delta 中的 0 代表当我们 fine-tune 整个 model 时的 performance
+ 蓝色这条线表示越往右边微调参数越多

### 2.4 Weighted Features

我们之前是把 input sequence 后扔进整个 model 后，拿到最后的 Embedding 输给 down-stream task layer 中。但还有一种做法，因为每一层他所抽取的资讯是不一样的，所以一种做法是**把不同层的 feature 给 weighted sum 起来得到一个新的 embedding**，这个 embedding vector 同时综合了多层抽取的资讯，然后再把它丢到 down-stream 中：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910111338745.png" alt="image-20220910111338745" style="zoom:80%;" /></center>

这里的 $w^1、w^2\dots$ 该设为多少呢？它们可以被视为 task specific layer 的参数的一部分从而一起 learn 出来。

## 3. Why Pre-train Models?

GLUE 是检测一个模型了解人类语言的能力：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910135932970.png" alt="image-20220910135932970" style="zoom:80%;" /></center>

+ 黑色的这条线表示人类理解这个数据集的能力，可以看到后面提出的 pretrain model 都至少可以在这个 corpus 上超越人类的 performance 了。

有很多 paper 讨论了为什么 pretrain model 是有效的，这里选其中一篇来讲解。这里选了 arxiv 上 1908.05620 这一篇，分析其中的两个结果

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910140557668.png" alt="image-20220910140557668" style="zoom:78%;" /></center>

+ 虚线代表没有 pretrain 的 model，实线代表有 pretrain 的 model，它们的 model 大小是一样的。
+ 可以看到，有 pretrain 的 model 在 training 时 training loss 下降的非常快。

那只看 training loss 是不是有可能是因为 pretrain model 会 overfitting 呢？它在面对新的数据时 generalize 的能力如何呢？我们来看下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910161032304.png" alt="image-20220910161032304" style="zoom:80%;" /></center>

+ 可以看到，fine-tuning BERT 所处在的 local minima 是一个盆地，而左图是一个峡谷。一般来说，local minima 处在盆地时的泛化能力是要比处在峡谷时要强的。