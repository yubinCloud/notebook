---
title: 神经网络压缩
date: 2022-09-13 15:09:52
permalink: /pages/lhy/network-compression/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

为什么需要 smaller model 呢？因为有时候我们需要把 ML model 部署到 resource-constrained 的环境下，如智能手表、无人机等 loT device、edge device上。为什么不在云端运算呢？这可能涉及到 lower latency、privacy 等理由。

我们这章只将在 soft-ware 上的 solution，而不涉及 hard-ware 上的 solution。

## 1. Network Pruning

### 1.1 What is Network Pruning

“树大必有枯枝”。**Networks are typically over-parameterized** (there is significant redundant weights or neurons)，所以往往可以 prune them。

Network Pruning 是怎样进行的呢？大概如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913152714768.png" alt="image-20220913152714768" style="zoom:67%;" /></center>

+ 先 pretrain 一个很大的 model，然后评估里面 weight / neuron 的重要性，将一些不重要的 remove 掉，这样 accuracy 会掉一点，这时再 fine-tune 一下使 network 得到恢复，然后继续循环这个过程。

prune 的单位可以以 weight parameter 为单位，也可以以 neuron 为单位，那以这两者当作单位在实践中有什么不同呢？

### 1.2 Practical Issue

#### 1.2.1 Weight pruning

假如我们是评估某一个 parameter 重不重要而决定能不能去掉，那我们把不重要的 parameter 去掉之后，得到的 Network 的形状可能会是不规则的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913162755149.png" alt="image-20220913162755149" style="zoom:67%;" /></center>

这种不规则带来的结果就是**难以去实现**，同时也不容易用矩阵的乘法、用 GPU 去 speed-up。

一篇 paper 探讨了 weight pruning 的效果：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913163356618.png" alt="image-20220913163356618" style="zoom: 72%;" /></center>

+ 紫色的线对应“Sparsity”，后面接近 1 表示接近 95% 的 parameter 都被 prune 掉了，同时 accuracy 没有掉太多，但看 speed 的话，不同颜色的矩形表示在不同的 device 上运行，speed-up 大于 1 才会有加速，可以看到大部分的都是并没有起到加速的效果，反而还变慢了

说明把很多 parameter 给 prune 掉，使得 Network 变得不规则，反而使得在 GPU 资源上并没有起到期望的加速效果。因此，**weight pruning 不见得是一个好的方法**。

#### 1.2.2 Neuron pruning

Neuron pruning 相对来说就是一个好一点的方法了：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913164154879.png" alt="image-20220913164154879" style="zoom:67%;" /></center>

它比较容易用 pytorch 来实现，也比较好用 GPU 来加速。

### 1.3 Why Pruning？

#### 1.3.1 Lottery Ticket Hypothesis

为什么是先 train 一个 large model 再让他变小，而不是直接 train 一个 smaller model 呢？

一个普遍的答案是**大的 network 比较好 train**。你会发现，你直接 train 一个小的 network 往往没有办法得到跟大的 network 做 pruning 完后的一样的正确率。

> 为什么 larger network 比较好 train 可以参考以前的李老师的录影。

还有一个解释是**大乐透假说**（Lottery Ticket Hypothesis）。我们知道 train 一个 network 是比较看人品的，抽到一组比较好的初始参数，结果可能就会比较好。那我们可以把 large network 视为多个小的 sub-network，train 一个 large network 就可以视为 train 多个 sub-network，每一个小的 sub-network 不一定能够成功被训练出来，但在众多的 sub-network 里面，只要其中一个人成功，就可以“一人得道，鸡犬升天”，这就好像玩大乐透的时候买很多彩票就会中奖率变高一样：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913170540570.png" alt="image-20220913170540570" style="zoom:67%;" /></center>

这个 Lottery Ticket Hypothesis 是怎样在实验中被证实的呢？如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220914163606718.png" alt="image-20220914163606718" style="zoom:67%;" /></center>

+ 我们先 random init 一个 large network，然后 train 好后再进行 pruning 得到一个 small network。这时如果把这个 small network 重新 random init 一遍，会发现 train 不起来，但如果这个 small network 的参数与原始 large network 的初始参数相同，就可以 train 起来了。

#### 1.3.2 Deconstructing Lottery Tickets

Lottery Ticket Hypothesis 是一个很著名的 hypothesis，后来有一篇 paper 是 [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](https://arxiv.org/abs/1905.01067) 得出了很多有趣的结论，这里只讲它的结论。

##### 1）Different pruning strategy

他做了很多实验尝试了不同的 pruning strategy，发现了某两个 strategy 是最有效的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220914165616955.png" alt="image-20220914165616955" style="zoom: 80%;" /></center>

+ 他发现训练前和训练后的绝对值差异越大，那 prune 掉那些 network 后得到的结果是越有效的。

##### 2）“sign-ificance” of initial weights: Keeping the sign is critical

一组好的 initial weights 到底好在哪里呢？ 研究发现说，如果 prune 完后的 small model 重新初始化时只要不改变原初始参数的正负号，就可以 train 起来。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220914170644551.png" alt="image-20220914170644551" style="zoom:80%;" /></center>

这个实验说明，初始参数的正负号是这个 model 能不能 train 起来的一个关键因素。

##### 3）Pruning weights from a network with random weights

就好像米开朗琪罗雕刻出大卫的过程被自己说是“不过是从一个大的石头里把大卫释放出来”一样，有没有可能从一个随机初始化的 large network 里面的一个小子集组成的 network 在不需要 train 时就已经可以做分类等任务了呢？实验发现答案是“是”，而且这样得到的效果还不错。

#### 1.3.3 Rethinking the Value of Network Pruning

Lottery Ticket Hypothesis 就一定是对的嘛？不一定，有一篇 [paper](https://arxiv.org/abs/1810.05270) 做了一些实验打脸了 Lottery Ticket Hypothesis：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220914172102092.png" alt="image-20220914172102092" style="zoom:80%;" /></center>

有一些 model，然后列出了 unpruned 的情况下的 performance，然后对 model 进行 prune 得到 small model，再进行 fine-tune，会发现 performance 与 unpruned 的差不多；然后 scratch-E 指的是真的重新对 small model 进行随机初始化参数（不是 original random init，即不是从原来 large model 的 init weights 中借过来的），这些 scratch-E 的 model 的 performance 果然如人们预期般不如原来 unpruned 的效果好，**但是**，实验发现再多 train 几个 epoch，结果就是 scratch-B 这一列，可以看到 performance 就达到了与原来的同一水平了。所以说，也许“小的 network 不如大的好 train”只是一个幻觉，只不过需要多 train 几个 epoch。

同时 paper 还指出，Lottery Ticket Hypothesis 是在某些情况下才能观察到，它只在 small learning rate、unstructured（指以 weight 作为单位来 pruning）的情况下才能观察到 Lottery Ticket Hypothesis 现象。

所以，Lottery Ticket Hypothesis 是真是假，还有待未来的研究来证实。

## 2. Knowledge Distillation



