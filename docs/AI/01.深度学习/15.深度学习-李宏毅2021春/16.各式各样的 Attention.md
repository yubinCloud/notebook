---
title: 各式各样的 Attention
date: 2022-09-02 10:15:37
permalink: /pages/lhy/various-attention/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

## 1. self-attention 的问题

### 1.1 复习一下 self-attention

假设 sequence length = N，那么一次计算中会有 N 个 key，N 个 query，两两做 dot-product 产生一个 N * N 的 Attention Matrix：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220902103207542.png" alt="image-20220902103207542" style="zoom:67%;" /></center>

拿到 Attention Matrix 后，对 value vector 做 weight sum 就可以了。

而 self-attention 的**痛点**在于：<u>计算 N * N 的 Attention Matrix 的计算量可能非常惊人，尤其是 sequence length N 非常长时</u>。

### 1.2 Notice

+ Self-attention is only a module in a larger network.
+ **Self-attention dominates computation when N is large in Transformer.** 所以当 N 不是很大时，可能整个 network 受 feed forward 等其他 module 的 dominate，此时改进 self-attention 的效果也许对加快整体的训练不会有太大改进。
+ Usually developed for image processing. 因为假如要处理一个 256 * 256 的 image，那此时 N = 256 * 256，计算量将非常大。

## 2. 改进 Self-Attention

### 2.1 Skip Some Calculations with Human Knowledge

idea：在计算 Attention Matrix 时，也许我们不需要计算每一个值，也许我们可以凭借 human knowledge 来预先填上一些值。

#### 2.1.1 Local Attention / Truncated Attention

一种想法是，有些情况在做 Attention 的时候，有些位置不需要看整个 sequence，而是只需要看左右邻居就可以理解一个位置的 token 存有什么样的资讯：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220902104745890.png" alt="image-20220902104745890" style="zoom:67%;" /></center>

这样的话，我们可以直接把更长距离的 attention weight 设为 0，即：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220902105046864.png" alt="image-20220902105046864" style="zoom:80%;" /></center>

+ 灰色部分直接被设为 0
+ 蓝色部分才需要去计算 attention weight

上面这种方法称为 <mark>Local Attention / Truncated Attention</mark>。但这存在一个问题：每次做 attention 时只能看得到周围一小个范围的资讯，那这样的话就会跟 CNN 很像了。<u>这种方式会加快你的运算，但不一定会带来好的结果</u>。

#### 2.1.2 Stride Attention

既然说只看邻居不好，那我们就看一下远一点的邻居：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220902105733814.png" alt="image-20220902105733814" style="zoom:67%;" /></center>

+ 在这里，我们先跳两格，看三格位置处的资讯，这样就可以看到更大的范围的资讯，把 Attention Matrix 网格画出来的话就是这样（灰色设为 0，青色才需要计算）：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220902105933678.png" alt="image-20220902105933678" style="zoom:67%;" /></center>

这种方式称为 <mark>Stride Attention</mark>。上面例子是空两格，当然也可以空一格、空三格 ….

#### 2.1.3 Global Attention

刚刚所说的是只能知道周围发生了什么事，但如果想知道一整个 sequence 发生了什么事情，就需要用 <mark>Global Attention</mark>。

这需要在原来的 sequence 上加一些 special token，这些 special token 代表这些 position 需要做 Global Attention。

Global Attention 会做两件事：

+ Attend to every token.（也就是从 sequence 的每一个 token 里面收集资讯） --> collect global information.
+ Attended by every token. （也就是所有 token 都去看一下这个 special token 发生了什么资讯）  --> it knows global  information.

Global Attention 有两种做法：

+ 直接在 original sequence 中 assign 一些 token 作为 special token。比如将作为开头 token 的 CLS 作为 special token，或将句号作为 special token。【如下左图】
+ 外加一些额外的 token 作为 special token，这样不管 original sequence 是什么，都硬插几个 token 作为 special token

