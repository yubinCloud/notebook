---
title: Life Long Learning
date: 2022-09-16 10:44:48
permalink: /pages/lhy/life-long-learning/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

<mark>Life Long Learning</mark>（**LLL**）比较符合人们对 AI 的幻想：

<center><img src="C:\Users\yubin\AppData\Roaming\Typora\typora-user-images\image-20220916104819831.png" alt="image-20220916104819831" style="zoom:80%;" /></center>

+ 人们教他一个 task，他就学会一个 task，最终他就变成了“天网”从而统治人类。

Life Long Learning 有很多很潮的名字：Continuous Learning、Never Ending Learning、Incremental Learning。

在 real-world application 中，LLL 也是有用处的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916105403356.png" alt="image-20220916105403356" style="zoom:67%;" /></center>

## 1. Catastrophic Forgetting

### 1.1 Catastrophic Forgetting 是什么

下面通过一些 example 来说明 Life Long Learning 的一个难点：Catastrophic Forgetting。如下图所示，假设我们有如下两个 task（这看起来像是同一 task 的两个不同 domain，但目前文献上说的就是这样，还做不到真的两个完全不同的任务），我们现在 task 1 上训练一个 model，可以看到其结果在 task 2 也已经达到了 96% 的正确率了，效果非常好，然后我们继续把这个学好的 model 放到 task 2 上去继续学，学完后分析结果发现，这个 model 预期般地在 task 2 上 performance 更好了，但 task 1 上的 performance 却变得很差了，它发生了遗忘：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916110401418.png" alt="image-20220916110401418" style="zoom:67%;" /></center>

也许你会认为可能是因为 model 大小有限，导致其能力有限，从而导致其学完 task 2 后就忘了 task 1。那接下来看另外一个实验：如下图，我们把 task 1 和 task 2 的训练资料倒在一起同时去训练这个 model，可以发现这个 model 是可以同时学好两个 task 的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916110923520.png" alt="image-20220916110923520" style="zoom:67%;" /></center>

这说明这个 model 有能力同时学好两个 task，但如果依次去学习这两个 task，它会遗忘旧的 task。

这种现象是很普遍的，在一个 QA 的任务上，这个任务是 Given a document, answer the question based on the document. 这个任务使用的是 bAbi corpus，它是一个很小的数据集，包含了 20 个 QA task，如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916132723497.png" alt="image-20220916132723497" style="zoom:80%;" /></center>

用该数据集训练 model，结果如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916133356643.png" alt="image-20220916133356643" style="zoom:67%;" /></center>

+ 左图表示一次用 20 个 task 来训练 model，纵轴表示它在 task 5 上的 accuracy，可以看到当训练完 task 5 后 performance 很好，但一旦用 task 6 训完，它在 task 5 上的 accuracy 就直接掉到 0 了
+ 右图表示同时训练这 20 个 task，每个点表示在某个 task 上的 accuracy，可以看到这个 model 是可以在多个 task 上都表现很好的。

所以我们说 machine “是不为也，非不能也”。这种遗忘很厉害的现象称为 <mark>Catastrophic Forgetting</mark>（**灾难性遗忘**）：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916135616510.png" alt="image-20220916135616510" style="zoom:67%;" /></center>

### 1.2 为什么不用 multi-task training 来解决

根据刚刚说的，那你可能觉得将多个 task 的资料一块训练就可以了，这种方式叫做 <mark>Multi-task training</mark>，但这会导致 Computation issue 和 Storage issue：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916140338649.png" alt="image-20220916140338649" style="zoom:80%;" /></center>

> 这就好比说，你要让一个人学一门新的课，他学的时候还得把以前学过的也再看一遍才不至于遗忘。

Multi-task training can be considered as the upper bound of LLL. 因此在做 LLL 研究时，往往先跑个 multi-task training 的结果，知道它的 upper bound 在哪里，然后再比较新的技术和这个 upper bound。

### 1.3 为什么不 train a model for each task

+ Eventually we cannot store all the models …
+ Knowledge cannot transfer across different tasks.

### 1.4 Life-Long learning v.s. Transfer learning

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916151759272.png" alt="image-20220916151759272" style="zoom: 67%;" /></center>

+ transfer learning 只关注在 task 2 上的 performance
+ life-long learning 同时关注在 task 1 上的 performance

## 2. Evaluation

在讲 life-long learning 之前，我们先讲一下对 LLL performance 的**评估方法**。

首先我们需要一系列的 task：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916153100486.png" alt="image-20220916153100486" style="zoom:67%;" /></center>

+ 在图片的上面部分中，task 1 是正常的手写数字辨识，task 2 其实也是，只不过这里面的图片是 task 1 中的图片经过某种固定规律转换过来的。甚至有的直接将图片旋转个 15 度来作为新的 task。
+ 图片的下面部分中，task 1 是分类 0 和 1，task 2 是分类 2 和 3 …

然后我们就可以进行 life-long learning，并对 model 建立如下 evaluation：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220916170211883.png" alt="image-20220916170211883" style="zoom:67%;" /></center>

其中 $R_{i,j}$：after training task i, performance on task j

+ If $i \gt j$，那 i 是比较后面的任务，j 是比较前面的任务，此时 $R_{i,j}$ 表示在已经训练完 task i 后，在过去 task j 上的 performance 怎样
+ If $i \lt j$，那代表我们刚学完 task i 还没去学 task j，但 machine 会不会就无师自通了呢，也就看的是 machine 的 transfer 的能力

最常见的评价方法是 <mark>Accuracy</mark>：

$$Accuracy = \frac{1}{T}\sum^T_{i=1}R_{T,i}$$ 

+ 也就是将上表中最后一行的 R 全部累加起来求平均，用这个值来代表 life long learning 的方法的好坏。
+ 其中，$R_{T,T}$ 可能是最高的，$R_{T,1}$ 可能是最低的，因为学完 task T 后，task 1 已经忘得干干净净了。

还有一种评估方法是 <mark>Backward Transfer</mark>：

$$Backward \ Transfer = \frac{1}{T-1} \sum^{T-1}_{i=1}[R_{T,i} - R_{i,i}]$$

+ 它表示一系列值的平均，上表中的每一列产生一个差值，是用最下面一行的 R 减去这一列上对角线上的 R 得到的，这个过程如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918151014362.png" alt="image-20220918151014362" style="zoom:67%;" /></center>

因此每一个差值也就代表着，对 task i 来说，在刚学完 task i 上的正确率，和学完 T 个 task 后在 task i 上的正确率差多少。

还有一种评估方式是 <mark>Forward Transfer</mark>：

$$Forward \ Transfer = \frac{1}{T-1}\sum^T_{i=2}[R_{i-,i}-R_{0,1}]$$

这个指标想要问的问题是说，在看过一系列的任务后，在还没有看过 task i 而学完 task 1 ~ task i-1 后，你的模型到底可以学出什么样的成果：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918152153885.png" alt="image-20220918152153885" style="zoom:67%;" /></center>

下面，我们就要讲 life long learning 的解法了，这里讲三种解法：

+ Selective Synaptic Plasticity
+ Additional Neural Resource Allocation
+ Memory Reply

## 3. Selective Synaptic Plasticity

### 3.1 Why Catastrophic Forgetting?

我们先来想一下为什么 Catastrophic Forgetting 这件事情会发生呢？如下图所示，我们有两个 task，model 有两个参数 $\theta_1$ 和 $\theta_2$，下图是两个 task 的 loss surface，蓝色越深代表 loss 越小：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918162704219.png" alt="image-20220918162704219" style="zoom:67%;" /></center>

+ 在 task 1 上学习时，参数变化 $\theta^0 \to \theta^b$，然后再从 task 2 上学习，参数变化 $\theta^b \to \theta^*$。可我们再回到 task 1 上来看，会发现在 $\theta^*$ 上 loss 又是很大的，从而产生了 Catastrophic Forgetting 现象。
+ 那我们在 task 2 上学习时，可不可以是让 $\theta$ 的移动朝下一点，像红色标志那样，这样在两个 task 上就会都表现不错。这种思路就是将要讲的 Selective Synaptic Plasticity 方法。

### 3.2  regularization-based 的基本想法

**Basic Idea**: Some parameters in the model are important to the previous tasks. Only change the unimportant parameters.

那我们假设 $\theta^b$ 是从前一个 task 中学习到的，那我们就会给每个 parameter  $\theta_i^b$ 一个 guard $b_i$，然后在学习当前 task 时改写一下 loss function：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918163740382.png" alt="image-20220918163740382" style="zoom:67%;" /></center>

+ $b_i$ 越大，就代表我们越希望 parameter $\theta_i$ 与原先的 $\theta_i^b$ 很接近。

注意，我们并不要求所有参数都与原来很接近，而只是要求部分参数与原来的很接近，具体要求哪些是由 $b_i$ 决定的。假如我们很极端：

+ If $b_i = 0$, there is no constraint on $\theta_i$  =>  **Catastrophic Forgetting**
+ If $b_i = \infty$， $\theta_i$ would always be equal to $\theta_i^b$  => **Intransigence**

### 3.3 $b_i$ 的设定

对 $b_i$ 的设定是研究的重点，这里是采用了人工设定的方法。那怎么设定呢？本节只讲一下大概的思路。

以下图为例，我们可以让 $b_i$ 在每个参数方向上动一下，看看对 loss 的影响，从而判断这个参数的重要性，进而来设定 $b_i$ 的大小：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918165120793.png" alt="image-20220918165120793" style="zoom:67%;" /></center>

如果按照上面分析，我们将 $b_1$ 设的比较小，将 $b_2$ 设的比较大，那么在 task 2 上训练的结果就如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918170013332.png" alt="image-20220918170013332" style="zoom:67%;" /></center>

可以看到在 task 2 上 $\theta$ 的移动方向改变了，而且新的 $\theta^*$ 放到 task 1 中对它的伤害也不大。

接下来的图片就是 paper 中一个真实的实验结果，这种图在 life-long learning 中是非常常见的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220918170635659.png" alt="image-20220918170635659" style="zoom:67%;" /></center>

+ 横轴代表依序训练的过程，纵轴代表在不同 task 上的正确率
+ 来看不同颜色的线，SGD 的蓝线代表 $b_i = 0$ 的实验；L2 的绿线代表 $b_i=1$ 的实验，可以看到 L2 在 task A 上一直表现不错，但学习 task B 时却在 task B 上 performance 一直不太好，出现了 intransigence 现象。因此，如果所有的参数都有一样的限制，那么这对模型来说限制太大了，会导致这个 task 训练不起来。
+ 再来看红色这条线，如果我们给不同的参数不用的 $b_i$，有的参数的 $b_i$ 大，有的参数的 $b_i$ 小，只固定某些参数，而某些参数可以变动，也就得到了红色这条线。可以看到红色这条线在每个任务上表现都是最好的。

其实我们没有真的告诉你 $b_i$ 怎么算的，具体的算法可以参考一下 paper，每种方法都是他自己要考量的点和所要解决的问题：

+ [Elastic Weight Consolidation（EWC）](https://arxiv.org/abs/1612.00796)
+ [Synaptic Intelligence（SI）](https://arxiv.org/abs/1703.04200)
+ [Memory Aware Synapses（MAS）](https://arxiv.org/abs/1711.09601)
+ [RWalk](https://arxiv.org/abs/1801.10112)
+ [Sliced Cramer Preservation（SCP）](https://openreview.net/forum?id=BJge3TNKwH)

### 3.4 Gradient Episodic Memory（GEM）

除了以上讲的 regularization-based 的方法以外，更早期的一个做法叫做 Gradient Episodic Memory（GEM），它也是一个很有效的方法，但他不是在参数上做限制，而是在 gradient update 的方向上做限制。