---
title: Flow-based Generative Model
date: 2022-12-18 19:38:47
permalink: /pages/lhy/flow-based-model/
categories:
  - AI
  - 深度学习
  - 李宏毅-2017版
tags:
  - 
---

之前已经讲了三类 Generative Models：

+ Component-by-component（Autoregressive model）
+ 变分 Auto-Encoder
+ GAN

但他们各自有各自的问题：

+ Component-by-component（Autoregressive model）
  + What is the best order for the components?
  + Slow generation
+ Variational Auto-encoder
  + Optimizing a lower bound
+ Generative Adversarial Network
  + Unstable training

## 1. Generator

A generator G is a network. The network defines a probability distribution $P_G$.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218202606762.png" alt="image-20221218202606762" style="zoom:80%;" />

那对我们而言，什么样的 generator G 是我们想要找出来的呢？我们会希望 distribution $P_G(x)$ 与 distribution $P_{data}(x)$ 越接近越好。

怎样让两者越接近越好呢？常见的做法是我们在训练 G 的时候，让他的训练目标是 **maximize likelihood**，也就是说我们从 distribution $P_{data}(x)$ 中 sample 出 m 笔 data：$\{ x^1,x^2,...,x^m \}$，然后在训练时希望 sample 出来的 m 笔 data 的 likelihood 越大越好：

$$G^* = \arg\max_G \sum^m_{i=1} \log P_G(x^i)$$

为什么 maximize likelihood 就可以了呢？因为这个过程就等同于 minimize 两个 distribution 之间的 KL divergence：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218203618155.png" alt="image-20221218203618155" style="zoom:80%;" /></center>

所以 **maximize likelihood 就等同于让 $P_G$ 与 $P_{data}$ 越相近越好**。

而 Flow-based model 的好处就是：**Flow-based model directly optimizes the objective function**.（也就是可以直接 maximize likelihood）。

## 2. Math Background

要想理解 Flow-based model，你需要理解三样东西：Jacobian, Determinant, Change of Variable Theorem.

### 2.1 Jacobian Matrix

我们假设有如下的符号：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218204409436.png" alt="image-20221218204409436" style="zoom: 80%;" />

+ z 和 x 都是多维 vector，而且两者的维度可以不一样，这里为了方便，举例的时候就假设都是 2 dim。
+ 有一个 function f：x = f(z)，你可以想象 f 就是你的 generator，z 就是你的 input，x 就是你 generate 出来的东西。

**f 的 Jacobian 就是把 input component 和 output component 两两去做偏微分，然后全部收集起来得到一个 matrix**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218213326366.png" alt="image-20221218213326366" style="zoom:80%;" />

如果写成 $z = f^{-1}(x)$，那么 $f^{-1}$ 的 Jacobian 就可以写成：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218213504419.png" alt="image-20221218213504419" style="zoom:80%;" />

这两个 Jacobian 是有关系的，两个 matrix 的相乘等于 identity matrix。 所以说，如果函数就有 inverse 的关系，那他们的 Jacobian 也具有 inverse 的关系。

### 2.2 Determinant（行列式）

> Determinant 就是给你一个 square matrix，有一个 operation，你把这个 matrix 代进去算一算，得到一个 scalar，这个 scalar 就是 determinant。

The <mark>determinant</mark> of a **square matrix** is a **scalar** that provides information aboput the matrix.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218214241059.png" alt="image-20221218214241059" style="zoom:80%;" />

有几个结论要用：

+ $det(A) = \frac{1}{det(A^{-1})}$

+ $det(J_f) = \frac{1}{J_{f^{-1}}}$
+ 一个 matrix 做 transpose 不会改变它的 determinant

determinant 有什么含义呢？determinant 指的是一个高维空间中“体积”的概念：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218214747515.png" alt="image-20221218214747515" style="zoom:67%;" /></center>

### 2.3 Change of Variable Theorem

我们有一个 distribution $\pi(\color{blue}{x})$，然后假设这个 z 代入到 f 后会得到 x，即 $x = f(z)$，这样 x 也形成了一个 distribution $p(\color{green}{x})$：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218215151706.png" alt="image-20221218215151706" style="zoom:67%;" />

我们现在想知道 $\pi(z)$ 与 $p(x)$ 中间有什么样的关系。如果你能写出这两者之间的关系，其实你就可以分析一个 generator，因为这两个 distribution 就可以看成一个 generator 的 input 和 output。这两者的关系其实是可以写出来的：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218215511498.png" alt="image-20221218215511498" style="zoom: 80%;" />

现在先举一个例子来说明我们问的问题到底是什么，下面是一个很简单的例子，其中 $\pi(z)$ 是一个 uniform distribution：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218215821616.png" alt="image-20221218215821616" style="zoom:80%;" />

接下来考虑更加 general 的 case，我们不知道 $\pi(z)$ 和 $p(x)$ 的 distribution 怎么表达的，但我们知道 function f，那给一个点 $z'$ 加一个 $\Delta z$，$z'$ 与 $z' + \Delta z$ 之间就可以假设成是一个 uniform distribution：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218220520580.png" alt="image-20221218220520580" style="zoom:80%;" /></center>

为什么要加绝对值呢？因为 z 和 x 的变化情况有下面两种可能：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218220850197.png" alt="image-20221218220850197" style="zoom: 80%;" /></center>

刚刚的例子都是 z 和 x 是一维的，现在举一个 z 和 x 都是二维的情况：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218221300623.png" alt="image-20221218221300623" style="zoom:80%;" /></center>

+ 左边蓝色部分经过 f 变成了右边绿色部分
+ $\Delta x_{11}$ 是 $z_1$ 改变的时候，$x_1$ 的改变量；$\Delta x_{21}$ 是 $z_2$ 改变的时候，$x_2$ 的改变量；另外两个类似。

如果这一步没有问题的话，那接下来就只是对这个关系式进行一番整理就结束了：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218222759918.png" alt="image-20221218222759918" style="zoom:80%;" /></center>

+ 可以得知，$p(x')$ 与 $\pi(z')$ 之间的关系是可以写出来的，也就是差一个 Jacobian 的绝对值。
+ 重点是你要记住橙色标注的部分，这就是 <mark>Change of Variable Theorem</mark>。

## 3. Flow-based Model

之前我们要寻找的是 $G^* = \arg\max_G \sum^m_{i=1} \log P_G(x^i)$，现在我们已经可以写出 $P_G$ 了，因此可以得到：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218225829475.png" alt="image-20221218225829475" style="zoom:80%;" /></center>

由上图可以看出，我们只需要 maximize $\log P_G(x^i)$ 就可以了，我们可以通过 gradient descent 来做，但为了能够使用 gradient descent，你需要知道这个后面的具体表达式，这就产生了两个 limitation：

+ 你需要能够计算 $det(J_G)$，但是由于 $J_G$ 的维度往往非常高，这导致它的 det 计算非常昂贵，因此你需要精心设计你的 generator G，让他的 Jacobian 的 determinant 是容易计算的；
+ 你需要能够知道 $G^{-1}$，但为了让 G 是 inversable 的，z 与 x 必须是相同维度的，这也是 flow-based model 与 VAE 等模型的一个明显区别，比如你要 generate 一个 100 * 100 * 3 的 image，那你输入的 z 也必须是这个形状。

以上就是 G 的两个 limitation。

这个 G 是一个 network，正是由于 G 的 limitation，那一个 G 的能力可能有限，这时可以想到可以有多个 G，多个 G 连起来，能力可能就非常可观了，多个 G 就像一个 flow，所以称它为 flow-based model：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218230836672.png" alt="image-20221218230836672" style="zoom:67%;" /></center>

接下里就是 maximize 这个 $\log p_K(x^i)$ 就好了。

那实际上，你是怎样 train 这个 Flow-based Model 的呢？我们先考虑只有一个 generator 的 case：

你会发现在 $\log p_G(x^i)$ 这个式子当中，只出现了 $G^{-1}$，所示实际上我们训练的是 $G^{-1}$，但是在使用的时候是用 G 来做 generation：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221218232604169.png" alt="image-20221218232604169" style="zoom: 80%;" /></center>

我们从真实的 $P_{data}$ 中 sample 出一个 $x^i$，经过 $G^{-1}$ 得到 $z^i$，然后我们看一下所要 maximize 的表达式，从第一项 $\log \pi(G^{-1}(x^i))=\log \pi(z^i)$ 中来看，由于 $\pi(z)$ 的分布是 normal distribution，因此这一部分希望让 $z^i$ 称为 zero vecto，因为这个时候的 $\pi(z^i)$。但如果 $z^i$ 是 zero 的话：

+ $J_{G^{-1}}$ would be zero matrix;
+ $det(J_{G^{-1}})=0$;

但这会导致第二部分 $\log |det(J_{G^{-1}})|$ 有变向 -inf 的趋势，因此第一项和第二项可以一块来调节所期望 G 输出的的 $z^i$。 

## 4. Coupling Layer

### 4.1 Coupling Layer 是什么

一种 G 叫做 <mark>Coupling Layer</mark>，它的计算过程如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219130045976.png" alt="image-20221219130045976" style="zoom:72%;" /></center>



+ z 被拆成了两个部分：1~d 和 d+1~D
+ F 和 H 是一个任意的 network，要多复杂都可以

### 4.2 Couping Layer 的 inverse

现在就要来求 G 的 inverse：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219130520073.png" alt="image-20221219130520073" style="zoom:72%;" /></center>

+ 前 d 维只需要 copy 回去就好
+ 后面这部分维度，因为 $x_{i>d}=\beta_i z_i + \gamma_i$，因此 $z_{i>d}=\frac{x_i-\gamma_i}{\beta_i}$

### 4.3 Coupling Layer 的 Jacobian

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219131421290.png" alt="image-20221219131421290" style="zoom:80%;" /></center>

由于 x 和 z 都被分成了两部分，那 Jacobian $J_G$ 可以被分成四部分，然后我们逐个去求：

+ 左上角部分：由于 x 是直接从 z copy 过去的，因此求微分就得到了 Identity；
+ 右上角部分：由于 1~d 的 x 与 d+1~D 的 z 没有关系，因此这部分求微分得到 zero；
+ 左下角部分：因为我们最终是想就 Jacobian 的 det，而右上角部分为 zero，所以这部分是啥已经无所谓了，所以 I don’t care.
+ 右下角部分：根据 x 与 z 的关系：$x_{i>d}=\beta_i z_i + \gamma_i$，所以只有下标相同的 x 与 z 才具有关系，因此求微分的结果是一个 Diagonal Matrix

对这个 Jacobian 求 det，可以看出结果就是右下角的 Diagonal 的 det，所以 $det(J_G)$ 的结果就是右边的等式，最终的结果就是 F 输出的 $\beta_{d+1} \sim \beta_D$ 的连乘。这样，Jacobian 的 determinant 就很容易求了。

### 4.4 Coupling Layer - Stacking

当我们把这种 Coupling Layer 叠起来，就得到了一个完整的 generator：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219140156235.png" alt="image-20221219140156235" style="zoom:80%;" /></center>

但当你把 Coupling Layer 叠起来的时候，会出现一个莫名其妙的问题。因为在 layer 的前后，vector 的前半段都是直接 copy 过去，那堆叠之后都是直接 copy，那最后上半段的 vector 就是什么都没有做。如果你今天用来 generate 一个 image，那么你会发现你 generate 的 image 的前半段都是输入进来的 Gaussian noise，这显然不是你期望的。

所以你在堆叠 Coupling Layer 的时候，需要做一点手脚，一种做法是在堆叠的时候故意让 copy 的位置产生交替：第一个 layer 是 copy 前半段，第二个 layer 是 copy 后半段…. 这样就避免了你把 Gaussian noise 带到最后 generate 的结果中，这个做法如下图的下面部分所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219140723652.png" alt="image-20221219140723652" style="zoom:67%;" /></center>

### 4.5 一个具体的例子

现在我们用一个 generate image 的具体例子来说一下是怎么做的。这时有两种做手脚的方法：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219141047789.png" alt="image-20221219141047789" style="zoom:80%;" /></center>

+ 一种方法是，在一张 image 里面做手脚，一个 image 的 matrix 的每个元素有横纵坐标，那我们可以规定在第一个 layer 中，横纵坐标之和为奇数的做 copy，和为偶数的做 transform，然后在第二个 layer 中，横纵坐标之和为偶数的做 copy，和为奇数的做 transform。
+ 另一种方法是，一个 image 可以分成 3 个 channel，那可以在第一个 layer 里面某个 channel 做 copy，另一部分 channel 做 transform。然后在第二个 layer 中再交替。
+ 以上两种的 case 甚至可以交替使用。

## 5. GLOW

> [GLOW](https://arxiv.org/abs/1807.03039)

### 5.1 1 × 1 Convolution

1 × 1 Convolution 如下图所示，把第一个 pixel 的 3 个 value 组成的 vector，经过 3 * 3 的 W 得到输出的 3 个 value 组成的 vector，把它放到 output 的第一个 pixel 的位置：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219144054182.png" alt="image-20221219144054182" style="zoom:80%;" /></center>

这个 W 是 learn 出来的，但这个 W 有什么含义呢？这个 W 有可能学会 shuffle the channels，比如：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219144403004.png" alt="image-20221219144403004" style="zoom:80%;" /></center>

If W is invertible, it is easy to compute $W^{-1}$。但问题是 W 是 learn 出来的，它不一定是 invertable 的。但其实一般你随机得到的 matrix 都是 invertable 的，所以这篇 paper 并没有关于这部分的更多说明，反正 learn 出来的 matrix 一般都是 invertable 的。

它的 Jacobian 呢？在一个 pixel 处：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219144802422.png" alt="image-20221219144802422" style="zoom:80%;" />

这一步很神奇。然后就可以得到：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219145057572.png" alt="image-20221219145057572" style="zoom: 80%;" />

这个 d × d 的 Jacobian 中，横纵轴上的蓝绿色 vector 就是 image 中某个 position 的所有 channel 组成的 vector，不同 position 处没有关系，因此求微分是 zero，同一 position 的 pixel 处求微分就是 matrix $W$。

这个矩阵的 determinant 是很好算的，就等于 $(det(W))^{d \times d}$，而且由于 W 是 3 * 3 的，因此 $det(W)$ 也很容易计算。

### 5.2 Demo

OpenAI 有一个关于 GLAW 的 demo：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219153237718.png" alt="image-20221219153237718" style="zoom:55%;" />

它还可以让人给笑起来。给他一堆 labeled  的笑与不笑的图片，它可以让人笑起来：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219153447510.png" alt="image-20221219153447510" style="zoom:67%;" />

### 5.3 To Learn More …

GLOW 有很多应用，尤其是在语音合成上，因为不知道为什么，GAN 做语音合成的效果很差，下面是一些应用：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221219153623155.png" alt="image-20221219153623155" style="zoom: 67%;" />