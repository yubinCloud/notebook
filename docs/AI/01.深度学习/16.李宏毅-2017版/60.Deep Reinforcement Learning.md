---
title: Deep Reinforcement Learning
date: 2024-10-03 10:18:00
permalink: /pages/ml/lhy/drl17/
categories:
  - AI
  - 机器学习
  - 李宏毅-机器学习（2017）
tags:
  - 
---

强化学习在新版课程中已经有讲解了，这里是老版课程的笔记，其中介绍了 PPO 以及 Deep Q Learning 的 RL 方法。关于 RL 的基本知识，这里就不再重复了。

## 1. RL 中的困难

RL 中主要有两个难点：

- **Reward delay**：<u>一个 game 中，很多 action 并不能立刻取得游戏分数，但对 Actor 在之后取得游戏分数至关重要</u>。比如 space invader 游戏中，左右移动并不会获得分数，只有 fire 动作会获得分数，如果仅仅将游戏奖励当作 RL 的 reward，那训练的 Actor 会倾向于一直 fire 而不在左右移动。
- **Agent’s actions affect the subsequent data it receives**：Actor 的 action 会影响它接下来所看到的画面，所以在 RL 中，让 Actor 能**探索**没有做过的行为，是一件很重要的事情。

## 2. Outline

### 2.1 Policy-based 和 Valued-based

![image-20241003102359101](/Users/yubin/Pictures/typora/60.Deep Reinforcement Learning/image-20241003102359101.png)

- 在 Policy-based 的方法中，会 learn 一个负责做事情的 Actor
- 在 Valued-based 的方法中，会 learn 一个不做事情的 Critic，它专门批评
- 要把 Actor 和 Critic 加起来的，叫做 Actor-Critic 方法

截止上课时，当前最新的 A3C 就是 *Asynchronous  Advantage Actor-Critic* 方法。

### 2.2 RL 也是寻找一个 Function

RL 中的 Actor 在决定采取 action 时，其实就是：$Action = \pi(Observation)$。Actor 就是这里面的 $\pi$，也就是我们想寻找的 function。

> 有些 paper 中也称这里面的 Actor 为 *Policy*

RL 的过程可以看成三步：

1. Neural network as Actor
2. 决定一个 Actor 的好坏
3. 选一个最好的 Actor：Gradient Ascent

#### 1）Neural network as Actor

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003104852884.png" alt="image-20241003104852884" style="zoom:67%;" />

相比于 lookup table，使用 NN 作为 actor 能够具有更好的泛化性。

#### 2）决定一个 Actor 的好坏

这里用 actor $\pi_\theta$ 表示具有 parameter $\theta$ 的 network，使用 s 表示 observation

让 actor 去玩一个 game，用 $R_\theta$ 表示 total reward。

对于同一个 actor，尽管每次采取相同的 actions，也有可能得到不同的 total reward。所以我们定义 $\bar{R_\theta}$ 表示 expected value of $R_\theta$。

所以，**$\bar{R_\theta}$ 就衡量了 actor 的好坏**。

---

**$\bar{R_{\theta}}$ 怎样计算呢**？

一个 episode 可以视为一个 trajectory $\tau$，它是一个由 observation、action、reward 交替出现而形成的 sequence。

当一个 actor 确定后，$\tau$ 出现的概率也就确定了，每一次 play game 都相当于从这个概率中做一次采样，这个概率可以写成 $P(\tau|\theta)$。

> 可以想象，不同的人来玩同一个游戏，得到的 trajectory 大概率服从的概率分布也是不同的

这样，expected value 就可以这样计算了：

$$\bar{R_{\theta}} = \sum_{\tau} R(\tau)P(\tau|\theta) \approx \frac{1}{N} \sum^N_{n=1}R(\tau^n)$$

理论上，可以 sum over all possible trajectory，但实际中，你只能让 actor $\pi_\theta$ 去 play game N 次，得到 N 个 trajectory $\{ \tau^1, \tau^2, \dots, \tau^N \}$，也就相当于从 $P(\tau|\theta)$ 中 sample 出 trajectory N 次。

#### 3）选一个最好的 function：Gradient Ascent

既然知道了怎样衡量 actor，那就可以选出一个最好的 actor。

做法就是：**Gradient Ascent**

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003113024610.png" alt="image-20241003113024610" style="zoom:80%;" />

这里的 $\bar{R_{\theta}}$ 的微分怎样计算呢？经过推导：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003113142966.png" alt="image-20241003113142966" style="zoom:80%;" />

除此之外，可能还需要给公式中的 $R(\tau^n)$ 减去一个 baseline 来标准化，从而让他有正有负。

## 3. PPO

### 3.1 on-policy 到 off-policy

on-policy 和 off-policy 两种 training 方法的区别如下：

- **on policy**：The agent learned and the agent interacting with the environment is <u>the same</u>.
- **off policy**：The agent learned and the agent interacting with the environment is <u>different</u>.

在 on-policy 方法中，我们会使用 actor $\pi_\theta$ 去收集这一轮的数据，然后更新网络参数 $\theta$，一旦这个网络参数被更新，那下一轮就需要重新再次收集数据。而 collect data 的过程很耗费时间，导致 on-policy 方法的训练速度较慢。

思路：使用另一个 actor $\pi_{\theta'}$ 去 sample 数据，然后用于更新 $\theta$。由于 $\theta'$ 是 fixed，所以可以重复利用 sample data。

 <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003140934413.png" alt="image-20241003140934413" style="zoom: 67%;" />

### 3.2 Importance Sampling

在具体讲解之前，需要先看一个比较泛用的技术：<mark>Importance Sampling</mark>。

我们在计算 f(x) 在概率分布 p(x) 下的期望时，往往可以从 p(x) 概率分布中采样出一堆 $x^i$，然后计算这些样本的 f(x) 的均值，作为对期望的估计：

$$E_{x \sim p}[f(x)] \approx \frac{1}{N} \sum^N_{i=1}f(x^i)$$

但如果我们不能从 p(x) 概率分布中做采样的话，那这里就不能这样计算了。可以使用一个技巧做一下转换：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003141856461.png" alt="image-20241003141856461" style="zoom: 50%;" />

可以看到**经过转换，期望的计算只需要能够从另外一个分布 q(x) 中做采样就好了，而不需要从 p(x) 做采样**。

尽管理论上可以这么等价，**但是在实际中并不能随便选一个 q(x) 概率分布，而是要求 q(x) 与 p(x) 不能差太多**。感性上的原因是：以上公式中计算期望是等价的，但这两个的方差却是不同的，当方差差距过大的时候，采样数据量不足的话可能会让结果出现很严重的偏差。因为我们计算期望不是直接计算的，而是通过采样来估计的，方差差距过大会让这种估计变得很不稳定。

### 3.3 使用 Importance Sampling 后

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003144629662.png" alt="image-20241003144629662" style="zoom: 50%;" />

将上面的 $\bar{R_{\theta}}$ 经过多步转化后，可以得到如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003151419424.png" alt="image-20241003151419424" style="zoom: 50%;" />

于是有了 $J^{\theta'}(\theta)$ 这个 objective function。

### 3.4 PPO 1：近端策略优化惩罚

**Proximal Policy Opetimization**（**PPO**）

但前面因为使用了 Importance Sampling，所以 $\theta'$ 与 $\theta$ 是不能差太多的，于是 PPO 在 objective function 上又加了一项 $KL(\theta, \theta')$ 来作为 constrant：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003151819506.png" alt="image-20241003151819506" style="zoom: 50%;" />

这样也就限制了训练出来的 $\theta$ 与 $\theta'$ 越像越好。

> PPO 的前身有一个 TRPO 方法，他的 objective function 直接就是 $J^{\theta'}(\theta)$，同时另外把 $KL(\theta', \theta) \lt \delta$  当做了额外的限制条件，但这样的优化问题就难以来实施了，所以不如 PPO 那样直接把 constrants 写进了 objective function 中更容易实践。

注意，这里的 KL 计算的是两个 actor 的 behavior 的差距，而并非只是简单的两个参数的距离。

<mark>PPO Algorithm</mark>：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003153111247.png" alt="image-20241003153111247" style="zoom: 50%;" />

可以看到，与 on-policy 的训练方法明显不同的一点就是，这里在 collect 一批 data 后，可以用来多次 update parameters。

KL 惩罚项的系数 $\beta$ 的计算方式也很直观，如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003153245577.png" alt="image-20241003153245577" style="zoom:80%;" />

如果 update 后 KL 差距过大，说明 KL 惩罚项起到的作用较少，这时候应该增大惩罚项的作用，反之亦然。

另外，这里的 KL 的计算也是 sample 出一大堆数据来计算。

### 3.5 PPO 2：近端策略优化裁剪

PPO 算法还有另一种实现方式，不将 KL 散度直接放入似然函数中，而是进行一定程度的裁剪。

这种实现方式中，公式看着很复杂，但是想要表达的思想特别直观。PPO 2 的算法如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20241003154606898.png" alt="image-20241003154606898" style="zoom:67%;" />

可以看到，这个 objective function 是一个 min 函数，包含两项，下半部分的坐标图的横轴代表 $\frac{P_\theta}{P_{\theta^k}}$，其中，绿色的线代表 min 中的第一项，即不做任何处理，蓝色的线为第二项，如果两个分布差距太大，则进行一定程度的裁剪。最后对这两项再取 min，防止了 θ 更新太快。

上式看起来很复杂，其实很简单，它想做的事情就是希望 $p_\theta(a_t | s_t)$ 跟 $p_{\theta^k}(a_t | s_t)$，也就是做示范的模型跟实际上学习的模型，在优化以后不要差距太大。

- 操作符 min 作用是在第一项和第二项中选择最小的。
- 第二项前面有个裁剪（clip）函数，裁剪函数是指：在括号里有三项，如果第一项小于第二项，则输出1 − ε；如果第一项大于第三项的话，则输出1 + ε。
- ε 是一个超参数，要需要我们调整的，一般设置为0.1或0.2 。

坐标图中，红色的线就是整个 min 函数的值，可以看到，当 A 大于 0 时，我们自然希望 $\frac{P_\theta}{P_{\theta^k}}$ 越大越好，但是可以看到，当过大的时候，值就不变化了，梯度就没了，这也就限制了两者的比值不能超出某个范围，从而限制了两个分布的相似性。

以上就是 PPO 的算法。

