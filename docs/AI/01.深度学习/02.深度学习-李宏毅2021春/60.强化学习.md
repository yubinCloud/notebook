---
title: 强化学习
date: 2022-06-30 16:39:22
permalink: /pages/lhy/RL/
categories:
  - AI
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

之前所讲的技术基本上都是基于 Supervised Learning，它们的数据都有一个 label，但在 RL 里面就是要面对另一个问题：**机器当给我们一个 input 的时候，我们不知道最佳的 output 应该是什么**。比如下围棋，面对一个盘势，怎样的下一步是最好的答案是不知道的（尽管一些棋谱中能给出较好的答案），**在这个你不知道正确答案是什么的情况下，往往就是 RL 可以派上用场的时候**。

但是 RL 在学习时，机器也不是一无所知的，我们虽然不知道正确的答案是什么，但机器会知道什么是好，什么是不好，机器会与环境互动，得到一个叫做 reward 的东西。藉由知道什么样的输出是好的，什么是不好的，机器还是可以学出一个模型。

本章的 Outline：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630170307941.png" alt="image-20220630170307941" style="zoom:67%;" /></center>

我们想说的是 RL 也跟 Machine Learning 一样也是三个步骤，具有相同的框架，不要觉得难学。

## 1. What is RL?

### 1.1 相关概念

我们已经说了 Machine Learning 就是找一个 function，RL 同样也是如此。

在 RL 里面，会有一个 <mark>Actor</mark>，还有一个 <mark>Environment</mark>，这个 **Actor 与 Environment 会进行互动**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630170736614.png" alt="image-20220630170736614" style="zoom: 85%;" /></center>

+ Environment 会给 Actor 一个 Observation 作为输入；
+ Actor 看到 Observation 后会有一个输出 Action，这个 Action 会去影响 Environment；
+ Actor 采取 Action 后，Environment 会给予一个新的 Observation；

这里的 Actor 本身就是一个 function，是我们所要找的 function，其输入是 Environment 给它的 Observation，输出是这个 Actor 要采取的 Action，在这个互动过程中，Environment 会不断地给 Actor 一些 Reward，来告诉它说这个 Action 是好的还是不好的。**要找的 Actor 这个 function 就是最大化最终得到的 Reward 的总和**。

### 1.2 Example: Playing Video Game

拿 Space Invader 这个简单的小游戏来作为例子。最早的几篇 RL 的论文都是让机器去玩 Space Invader 这个游戏。

> Space Invader：
>
> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630185529191.png" alt="image-20220630185529191" style="zoom: 88%;" />
>
> + 要操控的是下面的绿色太空梭，可以采取的 Action 有三个：左移、右移和开火，要做的就是杀掉画面上的外星人。
> + 开火几种黄色的外星人的话，外星人就死掉了。
> + 你前面橙色的东西是防护罩，你不小心打到它也会使它减小，也可以躲在它后面抵挡外星人的攻击。
> + 杀死外星人或者打掉最上面的补给包会奖励 score，这个 score 就是 Environment 给我们的 Reward。
> + 游戏的终止条件：外星人都被杀光或者你的母舰被外星人击中。

如果要用 Actor 去玩 Space Invader：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630190050323.png" alt="image-20220630190050323"  /></center>

Actor 坐在一个人的角度去操控摇杆，Environment 就是游戏的主机，Observation 是游戏的画面，Action 是向左、向右和开火三种可能的行为之一，得到的 score 就是 Reward。游戏画面改变的时候，就代表有了新的 Observation 进来，此时你的 Actor 就会决定采取新的 Action。

我们的目标就是 learn 出一个 Actor，它可以在玩这个游戏时得到的 Reward 总和是最大的。

### 1.3 Example: Learning to play Go

实如果把 RL 拿来玩围棋，那你的 Actor 就是就是 AlphaGo，Environment 就是 AlphaGo 的人类对手。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630191146451.png" alt="image-20220630191146451" style="zoom: 80%;" /></center>

在下围棋里面，Actor 所采取的行为几乎没有办法得到任何 Reward，而是定义说如果赢了就得到 1 分，输了就得到 -1 分。

### 1.4 RL 的三个步骤

课程一开始就说了 Machine Learning 就是三个步骤：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630191659242.png" alt="image-20220630191659242" style="zoom:80%;" />

1. 有一个 function，里面有一些未知的 params，这些未知数是要被找出来的；
2. 定一个 loss function
3. Optimization：想办法找出未知 params 去最小化 loss

其实 RL 也是一样的三个步骤，分别来看一下。

#### :footprints: Step 1：Function with Unknown

这里有未知数的 function 就是 Actor，这个 Actor 就是一个 Network，现在通常叫它 **Policy Network**。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630192153053.png" alt="image-20220630192153053" style="zoom:80%;" />

这个 Network 会给每个可能的 Action 输出一个得分，且这些得分总和为 1：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630192335745.png" alt="image-20220630192335745"  />

至于这个 Network 的架构，可以你自己去设计，它可以是 CNN，也可以是 RNN 甚至是 Transformer 等。

在最后机器在决定采取哪一个 Action 时取决于输出的每一个 Action 的分数。**常见的做法是把这个分数当做一个概率，然后按照这个概率去 Sample，从而随机决定要采取哪一个 Action**。比如上图中，“向左”得到 0.7 分，那就有 70% 的几率采取这个 Action。采取 Sample 这种思路的好处是机器的每一次所采取的行为会略有不同，不至于太死板。比如你剪刀石头布时，总出石头也会被打爆。

#### :footprints: Step 2：Define “Loss”

在 RL 里面，loss 长什么样呢？我们先看一下互动的过程。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630193027037.png" alt="image-20220630193027037"  />

+ 初始游戏画面 Observation $s_1$ 输给 Actor，采取 Action $a_1$，得到 Reward $r_1$，并产生 Observation $s_2$，继续刚刚的过程。

不断刚刚的过程直到机器采取某个 Action 后游戏结束了，那从游戏开始到结束的这整个过程称为一个 <mark>Episode</mark>。整个游戏过程所有得到的 Reward 累加称为 <mark>Total Reward</mark>，也称为 <mark>Return</mark>。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630193645220.png" alt="image-20220630193645220" style="zoom:80%;" /></center>

这里目标是 Total Reward 越大越好，但 loss 是越小越好，所以在 RL 的情景下，我们可以把 Total Reward 取负号当做我们的 loss。

#### :footprints: Step 3：Optimization

Actor 与 Environment 互动的过程再用图表示一次如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630194224430.png" alt="image-20220630194224430" style="zoom:88%;" />

$s$ 与 $a$ 形成的 sequence $s_1 \ a_1 \ s_2 \ a_2 \ s_3 \dots$ 叫做 <mark>Trajectory</mark>，也记作 $\tau$。

**Reward $r_i$ 通过 Observation $s_i$ 和 Action $a_i$ 计算得到**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630194633904.png" alt="image-20220630194633904"  />

**整个 Optimization 的过程就是找一个 Network 的参数，让产生出来的 Return $R$ 越大越好**。乍看起来没什么难的，但 **RL 困难的地方在于，这不是一个一般的 Optimization 问题**，因为 Environment 有很多问题导致它跟一般的 Network Training 不太一样。

+ 第一个问题是，Actor 的 output 具有随机性。因为其输出的 Action 是 sample 产生的，如果把整个 Environment、Actor 和  Reward 合起来当成一个巨大的 network 来看待，那这个 network 可不一般，它里面的某一层 layer 每次产生的结果是不一样的。
+ 另一个更大的问题是，你的 Environment 和 Reward 根本就是 network，他们只是一个黑盒子而已，你根本不知道里面发生了什么事。刚刚所说的 Reward 是明确的一条规则，但更麻烦的是在一些 RL 问题中，Reward 与 Env 都是具有随机性的。比如在电玩的应用中，同样的 Action，游戏机到底给你怎样的回应是有乱数的。

目前一般的 gradient descent 还无法训练这个 network 来找出 Actor 来最大化 Return。所以 **RL 真正难点在于怎么解这个 Optimization 问题**。这就是 RL 跟一般的 ML 不一样的地方。

但我们还是可以把 RL 看成三个阶段，只是 maximize reward 时跟以前的方法有点不一样。

## 2. Policy Gradient

RL 中用来解 Optimization 的一个常用演算法是 <mark>Policy Gradient</mark>。

### 2.1 How to control your actor

在讲 Policy Gradient 之前，我们先来看看怎样操控一个 Actor 的输出，即怎样让一个 Actor 在看到某一个特定 Observation 时采取一个特定的 Action。

这其实可以想成一个分类的问题，比如让 Actor 输入 s，输出是 $\hat a$：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630215326157.png" alt="image-20220630215326157" style="zoom: 80%;" /></center>

假设你希望 Actor 采取 $\hat a$ 这个行为的话，就定一个 loss，这个 loss 等于 Cross-Entropy，然后用它来 train。

但假设你还想让你的 Actor 不要采取某个行为，比如输入 $s'$，不要采取 $\hat a'$，可以这样做：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630220205070.png" alt="image-20220630220205070" style="zoom:80%;" /></center>

这个过程就像在 train 一个 classifier 一样，用于去控制 Actor 的行为，而且这一部分的过程就是 Supervised Learning，等下会看到它与一般的 Supervised Learning 有啥区别。

所以我们要 train 一个 Actor，其实就是需要收集一些训练资料，这个训练资料说希望在 $s_1$ 时采取 $\hat a_1$，希望在 $s_2$ 时不采取 $\hat a_2$。如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630220702592.png" alt="image-20220630220702592" style="zoom:80%;" /></center>

甚至还可以更进一步，可以说每一个行为并不是只有好或者不好，而是有程度区别的，有非常好的，有 nice to have 的，有 a little bad 的，有非常差的。所以我们现在给每一个 $s_i$ 和 $\hat a_i$ 的 pair 都对应一个分数，这个分数代表了我们多希望机器在看到 $s_i$ 时去执行 $\hat a_i$ 这个 action。如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630221154046.png" alt="image-20220630221154046" style="zoom:80%;" /></center>

+ 比如说看到 $s_1$ 时我们很期待执行 $\hat a_1$，看到 $s_3$ 时我们也期待执行 $\hat a_3$，但期待程度不如前者那么高。
+ 比如说看到 $s_N$ 时特别不期待执行 $\hat a_N$，而看到 $s_2$ 时虽然也不期待执行 $\hat a_2$，但真执行了伤害也没有很大。

所以我们透过 $A_n$ 可以控制每一个 action 我们有多希望 Actor 去执行，接下来有下面这个 loss 后就一样可以 train 一个 $\theta^*$，从而找到一个符合我们期待的 Actor。

<center> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220630221703100.png" alt="image-20220630221703100" style="zoom:80%;" /></center>

接下来的难点就是怎样定出 $A_n$，还有一个问题是怎样产生 $s_i$ 与 $\hat a_i$ 这对 pair，并怎样知道期不期待执行。

### 2.2 不同 version 的 idea

之前所看到流程与 Supervised Learning 没什么不同，接下来的重点是怎么定义 A。

#### 2.2.1 Version 0

先说一个最简单的版本，其实也是不怎么正确的版本。助教的 sample code 就是这个版本。

首先还是需要收集一些训练资料，就是需要**收集 s 和 a 的 pair**。怎么收集呢？你需要先有一个 Actor 与 Env 互动，就可以收集到 s 与 a 的 pair。

一开始 Actor 是随机的东西，然后把它每一个 s 执行的行为 a 记录下来，这个过程通常要让 Actor 与 Env 做多个 Episode，然后就可以收集到足够的资料。比如助教的 sample  code 就是跑了 5 个 Episode。

接下来**我们需要去评价每一个 action 到底是好还是不好**，评价完后既可以拿这个评价结果来训练我们的 Actor。怎样评价呢？之前说过我们用 $A_i$ 来评价一个 pair $\{s_i, a_i\}$，在某一个 step 的 observation $s_i$ 时，执行了 $a_i$，然后会得到 reward $r_i$，那如果 reward 是正的，那也许就代表这个 action 是好的，如果 reward 是负的，那也许就代表这个 action 是不好的。所以我们可以就让 $A_i = r_i$：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701135529526.png" alt="image-20220701135529526" style="zoom: 80%;" /></center>

之所以说这是一个不好的版本，是因为这样得到的 Network 是一个短视近利的 Actor，它只知道会一时爽，完全没有长程规划的概念。比如在 Space Invader 游戏中，只有 “fire” 才会得到 reward，那训练的 Actor 将会倾向于一直开火。

我们知道说说每一个行为其实都会影响互动接下来的发展，所以说**每一个行为并不是独立的，每一个行
为都会影响到接下来发生的事情**。我们今天在与 Env 互动时，有一个问题是 <mark>Reward Delay</mark>，就是有时候你需要牺牲短期的利益以换取更长程的目标。

接下来我们正式进入 RL 的领域，来看看真正的 Policy Gradient 是怎么做的。

#### 2.2.2 Version 1

**在 Version 1 里，$a_1$ 有多好不是取决于 $r_1$，而是取决于 $a_1$ 之后所有发生的事情**。我们会把 $a_1$ 执行完后所有得到的 Reward $r1,r2,\dots,r_N$ 通通加起来，得到一个数值 $G_1$，然后就让 $A_1 := G_1$，拿这个 $G_1$ 当作评估一个 Action 好不好的标准。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701140644292.png" alt="image-20220701140644292" style="zoom:80%;" /></center>

这里的 $G_t$ 叫做 <mark> Cumulated Reward</mark>，<u>它从 t 这个时间点开始，把 $r_t$ 一直加到 $r_N$</u>，用它来评估一个 Action 的好坏，这听起来比之前合理多了。

但仔细想想会发现，这个 version 也有问题。假设这个游戏非常长，你把 $r_N$ 归功于 $a_1$ 好像也不太合适，因为说有做了 $a_1$ 才导致了 $r_N$ 的可能性很低。来看下一个 version。

#### 2.2.3 Version 2

Version 2 的 Cumulated Reward 用 $G'$ 表示，在计算它时，我们会在 $r$ 前面乘一个 Discount factor $\gamma$，这是一个小于 1 的值。$G_1$ 的计算时 $r_1$ 加上 $r_2$ 再加上 $r_3$ 等等，但 $G_1'$ 是 $r_1$ 加上 $\gamma \cdot r_2$ 再加上 $\gamma^2 \cdot r_3$ 等等，也就是距离采取这个 Action 越远，我们的 $\gamma$ 平方项就越多，也就是其 $r$ 的作用越小。如下图所示：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701141942019.png" alt="image-20220701141942019" style="zoom:80%;" /></center>

这个方法把距离 $a_1$ 比较近的那些 reward 给予了比较大的权重。当然你也可以改变  $G'$的计算方式来改变语义。

> 像围棋这种游戏结尾才有分数的游戏，可以这样做：采取一连串 action，只要最后赢了，这一串的 action 都是好的，如果输了，这一连串的 action，通通都算是不好的。你也能感觉出来这很难 train，确实很难 train，最早版本的 AlphaGo 就是这样 train 的。

#### 2.2.4 Version 3

**好和坏是相对的**，比如你一门课考了 60，这好不好呢，需要看别人都得了多少分，如果别人都是三四十分，那你就是最好，如果别人都是八九十分，那你就很不厉害，所以 reward 这个东西是相对的。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701143555749.png" alt="image-20220701143555749" style="zoom:80%;" /></center>

如果我们只是单纯地算 $G$，那么可能遇到一个问题：在这个游戏里始终都是拿到正的分数，这样即便有些 action 是不好的，但你仍鼓励 model 去采取这些 action。这怎么办呢？我们**需要做一个标准化**。最简单的一个方法是把所有的 $G'$ 都减掉一个 b，这个 b 通常叫做 <mark>Baseline</mark>。这样做的目的是让 $G'$ 有正有负。这还有一个问题：**怎样设定这个一个好的 Baseline**，从而让 $G'$ 有正有负呢？接下来的版本还会提到，目前先讲到这个地方。

### 2.3 Policy Gradient 怎么操作？

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701143816438.png" alt="image-20220701143816438" style="zoom:80%;" /></center>

首先给你的 Actor 一个 random params $\theta^0$，然后接下来进入 training iteration。在每一轮 iteration 中，你拿你的 Actor 去跟 Environment 做互动，得到一大堆 s 与 a 的 pair，接下来对他们进行评价，用 $A_i$ 来评价这些 action 好还是不好。计算好 A 后，就可以按照 loss function 定义计算 loss，然后用它来 update 你的 model，这个 update 的过程与 gradient descent 一模一样。

这里有一个很神奇的地方，一般的 training 的 data collection 是 for 循环外，比如说我有一堆资料，然后用他们来做 training，来做 update model，最后得到一个收敛的参数，然后拿这个参数来做 testing。但在 RL 里不是这样，你 data collection 的过程是在 for 循环里面的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701145018799.png" alt="image-20220701145018799" style="zoom:80%;" /></center>

这样假设你打算跑 for 循环 400 次，那你就得收集资料 400 次，**一旦更新完一次参数以后，接下来你就要重新去收集资料了**。用图形化的方式表示如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701145143452.png" alt="image-20220701145143452" style="zoom:80%;" /></center>

**为什么不能事先收集资料，而是每次 update network 后就要重新再收集资料呢**？这边一个比较简单的比喻是”一个人的食物，可能是另外一个人的毒药“，由 $\theta_{i-1}$ 收集的资料是它与环境互动的经验，这些经验可以拿来更新 $\theta_{i-1}$，但这些经验并不一定适合参数为 $\theta_i$ 的 model。所以**同一个 action，对不同的 Actor 而言，它的好坏是不一样的**。而且在面对同一个 Observation $s_i$ 时，不同参数的 model 可能采取的行为也不一样。**每次 Actor 的更新都要重新去收集资料，这也是 RL 的 training 为什么非常花时间的原因**。

### 2.4 On-policy v.s. Off-policy

刚刚我们说的这个训练的 Actor 跟要拿来跟环境互动的 Actor 是同一个，这种情况叫做 <mark>On-policy Learning</mark>。刚刚所示范的 Policy Gradient 的 algorithm 就是 On-policy 的 learning。还存在另外一种状况叫做 <mark>Off-policy Learning</mark>。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701150527366.png" alt="image-20220701150527366" style="zoom:80%;" /></center>

这里不再细讲 Off-policy 的 learning，Off-policy 的 Learning 期待能够做到的事情是让要训练的 Actor 与跟环境互动的 Actor 是分开的两个 Actor，也就是解决让训练的 Actor 能不能根据其他 Actor 跟环境互动的经验来进行学习的问题。

Off-policy 有一个非常显而易见的好处是你就不用一直收集资料了，它可以做到收集一次资料就 update 参数很多次。毕竟每轮 iteration 都重新收集资料是导致训练时间慢的一个重要原因。

有一个经典的 Off-policy 的方法 —— <mark>Proximal Policy Optimization</mark>，缩写是 <mark>PPO</mark>，这也是蛮强的一个方法。Off-policy 的重点就是：你在训练的那个 Network 要知道自己跟别人之间的差距，它要有意识的知道说它跟环境互动的那个 Actor 是不一样的。比如美国队长克里斯伊凡追女孩的方法只需要一个表白，而你就不一样了。具体的细节做法可以参考过去上课的录影。

### 2.5 Collection Training Data: Exploration

还有一个重要的概念叫做 <mark>Exploration</mark>，指的是今天**这个 Actor 在采取行为时是有一些随机性的**。而这个随机性其实非常地重要，很多时候你随机性不够，你会 train 不起来。举一个最简单的例子，假设你一开始初始的 Actor 永远都只会向右移动，如果它从来没有采取开火这个行为，它就永远不知道开火这件事情到底是好还是不好，它只有做了这件事，我们才能评估这个 action 好还是不好。

所以你今天在训练的过程中，这个拿去跟环境的互动的这个 Actor 本身的随机性是非常重要的。你其实会期待说跟环境互动的这个 Actor 随机性可以大一点，这样我们才能够收集到比较丰富的资料，防止出现有一些状况的 reward 是从来不知道的。**为了要让这个 Actor 的随机性大一点，甚至你在 training 的时候，你会刻意加大它的随机性**。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220701152346275.png" alt="image-20220701152346275" style="zoom:80%;" /></center>

比如说 Actor 的 output 本来是一个 distribution，有人会刻意加大这个 distribution 的 entropy，让它在训练时比较容易 sample 到那些几率比较低的 action。或者有人在 Actor 的 parameters 上加 noise，让它每一次采取的 action 都不一样。这个过程就是 Exploration，它是 RL training 中一个非常重要的技巧。

