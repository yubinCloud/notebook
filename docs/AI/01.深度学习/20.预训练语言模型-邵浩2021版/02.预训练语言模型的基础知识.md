---
title: 预训练模型的基础知识
date: 2022-08-26 22:01:00
permalink: /pages/dl/pretrained-lang-model/basic/
categories:
  - AI
  - 深度学习
  - 预训练语言模型
tags:
  - 
---

[[toc]]

简单来讲，语言模型就是计算一个句子的概率。

::: theorem 语言模型
给定一句由 n 个词组成的句子 $W=w_1,w_2,\dots,w_n$，计算这个句子的概率 $P(w_1,\dots,w_n)$，或者根据前文计算下一个词的概率 $P(w_n|w_1,w_2,\dots,w_{n-1})$ 的模型，称为<mark>语言模型</mark>。
:::

语言模型在 NLP 中用途很广泛，比如可以从几个识别出来的不确定的候选句子中选一个概率最大的句子。

## 1. Statistical Language Model

### 1.1 What is Statistical Language Model

<mark>统计语言模型</mark>（Statistical Language Model）的基本思想就是计算条件概率，对一个句子 $W=w_1,w_2,\dots,w_n$ 计算 P 的公式如下：

$$P(w_n|w_1,w_2,\dots,w_{n-1})=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2) \cdots P(w_n|w_1,w_2,\dots,w_{n-1})$$

以“判断这个词的词性”这句话为例，当给定前面的词序列是“判断，这个，词，的”时，想要知道下一个词是什么，则套用上面公式的右半部分得到：

$$P(w_{next}|判断,这个,词,的)=\frac{count(w_{next},判断,这个,词,的)}{count(判断,这个,词,的)}$$

+ 其中 $w_{next} \in V$ 表示词序列的下一个词，V 是词典。

### 1.2 n-gram Language Model

然而当句子很长时，上式的计算是很困难的，因为需要列举 $w_{next}$ 和其他词共现的所有可能情况并统计出现次数，但实际上，有一些词作为 $w_{next}$ 基本上不可能和前文同时出现，最后有很多计算结果都是 0。那么有没有简单的方法来计算这个数值呢？

这里需要引入 Markov Chain，也就是只假设 $w_{next}$ 只和其之前的一个 word 有相关性：

$$P(w_{next}|判断,这个,词,的) \approx P(w_{next}|的)$$

如果一个词的相关性不明显，则可以将条件放宽至两个词：

$$P(w_{next}|判断,这个,词,的) \approx P(w_{next}|词,的)$$

也就是用 $P(w_n|w_{n-k},\cdots,w_{n-1})$ 近似替代了之前的公式 $P(w_n|w_1,w_2,\cdots,w_{n-1})$。在工程实践中通常将 k 设置为 2，3 或 n，分别将它们称为二元语言模型（Bigram Language Model），三元语言模型（Trigram Language Model），以及 **n 元语言模型**（<mark>n-gram Language Model</mark>）。

在二元语言模型中，$P(w_i|w_1,w_2,\cdots,w_{i-1}) \approx P(w_i|w_{n-1})$，通过极大似然估计，可以写成：

$$P(w_i|w_{n-1}) = \frac{count(w_{i-1},w_i)}{count(w_{i-1})}$$

计算方式为先计算“$w_{i-1},w_i$” 同时按序出现的次数，再除以 $w_{i-1}$ 出现的次数、

### 1.3 Perplexity（困惑度）

