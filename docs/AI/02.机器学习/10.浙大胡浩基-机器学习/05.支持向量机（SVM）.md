---
title: 支持向量机（SVM）
date: 2022-06-26 22:18:58
permalink: /pages/ml/hhj/svm
categories:
  - AI
  - 机器学习
  - 浙大胡浩基-机器学习
tags:
  - 
---

## 1. 线性可分定义

在二维中，线性可分表示为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104445520.png" alt="image-20220707104445520" style="zoom: 67%;" /></center>

+ 存在一条直线将 x 和 ○ 分开

线性不可分则表示为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104819974.png" alt="image-20220707104819974" style="zoom:67%;" /></center>

+ 此时不存在一条直线将 x 与 ○ 分开

如果扩展到三维，则是用平面来分割：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104935191.png" alt="image-20220707104935191" style="zoom: 67%;" /></center>

如果维度大于等于四维，那么就是用**超平面**（Hyperplane）来分割。

我们借助数学对 Linear Separable 和 Nonlinear Separable 进行定义，以二维为例，直线就可以表示为一个方程：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707105224545.png" alt="image-20220707105224545" style="zoom:75%;" /></center>

+ 在直线的其中一边，$\omega_1 x_1 + \omega_2 x_2 + b \gt 0$，另一边则相反。
+ 可不可以让右边是小于 0 呢？可以呀，只需要假设 $\omega'=-\omega, b'=-b$，就可以是让右边小于 0，左边大于 0 了。所以我们只需要分析我们上图所示的情况即可。

接下来我们**用数学严格定义**训练样本以及他们的标签：假设我们有 N 个训练样本和他们的标签 $\{ (X_1, y_1),(X_2,y_2),\dots,(X_N, y_N) \}$，其中 $X_i=[x_{i1}, x_{i2}]^T$，$y_i = \{+1,-1 \}$，并令 $X_i$ 属于 $C_1$ 时 $y_i=+1$，反之则为 $-1$。

线性可分的严格定义：一个训练样本及 $\{(X_i, y_i),\dots,(X_N, y_N) \}$，在 $i=1 \sim N$  线性可分，是指存在 $(\omega_1, \omega_2, b)$ 使得对 $i=1 \sim N$，有：

+ 若 $y_i=+1$，则 $\omega_1 x_1 + \omega_2 x_2 + b \gt 0$
+ 若 $y_i=-1$，则 $\omega_1 x_1 + \omega_2 x_2 + b \lt 0$

若将 $\omega$ 表示成一个 vector $\omega = [\omega_1, \omega_2]^T$，那么上面的定义可以写成：

+ 若 $y_i=+1$，则 $\omega^T X_i + b \gt 0$
+ 若 $y_i=-1$，则 $\omega^T X_i + b \lt 0$

## 2. 线性可分时的最优分类 hyperplane

### 2.1 什么是最优分类 hyperplane？

支持向量机算法分成了两个步骤：

1. 解决线性可分问题
2. 再将线性可分问题中获得的结论推广到线性不可分情况

我们先看一下他是如何解决线性可分问题的。

> 如果一个数据集是线性可分的，那么就存在无数多个 hyperplane 将各个类别分开。

既然有上面的结论，那哪一个是最好的呢？比如一个二分类的问题：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716193434391.png" alt="image-20220716193434391" style="zoom:80%;" /></center>

问哪一个最好呢？大多数都是说 2 号线。其实原因在于：2 号线更能低于训练样本位置的误差。那这个 2 号线怎么画出来的呢？Vapnik 给出了基于最优化理论的回答。

将这个直线向左向右移动，直到能碰上一个或几个训练样本为止：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716193736561.png" alt="image-20220716193736561" style="zoom:80%;" />

+ 我们将移动后的平行线所擦到的训练样本为 <mark>support vector</mark>。
+ 两条平行线之间的距离叫做 <mark>margin</mark>。

这样我们所要找的线就是使 margin 最大的线，为了是直线唯一，又规定这条线是擦到 support vector 的两个线的正中间的那条线，也就是上图中的 2 号线了。

经过上面讨论，我们知道了 SVM 寻找的最优分类直线应该满足：

1. 该直线分开了两个类；
2. 该直线最大化 margin；
3. 该直线处于间隔的中间，到所有 support vector 的距离相等。

上面的结论是基于二维特征空间的结果，扩展到更高维度后，直线将变成 hyperplane，此时被称为**最优分类 hyperplane**，但结论不会变。

### 2.2 寻找最优分类 hyperplane

下面我们讲如何用严格的数学来把寻找这个最优分类 hyperplane 的过程写成一个最优化问题。

假定训练样本集是线性可分的，那么 SVM 需要寻找的是最大化 margin 的 hyperplane，这个优化问题可以写成下面的形式：

+ minimize：$\frac{1}{2} ||\boldsymbol{\omega}||^2$
+ 限制条件：$y_i(\omega^Tx_i+b) \ge 1, (i=1 \sim N)$

下面讨论为什么会这样？我们先看两个个事实：

::: note 事实 1
$\omega^T x + b = 0$ 与 $(\alpha \omega^T)x+(ab) = 0$ 是同一个超平面。（$\alpha \neq 0$）
:::

::: note 事实 2
一个点 $X_0$ 到超平面 $\omega^T x + b = 0$ 的距离为：
$$d=\frac{|\omega^T x_0 + b|}{||\omega||}$$
:::

借助于这两个事实，我们可以用 a 去缩放 $\omega$ 和  $b$，即 $(\omega,b) \to (a\omega,ab)$，最终使得缩放后的 $\omega$ 和  $b$ 能够在 support vector $x_0$ 上有 $|\omega^Tx_0 + b|=1$，而在 non support vector 上，有 $|\omega^Tx_0 + b| \gt 1$。

为什么可以这样推导呢？根据事实 1 可知，用 a 缩放后的仍是同一个超平面，而根据事实 2，support vector $X_0$ 到超平面的距离将变为：

$$d=\frac{|\omega^T x_0 + b|}{||\omega||} = \frac{1}{||\omega||}$$

有上面的式子可以看出来，**最大化 support vector 到 hyperplane 的距离等价于最小化 $||\omega||$**。因此优化问题定为 minimize $\frac{1}{2} ||\boldsymbol{\omega}||^2$ 和 minimize $||\omega||$ 是一样的。所以优化问题可以写成这样。

我们再来看限制条件。

support vector 到 hyperplane 的距离是 $\frac{1}{2}||\epsilon||$，在 non support vector 上 $|\omega^T x_0 + b| \gt 1$。综合两者，我们可以写出 SVM 的限制条件：

$$y_i(\omega^T x_i + b) \ge \color{red}{1}，i=1 \sim N$$

+ 其中 $y_i$ 的作用是协调超平面的左右
+ 右边的 1（标红）可以改成任意的正数，因为修改后计算出来的 $\omega$ 和 $b$ 也只比原来差了 a 倍，而根据事实 1，他们代表的是同一个 plane。

总结一下：

::: warning 总结
线性可分情况下，SVM 寻找最佳 hyperplane 的优化问题可以表示为：

+ minimize：$\frac{1}{2} ||\boldsymbol{\omega}||^2$
+ 限制条件：$y_i(\omega^Tx_i+b) \ge 1, (i=1 \sim N)$

其中 $(X_i, y_i)$ 是已知的，$(\omega,b)$ 是待求的。
:::

可以看到，**我们所表述的这个问题是凸优化（Convex Optimization）问题中的二次规划问题**。

::: note 二次规划的定义
1. 目标函数（object function）是二次项
2. 限制条件是一次项
:::

而 Convex Optimization 问题中，只有唯一一个全局极值，我们可以利用 gradient descent 方法来求出这个全局极值，从而解决这个 Convex Optimization 问题。

> 我们这里不详细探讨如何解 Convex Optimization 问题，而是将这个寻找 hyperplane 的问题转换为一个 Convex Optimization，并直接利用解这类问题的工具包来解出这个问题。

## 3. 线性不可分情况

### 3.1 线性模型的局限

在线性不可分的情况下，不存在 $\omega$ 和 $b$ 满足上面所有 N 个限制条件作为最优化的解，所以需要适当放松限制条件。

放松限制条件的基本思路是，对每个训练样本及标签 $(X_i, Y_i)$，我们设置一个<mark>松弛变量</mark>（**slack variable**） $\delta_i$，并将 N 个限制条件改写为：

$$y_i(\omega^T X_i+b) \ge 1-\delta_i，i=1 \sim N$$

可以看到，只要每个 slack var $\delta_i$ 取得足够大，这些不等式的限制条件一定是可以满足的，所以我们还需要加入限制条件，以防止 slack var 无限地大。

改造后的 SVM 优化版本：

::: theorem 改造后的 SVM 的优化问题

+ minimize：$\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i$ 或 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i^2$
+ 限制条件：
  1. $\delta_i \ge 0，i = 1 \sim N$
  2. $y_i[\omega^T X_i+b] \ge 1-\delta_i，i=1 \sim N$
   :::

> 以前的 objective function 只需要最小化 $\frac{1}{2}||\omega||^2$，而现在加了一个正则化项，这样是想让所有 slack var 的和越小越好，比例因子 C 起到了平衡两项的作用。

这里用于平衡两项的**比例因子 C 是人为设定的**，因此它是一个 hyper parameter，这个 hyper param 需要去调的。尽管如此，SVM 也是 hyper param 较少的一个算法了，因此不需要花很多时间炼丹。

到了这里，其实上面的解线性不可分的情况有时是不行的，比如下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716210149264.png" alt="image-20220716210149264" style="zoom:67%;" />

这基本和瞎猜没什么区别，问题就出在：我们假设分开两类的函数是线性的。但线性模型的表现力是不够的，面对上面这个数据集总是无法找到一个直线可以分开两类，因此**我们需要扩大可选的函数范围从而应对更加复杂的线性不可分的情况**。

### 3.2 从低维到高维的映射

为了达成扩大可选函数范围的目标，像神经网络或决策树等都是直接产生更多可选函数，比如神经网络是通过多层非线性函数的组合，而 SVM 做法的思想是：**将特征空间从低维映射到高维，然后用线性 hyperplane 对数据进行分类**。

举个例子，考察下面如图的异或问题，这两类样本是在二维空间中是线性不可分的：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716220302773.png" alt="image-20220716220302773" style="zoom: 50%;" />

而如果构造一个从二维到五维的映射 $\varphi(X)$，在五维的特征空间中，这个例子将有可能变成线性可分：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716220705227.png" alt="image-20220716220705227" style="zoom: 50%;" />

这样映射到五维空间后，$\varphi(x_1), \dots,\varphi(x_4)$ 变得线性可分了，因为：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716221145907.png" alt="image-20220716221145907" style="zoom: 50%;" />

这样就是 $X_{1}、X_{2}$ 是一类，另外两个是一类。

可以看到，通过人为指定一个二维到五维的映射 $\varphi(X)$ 后，线性不可分的数据集变成了线性可分的数据集。这件事情说明了一个**更一般的结论**：

::: theorem
假设在一个 **M 维空间**上随机取 N 个训练样本，同时随机地对每个训练样本赋予标签 +1 或 -1，同时假设这些训练样本线性可分的概率为 P(M)，则有当 M 趋于无穷大时，P(M) = 1。
:::

> 这个结论我们不再证明了，但可以直观理解一下。当 feature space 的 维度 $M$ 增加后，待估计参数 $(\omega, b)$ 的维度也会增加，也就是说整个算法模型的自由度也会增加，当然也就更有可能分开低维时候无法分开的数据集。

这个定理告诉我们，**将训练样本从低维映射到高维后，会增大线性可分的概率**。我们先放下对 $\varphi(X)$ 具体形式的探讨，并假设其已经确定下来，来看看 SVM 的优化问题将做出什么样的改变：

::: theorem 改造后的 SVM 的优化问题
+ minimize：$\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i$ 或 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i^2$
+ 限制条件：
  1. $\delta_i \ge 0，i = 1 \sim N$
  2. $y_i[\omega^T \color{red}{\varphi(X_i)}+b] \ge 1-\delta_i，i=1 \sim N$
:::

+ 主要改变是 $X_i$ 被 $\varphi(X_i)$ 替换；
+ 之前是 $\omega$ 维度与 $X_i$ 维度相同，而**现在与 $\varphi(X_i)$ 维度相同**。

可以看到，**高维情况下优化问题的解法和低维情况是完全类似的**。

现在还剩下一个问题：在 SVM 中，低维到高维的映射 $\varphi(X)$ 取什么样的形式呢？

### 3.3 Kernel Function 的定义

本节具体研究 $\varphi(X)$ 的形式，并引入 Kernel Function 的定义。

SVM 的创始人 Vapnik 对回答 $\varphi(X)$ 的具体形式这一问题是非常有创意的。他指出，<u>我们可以不用知道 $\varphi(X)$ 的具体形式，取而代之，如果对于两个 vector $X_1、X_2$，我们知道 $K(X_1,X_2)=\varphi(X_1)^T\varphi(X_2)$，那么我们仍然能够通过一些技巧获得一个测试样本 $X$ 的类别信息，从而完成测试样本类别的预测</u>。

我们定义 $K(X_1,X_2)$ 为 <mark>Kernel Function</mark>，它的值是一个实数，这从它的等式右边就可以看出来。

我们举两个例子来说明 Kernel Function 与低维到高维的映射 $\varphi(X)$ 之间的相互关系。

::: note Example：已知映射求 Kernel Function
:watermelon: 首先举一个已知映射 $\varphi$ 求 Kernel Function $K$ 的例子：

假设 $\varphi(X)$ 是一个将二维向量映射为三维向量的映射，例如：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716225615547.png" alt="image-20220716225615547" style="zoom:50%;" />

我们看一下与这个 $\varphi$ 相对应的 Kernel Function 的形式。假设有两个二维向量：$X_1=[x_{11}, x_{12}]^T, X_2=[x_{21}, x_{22}]^T$，此时有：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716225837642.png" alt="image-20220716225837642" style="zoom:60%;" />
:::

::: note Example：已知 Kernel Function 求映射
:watermelon: 然后举一个已知 Kernel Function 求映射 $\varphi$ 的例子：

假设 X 是一个 2 dim vector，$X_1=[x_{11}, x_{12}]^T, X_2=[x_{21}, x_{22}]^T$，然后假设：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716231016401.png" alt="image-20220716231016401" style="zoom:60%;" />

根据定义 $K(X_1,X_2)=\varphi(X_1)^T\varphi(X_2)$，那么 $\varphi(X)$ 的形式就是：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716231103538.png" alt="image-20220716231103538" style="zoom:60%;" />

值得一提的是，如果 $\varphi(X)$ 是这种形式，那 $K(X_1,X_2)$ 也就一定是上面那种形式，这可以看出两者具有一一对应的关系。

:::

从上面例子中可以看出，**Kernel Function $K$ 与映射 $\varphi$ 是一一对应的关系**。还要指出的是，**Kernel Function 的形式不能随意的取**，它必须要满足一定的条件才能分解为两个 $\varphi$ 内积的形式。

有大佬提出了如下定理：

::: theorem Mercer’s Theorem
$K(X_1,X_2)$ 能写成 $\varphi(X_1)^T\varphi(X_2)$ 的充要条件是：

1. $K(X_1,X_2) = K(X_2,X_1)$  <font color=blue>【交换性】</font>
2. $\forall C_i(i=1 \sim N)$, $\forall N$ 有 $\sum^N_{i=1} \sum^N_{j=1} C_i C_j K(X_i X_j) \ge 0$    <font color=blue>【半正定性】</font>
:::

**只要 K 满足交换性和半正定性，那么它就能写成 $\varphi$ 内积的形式**。比如：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716233130866.png" alt="image-20220716233130866" style="zoom:60%;" />

但在这个例子中却不能显式写出 $\varphi(X)$ 的具体表达式。

 虽然我们无法知道 $\varphi(X)$ 的具体形式，但是我们却可以通过一些方法知道 $\omega^T \varphi(X) + b$ 的值，进而可以知道一个测试样本 X 所属的类别。

::: warning 本节总结
在这一节中，我们定义了 Kernel Function $K(X_1,X_2)$，同时指出了它和低维到高维的映射 $\varphi(X)$ 的相互决定关系。
:::

下一节我们主要讲如何在已知 K 而不知 $\varphi(X)$ 的情况下去求解 SVM 的优化问题。

### 3.4 原问题和对偶问题

本节从更广泛的角度出发，来解释最优化理论中的原问题和对偶问题的定义，为后续进一步推导 SVM 的对偶问题做准备。

<mark>原问题</mark>（Prime problem）的定义如下：

::: theorem Prime problem 的定义
<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716234114598.png" alt="image-20220716234114598" style="zoom:80%;" />
:::

可以看到这个定义很宽泛，自变量 $\omega$ 是一个**多维变量**，目标函数是 $f(\omega)$。我们假设限制条件中不等式有 K 个，分别用 $g_i(\omega) \le 0$ 来表示；同时假设限制条件中等式有 m 个，分别用 $h_i(\omega)=0$ 来表示。

然后我们定义该原问题的对偶问题。首先先定义一个函数 $L(\omega,\alpha,\beta)$ 如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716234821522.png" alt="image-20220716234821522" style="zoom:72%;" /></center>

在此基础上，<mark>对偶问题</mark>（Dual problem）可以定义如下：

::: theorem Dual problem 的定义
<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716235001835.png" alt="image-20220716235001835" style="zoom:72%;" />
:::

这里面的 $\inf L(\omega,\alpha,\beta)$ 指的是遍历所有定义域的 $\omega$，找到使 $L(\omega,\alpha,\beta)$ 最小的那个 $\omega$，同时把最小的 L 函数值赋值为 $\theta(\alpha, \beta)$。

综合 prime problem 和 dual problem 的定义可以得到：

::: theorem 定理 一
如果 $\omega^*$ 是 prime problem 的解，$(\alpha^*, \beta^*)$ 是 dual problem 的解，则有

$$f(\omega^*) \ge \theta(\alpha^*, \beta^*)$$

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716235641988.png" alt="image-20220716235641988" style="zoom:67%;" /></center>
:::

> 最后一步的推导中 $f(\omega^*) + \alpha^{*T} g(\omega^*) + \beta^{*T}h(\omega^*) \le f(\omega^*)$ 成立是因为：
>
> + 由于 $\omega^*$ 是 prime problem 的解，所以 $g(\omega^*) \le 0$，$h(\omega^*)=0$
> + 由于 $(\alpha^*, \beta^*)$ 是 dual problem 的解，所以 $\alpha(w^*) \ge 0$

定义<mark>对偶差距</mark>（**Duality Gap**）为 $f(\omega^*) - \theta(\alpha^*,\beta^*)$。根据上面定理我们知道了 $f(\omega^*) \ge \theta(\alpha^*, \beta^*)$，所以 $Duality \ Gap \ge 0$。

 还有如下的一个强对偶定理：

::: theorem 强对偶定理（Strong Duality Theorem）
如果 $g(\omega)=A\omega + b$，$h(\omega)=C\omega + d$，且 $f(\omega)$ 为凸函数，则有 $f(\omega^*) = \theta(\alpha^*, \beta^*)$，则<u>对偶差距为 0</u>。
:::

简单来说就是，**如果 prime problem 的目标函数是凸函数，限制条件是线性函数，那么 $f(\omega^*) = \theta(\alpha^*, \beta^*)$，在这种情况下 Duality Gap 为 0**。

> 定理的证明可参考 《CONVEX OPTIMIZATION》

接下来说，假如强对偶定理成立，也就是有 $f(\omega^*) = \theta(\alpha^*, \beta^*)$，再根据定理一推出的不等式，我们可以这么说：

若 $f(\omega^*) = \theta(\alpha^*, \beta^*)$，则从定理一的证明推理过程中的 $f(\omega^*) + \alpha^{*T} g(\omega^*) + \beta^{*T}h(\omega^*) \le f(\omega^*)$ 这一步可以看出，由于有 $h(\omega^*)=0$，那对于所有 $i=1 \sim K$，必然有 $\alpha^{*T}g(\omega^*) = 0$，也就是要么 $\alpha_i=0$，要么 $g_i(\omega^*)=0$。这个条件称为 <mark>KKT 条件</mark>。

> KKT 条件是由 **K**arush、**K**uhn 和 **T**ucker 先后独立发表出来的。

::: warning 本节小结

+ 定义了 prime problem 和 dual problem
+ 强对偶定理
+ KKT 条件

:::

接下来，我们将利用这一节的知识，将 SVM 的 prime problem 转化为 dual problem，从而进一步完成 SVM 优化问题的求解。

### 3.5 转化为对偶问题

本节介绍把 SVM 的 prime problem 转化为 dual problem，从而进一步完成 SVM 优化问题的求解。

#### 3.5.1 证明：SVM 的 prime problem 满足强对偶定理

回顾一下目前 SVM 的优化问题：

+ minimize：$\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i$ 或 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i^2$
+ 限制条件：
  1. $\delta_i \ge 0，i = 1 \sim N$
  2. $y_i[\omega^T \varphi(X_i)+b] \ge 1-\delta_i，i=1 \sim N$

对比 prime problem 的定义，我们对这个版本进行改造，使它的形式成为一个 prime problem。改造的部分就是将原来的 $\delta_i$ 变成原来的相反数，并对限制条件 2 做一下变形，从而 SVM 的优化问题就可以表达成：

+ minimize：$\frac{1}{2} ||\omega||^2 \color{red}{-} C\sum^N_{i=1}\delta_i$ 或 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i^2$
+ 限制条件：
  1. $\delta_i \color{red}{\le} 0，i = 1 \sim N$
  2. $\color{red}{1 + \delta_i - y_i\omega^T \varphi(X_i) - y_ib \le 0，i = 1 \sim N}$

可以看到上面这个版本中，目标函数是凸函数，两个限制条件都是线性的，因此满足强对偶定理。

#### 3.5.2 使用对偶理论求解对偶问题

容易混淆的是，在 3.4 节讲的对偶理论的 prime problem 定义中，自变量 $\omega$ 在 SVM 优化问题中相当于被拆成了 $(\omega, b, \delta_i)$；而限制条件中的 $g_i(\omega) \le 0$ 相当于被拆成了 $\delta_i \le 0（i = 1 \sim N）$ 和 $1 + \delta_i - y_i\omega^T \varphi(X_i) - y_ib \le 0（i = 1 \sim N）$ 两部分；由于 SVM 优化问题中不存在等式形式的限制条件，因此不存在 $h_i(\omega)$。

按照上一节对偶问题的定义，我们可以将这个 SVM 优化问题的 prime problem 转换成 dual problem，**先看转换的结论**，写出来的形式如下：

::: theorem 转换后的对偶问题
+ maximize：$\theta(\alpha, \beta)= \color{blue}{\inf \{ } \frac{1}{2}||\omega||^2-C\sum^N_{i=1}\delta_i + \sum^N_{i=1}{\beta_i \delta_i} + \sum^N_{i=1}{\alpha_i[1+\delta_i-y_i \omega^T \varphi(X_i) - y_i b]} \color{blue}{\}}$

+ 限制条件：
  1. $\alpha_i \ge 0$
  2. $\beta_i \ge 0$

$\inf \{ \}$ 里面的一串式子就是 $L(\omega,b,\delta_i)$。
:::

注意的是，由于上一节的 $g_i(\omega) \le 0$ 被拆成了实际的两部分限制条件，所以转换后 dual problem 中理论形式上 $g_i(\omega)$ 的系数 $\alpha_i$ 相当于被拆成了这里的两部分系数 $\alpha_i$ 和 $\beta_i$，而原来理论形式上的 $\beta_i$ 在这里是没有了。

**再来看一下是如何转化成这个对偶问题的**。

求 $\inf \{ L(\omega,b,\delta_i) \}$ 部分，就是要遍历所有的 $(\omega,b,\delta_i)$ 来求 $L(\omega,b,\delta_i)$ 的最小值。我们可以对 $(\omega,b,\delta_i)$ 求导并令导数等于 0，这样我们可以得到（*修正：下图中左边的 $\partial \theta$ 都应该改成 $\partial L$*）：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220717131050965.png" alt="image-20220717131050965" style="zoom:67%;" /></center>

然后将获得的三个式子代入到 $L(\omega, b, \delta_i)$ 的表达式中，可以将 SVM 的 prime problem 转换为下面的对偶问题：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220717133358038.png" alt="image-20220717133358038" style="zoom:67%;" /></center>

> 将三个式子代入到 L 从而化简成这样的推导规程可以参考浙大胡浩基老师的现场版教学视频。

在目标函数的式子中，$\varphi(X_i)^T \varphi(X_j)$ 可以写成 Kernel Function $K(X_i,X_j)$，这样就不需要知道 $\varphi(X)$ 的具体形式了。

可以看到这个对偶问题也是一个二次规划问题，可以通过最优化理论的 SMO 算法非常方便地求解出 $\alpha$，注意所要最大化的目标函数 $\theta(\alpha, \beta)$ 中其实已经没有 $\beta$ 了，因此也可以写成 $\theta(\alpha)$。

我们已经将 SVM 问题化成了对偶问题，但我们还没有完全结束。通过转化我们求出了 $\alpha$，但我们的本质是想求最开始说的 $\omega^T \varphi(X) + b$ 中的 $\omega$ 和 $b$，那怎么把求 $\alpha$ 转化到求 $(\omega, b)$ 呢？

#### 3.5.3 把求 $\alpha$ 转化为求 $(\omega,b)$

其实这个事情很简单。在令 $\frac{\partial L}{\partial \omega} = 0$ 而得出了 $\omega = \sum^N_{i=1}{\alpha_i y_i \varphi(X_i)}$ 这个结论，但只通过这个公式，我们必须先知道 $\varphi$ 才能知道 $\omega$。而 **Vapnik 提出的算法的妙处在于，我们不见得需要 $\omega$**。

为什么呢？在 SVM 测试流程中，当来了一个样本 X，我们怎样知道它是属于哪个类呢？我们是：

+ 若 $\omega^T \varphi(X) + b \ge 0$，则 y = +1
+ 若 $\omega^T \varphi(X) + b \lt 0$，则 y = -1

在这里，我们可以不知道 $\omega$，我们只需要能够知道 $\omega^T \varphi(X) + b$ 这一个整体的值就可以了。我们把刚刚说的结论 $\omega = \sum^N_{i=1}{\alpha_i y_i \varphi(X_i)}$ 代入进去，可以得到：

$$\omega^T \varphi(X) = \sum^N_{i=1}{\alpha_i y_i \color{blue}{\varphi(X_i)^T \varphi(X)}} = \sum^N_{i=1}{\alpha_i y_i \color{blue}{K(X_i,X)}}$$

所以可以借助 Kernel Function 直接算出 $\omega^T \varphi(X)$ 这一部分。现在只剩下求 $b$ 了。

求 b 需要用到 KKT 条件：$\forall i = 1 \sim K, \alpha_i^*=0  \ \color{blue}{or} \ g_i^*(\omega^*)=0$。如果我们把它翻译成 SVM 的情况，就可以写成：

::: theorem KKT 条件应用到 SVM 中
$\forall i = 1 \sim N$，

1. 要么 $\beta_i = 0$，要么 $\delta_i=0$
2. 要么 $\alpha_i=0$，要么 $1+\delta_i-y_i \omega^T \varphi(X_i) - y_ib = 0$
:::

怎么求 b 呢？我们首先要取一个 $\alpha_i（0 \lt \alpha_i \lt C）$， 然后可以推出 $\beta_i = C - \alpha_i \gt 0$，此时 $\beta_i \neq 0$ 且 $\alpha_i \neq 0$，所以根据上面 KKT 条件我们可以得知只能是 $\delta_i=0$ 和 $1+\delta_i-y_i \omega^T \varphi(X_i) - y_ib = 0$，这俩式子一联立就可以求出 $b$ 了：

$$b = \frac{1-y_i \omega^T \varphi(X_i)}{y_i} = \frac{1-y_i \sum^N_{i=1} \alpha_i y_i K(X_i,X_j)}{y_i}$$

> 因为此时通过转化为对偶问题，已经可以将 $\alpha$ 求出来了，所以这里可以取它的一个分量 $\alpha_i$。当然也可以把所有 $\alpha_i$ 的可能取值都取一遍，各算出一个 b，然后计算这些这些 b 的平均值作为最终 $b$ 的值，这样也许会更准确。

有了上面的推导，我们就可以最终在得到输入样本 X 后，计算 $\omega^T \varphi(X) + b$ 并与 0 做比较从而判别出该样本属于哪一类。

## 4. SVM 的算法总体流程

经过上面的介绍，我们得出了如下的一个结论：

::: theorem 核函数戏法（Kernel Trick）
$$\omega^T \varphi(X) + b = \sum^N_{i=1}{\alpha_i y_i K(X_i, X)} + b$$
:::

有了 Kernel Trick 这个结论，我们就可以不需要知道 $\varphi(X)$，而仅需知道 Kernel Function $K(X_1, X_2)$ 就可以了。

最终我们可以得到如下的 SVM 判别标准：

::: theorem SVM 判别标准
+ 如果 $\sum^N_{i=1}{\alpha_i y_i K(X_i, X)} + b \ge 0$，那么 $X \in C_1$
+ 如果 $\sum^N_{i=1}{\alpha_i y_i K(X_i, X)} + b \lt 0$，那么 $X \in C_2$
:::

基于对偶问题的求解，我们可以得出支持向量机的训练和测试的流程：

::: note SVM 训练和测试的流程

**训练过程**：

输入 training data $\{ (X_i,y_i)\}，i = 1 \sim N$，其中 $y_i = \pm 1$，然后解下面这个优化问题从而求出 $\alpha$：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220717133358038.png" alt="image-20220717133358038" style="zoom:67%;" /></center>

然后再求 b：

$$b = \frac{1-y_i \sum^N_{i=1} \alpha_i y_i K(X_i,X_j)}{y_i}$$

**测试过程**：

一旦知道了 $\alpha = [\alpha_1, \dots]$ 和 $b$ 后，就可以按照 SVM 的判别标准来预测 X 的类别。

+ 如果 $\sum^N_{i=1}{\alpha_i y_i K(X_i, X)} + b \ge 0$，那么 $X \in C_1$
+ 如果 $\sum^N_{i=1}{\alpha_i y_i K(X_i, X)} + b \lt 0$，那么 $X \in C_2$

在这个过程中，完全没有用到低维到高维的映射 $\varphi$，而只是用到了 Kernel Function $K(X_1,X_2)$。

:::

上面讲的是让 SVM 优化问题的目标函数是 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i$ 情况下的训练和测试流程，课下可以自己试一下推导目标函数是 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i^2$ 情况下的 SVM 训练和测试流程。