---
title: 支持向量机（SVM）
date: 2022-06-26 22:18:58
permalink: /pages/ml/hhj/svm
categories:
  - AI
  - 机器学习
  - 浙大胡浩基-机器学习
tags:
  - 
---

## 1. 线性可分定义

在二维中，线性可分表示为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104445520.png" alt="image-20220707104445520" style="zoom: 67%;" /></center>

+ 存在一条直线将 x 和 ○ 分开

线性不可分则表示为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104819974.png" alt="image-20220707104819974" style="zoom:67%;" /></center>

+ 此时不存在一条直线将 x 与 ○ 分开

如果扩展到三维，则是用平面来分割：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104935191.png" alt="image-20220707104935191" style="zoom: 67%;" /></center>

如果维度大于等于四维，那么就是用**超平面**（Hyperplane）来分割。

我们借助数学对 Linear Separable 和 Nonlinear Separable 进行定义，以二维为例，直线就可以表示为一个方程：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707105224545.png" alt="image-20220707105224545" style="zoom:75%;" /></center>

+ 在直线的其中一边，$\omega_1 x_1 + \omega_2 x_2 + b \gt 0$，另一边则相反。
+ 可不可以让右边是小于 0 呢？可以呀，只需要假设 $\omega'=-\omega, b'=-b$，就可以是让右边小于 0，左边大于 0 了。所以我们只需要分析我们上图所示的情况即可。

接下来我们**用数学严格定义**训练样本以及他们的标签：假设我们有 N 个训练样本和他们的标签 $\{ (X_1, y_1),(X_2,y_2),\dots,(X_N, y_N) \}$，其中 $X_i=[x_{i1}, x_{i2}]^T$，$y_i = \{+1,-1 \}$，并令 $X_i$ 属于 $C_1$ 时 $y_i=+1$，反之则为 $-1$。

线性可分的严格定义：一个训练样本及 $\{(X_i, y_i),\dots,(X_N, y_N) \}$，在 $i=1 \sim N$  线性可分，是指存在 $(\omega_1, \omega_2, b)$ 使得对 $i=1 \sim N$，有：

+ 若 $y_i=+1$，则 $\omega_1 x_1 + \omega_2 x_2 + b \gt 0$
+ 若 $y_i=-1$，则 $\omega_1 x_1 + \omega_2 x_2 + b \lt 0$

若将 $\omega$ 表示成一个 vector $\omega = [\omega_1, \omega_2]^T$，那么上面的定义可以写成：

+ 若 $y_i=+1$，则 $\omega^T X_i + b \gt 0$
+ 若 $y_i=-1$，则 $\omega^T X_i + b \lt 0$

## 2. 线性可分时的最优分类 hyperplane

### 2.1 什么是最优分类 hyperplane？

支持向量机算法分成了两个步骤：

1. 解决线性可分问题
2. 再将线性可分问题中获得的结论推广到线性不可分情况

我们先看一下他是如何解决线性可分问题的。

> 如果一个数据集是线性可分的，那么就存在无数多个 hyperplane 将各个类别分开。

既然有上面的结论，那哪一个是最好的呢？比如一个二分类的问题：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716193434391.png" alt="image-20220716193434391" style="zoom:80%;" /></center>

问哪一个最好呢？大多数都是说 2 号线。其实原因在于：2 号线更能低于训练样本位置的误差。那这个 2 号线怎么画出来的呢？Vapnik 给出了基于最优化理论的回答。

将这个直线向左向右移动，直到能碰上一个或几个训练样本为止：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716193736561.png" alt="image-20220716193736561" style="zoom:80%;" />

+ 我们将移动后的平行线所擦到的训练样本为 <mark>support vector</mark>。
+ 两条平行线之间的距离叫做 <mark>margin</mark>。

这样我们所要找的线就是使 margin 最大的线，为了是直线唯一，又规定这条线是擦到 support vector 的两个线的正中间的那条线，也就是上图中的 2 号线了。

经过上面讨论，我们知道了 SVM 寻找的最优分类直线应该满足：

1. 该直线分开了两个类；
2. 该直线最大化 margin；
3. 该直线处于间隔的中间，到所有 support vector 的距离相等。

上面的结论是基于二维特征空间的结果，扩展到更高维度后，直线将变成 hyperplane，此时被称为**最优分类 hyperplane**，但结论不会变。

### 2.2 寻找最优分类 hyperplane

下面我们讲如何用严格的数学来把寻找这个最优分类 hyperplane 的过程写成一个最优化问题。

假定训练样本集是线性可分的，那么 SVM 需要寻找的是最大化 margin 的 hyperplane，这个优化问题可以写成下面的形式：

+ minimize：$\frac{1}{2} ||\boldsymbol{\omega}||^2$
+ 限制条件：$y_i(\omega^Tx_i+b) \ge 1, (i=1 \sim N)$

下面讨论为什么会这样？我们先看两个个事实：

::: tip 事实 1
$\omega^T x + b = 0$ 与 $(\alpha \omega^T)x+(ab) = 0$ 是同一个超平面。（$\alpha \neq 0$）
:::

::: tip 事实 2
一个点 $X_0$ 到超平面 $\omega^T x + b = 0$ 的距离为：
$$d=\frac{|\omega^T x_0 + b|}{||\omega||}$$
:::

借助于这两个事实，我们可以用 a 去缩放 $\omega$ 和  $b$，即 $(\omega,b) \to (a\omega,ab)$，最终使得缩放后的 $\omega$ 和  $b$ 能够在 support vector $x_0$ 上有 $|\omega^Tx_0 + b|=1$，而在 non support vector 上，有 $|\omega^Tx_0 + b| \gt 1$。

为什么可以这样推导呢？根据事实 1 可知，用 a 缩放后的仍是同一个超平面，而根据事实 2，support vector $X_0$ 到超平面的距离将变为：

$$d=\frac{|\omega^T x_0 + b|}{||\omega||} = \frac{1}{||\omega||}$$

有上面的式子可以看出来，**最大化 support vector 到 hyperplane 的距离等价于最小化 $||\omega||$**。因此优化问题定为 minimize $\frac{1}{2} ||\boldsymbol{\omega}||^2$ 和 minimize $||\omega||$ 是一样的。所以优化问题可以写成这样。

我们再来看限制条件。

support vector 到 hyperplane 的距离是 $\frac{1}{2}||\epsilon||$，在 non support vector 上 $|\omega^T x_0 + b| \gt 1$。综合两者，我们可以写出 SVM 的限制条件：

$$y_i(\omega^T x_i + b) \ge \color{red}{1}，i=1 \sim N$$

+ 其中 $y_i$ 的作用是协调超平面的左右
+ 右边的 1（标红）可以改成任意的正数，因为修改后计算出来的 $\omega$ 和 $b$ 也只比原来差了 a 倍，而根据事实 1，他们代表的是同一个 plane。

总结一下：

::: warning 总结
线性可分情况下，SVM 寻找最佳 hyperplane 的优化问题可以表示为：

+ minimize：$\frac{1}{2} ||\boldsymbol{\omega}||^2$
+ 限制条件：$y_i(\omega^Tx_i+b) \ge 1, (i=1 \sim N)$

其中 $(X_i, y_i)$ 是已知的，$(\omega,b)$ 是待求的。
:::

可以看到，**我们所表述的这个问题是凸优化（Convex Optimization）问题中的二次规划问题**。

::: tip 二次规划的定义
1. 目标函数（object function）是二次项
2. 限制条件是一次项
:::

而 Convex Optimization 问题中，只有唯一一个全局极值，我们可以利用 gradient descent 方法来求出这个全局极值，从而解决这个 Convex Optimization 问题。

> 我们这里不详细探讨如何解 Convex Optimization 问题，而是将这个寻找 hyperplane 的问题转换为一个 Convex Optimization，并直接利用解这类问题的工具包来解出这个问题。