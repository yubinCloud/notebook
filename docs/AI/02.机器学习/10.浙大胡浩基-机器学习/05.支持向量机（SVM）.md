---
title: 支持向量机（SVM）
date: 2022-06-26 22:18:58
permalink: /pages/ml/hhj/svm
categories:
  - AI
  - 机器学习
  - 浙大胡浩基-机器学习
tags:
  - 
---

## 1. 线性可分定义

在二维中，线性可分表示为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104445520.png" alt="image-20220707104445520" style="zoom: 67%;" /></center>

+ 存在一条直线将 x 和 ○ 分开

线性不可分则表示为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104819974.png" alt="image-20220707104819974" style="zoom:67%;" /></center>

+ 此时不存在一条直线将 x 与 ○ 分开

如果扩展到三维，则是用平面来分割：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707104935191.png" alt="image-20220707104935191" style="zoom: 67%;" /></center>

如果维度大于等于四维，那么就是用**超平面**（Hyperplane）来分割。

我们借助数学对 Linear Separable 和 Nonlinear Separable 进行定义，以二维为例，直线就可以表示为一个方程：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220707105224545.png" alt="image-20220707105224545" style="zoom:75%;" /></center>

+ 在直线的其中一边，$\omega_1 x_1 + \omega_2 x_2 + b \gt 0$，另一边则相反。
+ 可不可以让右边是小于 0 呢？可以呀，只需要假设 $\omega'=-\omega, b'=-b$，就可以是让右边小于 0，左边大于 0 了。所以我们只需要分析我们上图所示的情况即可。

接下来我们**用数学严格定义**训练样本以及他们的标签：假设我们有 N 个训练样本和他们的标签 $\{ (X_1, y_1),(X_2,y_2),\dots,(X_N, y_N) \}$，其中 $X_i=[x_{i1}, x_{i2}]^T$，$y_i = \{+1,-1 \}$，并令 $X_i$ 属于 $C_1$ 时 $y_i=+1$，反之则为 $-1$。

线性可分的严格定义：一个训练样本及 $\{(X_i, y_i),\dots,(X_N, y_N) \}$，在 $i=1 \sim N$  线性可分，是指存在 $(\omega_1, \omega_2, b)$ 使得对 $i=1 \sim N$，有：

+ 若 $y_i=+1$，则 $\omega_1 x_1 + \omega_2 x_2 + b \gt 0$
+ 若 $y_i=-1$，则 $\omega_1 x_1 + \omega_2 x_2 + b \lt 0$

若将 $\omega$ 表示成一个 vector $\omega = [\omega_1, \omega_2]^T$，那么上面的定义可以写成：

+ 若 $y_i=+1$，则 $\omega^T X_i + b \gt 0$
+ 若 $y_i=-1$，则 $\omega^T X_i + b \lt 0$

## 2. 线性可分时的最优分类 hyperplane

### 2.1 什么是最优分类 hyperplane？

支持向量机算法分成了两个步骤：

1. 解决线性可分问题
2. 再将线性可分问题中获得的结论推广到线性不可分情况

我们先看一下他是如何解决线性可分问题的。

> 如果一个数据集是线性可分的，那么就存在无数多个 hyperplane 将各个类别分开。

既然有上面的结论，那哪一个是最好的呢？比如一个二分类的问题：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716193434391.png" alt="image-20220716193434391" style="zoom:80%;" /></center>

问哪一个最好呢？大多数都是说 2 号线。其实原因在于：2 号线更能低于训练样本位置的误差。那这个 2 号线怎么画出来的呢？Vapnik 给出了基于最优化理论的回答。

将这个直线向左向右移动，直到能碰上一个或几个训练样本为止：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716193736561.png" alt="image-20220716193736561" style="zoom:80%;" />

+ 我们将移动后的平行线所擦到的训练样本为 <mark>support vector</mark>。
+ 两条平行线之间的距离叫做 <mark>margin</mark>。

这样我们所要找的线就是使 margin 最大的线，为了是直线唯一，又规定这条线是擦到 support vector 的两个线的正中间的那条线，也就是上图中的 2 号线了。

经过上面讨论，我们知道了 SVM 寻找的最优分类直线应该满足：

1. 该直线分开了两个类；
2. 该直线最大化 margin；
3. 该直线处于间隔的中间，到所有 support vector 的距离相等。

上面的结论是基于二维特征空间的结果，扩展到更高维度后，直线将变成 hyperplane，此时被称为**最优分类 hyperplane**，但结论不会变。

### 2.2 寻找最优分类 hyperplane

下面我们讲如何用严格的数学来把寻找这个最优分类 hyperplane 的过程写成一个最优化问题。

假定训练样本集是线性可分的，那么 SVM 需要寻找的是最大化 margin 的 hyperplane，这个优化问题可以写成下面的形式：

+ minimize：$\frac{1}{2} ||\boldsymbol{\omega}||^2$
+ 限制条件：$y_i(\omega^Tx_i+b) \ge 1, (i=1 \sim N)$

下面讨论为什么会这样？我们先看两个个事实：

::: note 事实 1
$\omega^T x + b = 0$ 与 $(\alpha \omega^T)x+(ab) = 0$ 是同一个超平面。（$\alpha \neq 0$）
:::

::: note 事实 2
一个点 $X_0$ 到超平面 $\omega^T x + b = 0$ 的距离为：
$$d=\frac{|\omega^T x_0 + b|}{||\omega||}$$
:::

借助于这两个事实，我们可以用 a 去缩放 $\omega$ 和  $b$，即 $(\omega,b) \to (a\omega,ab)$，最终使得缩放后的 $\omega$ 和  $b$ 能够在 support vector $x_0$ 上有 $|\omega^Tx_0 + b|=1$，而在 non support vector 上，有 $|\omega^Tx_0 + b| \gt 1$。

为什么可以这样推导呢？根据事实 1 可知，用 a 缩放后的仍是同一个超平面，而根据事实 2，support vector $X_0$ 到超平面的距离将变为：

$$d=\frac{|\omega^T x_0 + b|}{||\omega||} = \frac{1}{||\omega||}$$

有上面的式子可以看出来，**最大化 support vector 到 hyperplane 的距离等价于最小化 $||\omega||$**。因此优化问题定为 minimize $\frac{1}{2} ||\boldsymbol{\omega}||^2$ 和 minimize $||\omega||$ 是一样的。所以优化问题可以写成这样。

我们再来看限制条件。

support vector 到 hyperplane 的距离是 $\frac{1}{2}||\epsilon||$，在 non support vector 上 $|\omega^T x_0 + b| \gt 1$。综合两者，我们可以写出 SVM 的限制条件：

$$y_i(\omega^T x_i + b) \ge \color{red}{1}，i=1 \sim N$$

+ 其中 $y_i$ 的作用是协调超平面的左右
+ 右边的 1（标红）可以改成任意的正数，因为修改后计算出来的 $\omega$ 和 $b$ 也只比原来差了 a 倍，而根据事实 1，他们代表的是同一个 plane。

总结一下：

::: warning 总结
线性可分情况下，SVM 寻找最佳 hyperplane 的优化问题可以表示为：

+ minimize：$\frac{1}{2} ||\boldsymbol{\omega}||^2$
+ 限制条件：$y_i(\omega^Tx_i+b) \ge 1, (i=1 \sim N)$

其中 $(X_i, y_i)$ 是已知的，$(\omega,b)$ 是待求的。
:::

可以看到，**我们所表述的这个问题是凸优化（Convex Optimization）问题中的二次规划问题**。

::: note 二次规划的定义
1. 目标函数（object function）是二次项
2. 限制条件是一次项
:::

而 Convex Optimization 问题中，只有唯一一个全局极值，我们可以利用 gradient descent 方法来求出这个全局极值，从而解决这个 Convex Optimization 问题。

> 我们这里不详细探讨如何解 Convex Optimization 问题，而是将这个寻找 hyperplane 的问题转换为一个 Convex Optimization，并直接利用解这类问题的工具包来解出这个问题。

## 3. 线性不可分情况

### 3.1 线性模型的局限

在线性不可分的情况下，不存在 $\omega$ 和 $b$ 满足上面所有 N 个限制条件作为最优化的解，所以需要适当放松限制条件。

放松限制条件的基本思路是，对每个训练样本及标签 $(X_i, Y_i)$，我们设置一个<mark>松弛变量</mark>（**slack variable**） $\delta_i$，并将 N 个限制条件改写为：

$$y_i(\omega^T X_i+b) \ge 1-\delta_i，i=1 \sim N$$

可以看到，只要每个 slack var $\delta_i$ 取得足够大，这些不等式的限制条件一定是可以满足的，所以我们还需要加入限制条件，以防止 slack var 无限地大。

改造后的 SVM 优化版本：

::: theorem 改造后的 SVM 的优化问题

+ minimize：$\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i$ 或 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i^2$
+ 限制条件：
  1. $\delta_i \ge 0，i = 1 \sim N$
  2. $y_i[\omega^T X_i+b] \ge 1-\delta_i，i=1 \sim N$
   :::

> 以前的 objective function 只需要最小化 $\frac{1}{2}||\omega||^2$，而现在加了一个正则化项，这样是想让所有 slack var 的和越小越好，比例因子 C 起到了平衡两项的作用。

这里用于平衡两项的**比例因子 C 是人为设定的**，因此它是一个 hyper parameter，这个 hyper param 需要去调的。尽管如此，SVM 也是 hyper param 较少的一个算法了，因此不需要花很多时间炼丹。

到了这里，其实上面的解线性不可分的情况有时是不行的，比如下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716210149264.png" alt="image-20220716210149264" style="zoom:67%;" />

这基本和瞎猜没什么区别，问题就出在：我们假设分开两类的函数是线性的。但线性模型的表现力是不够的，面对上面这个数据集总是无法找到一个直线可以分开两类，因此**我们需要扩大可选的函数范围从而应对更加复杂的线性不可分的情况**。

### 3.2 从低维到高维的映射

为了达成扩大可选函数范围的目标，像神经网络或决策树等都是直接产生更多可选函数，比如神经网络是通过多层非线性函数的组合，而 SVM 做法的思想是：**将特征空间从低维映射到高维，然后用线性 hyperplane 对数据进行分类**。

举个例子，考察下面如图的异或问题，这两类样本是在二维空间中是线性不可分的：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716220302773.png" alt="image-20220716220302773" style="zoom: 50%;" />

而如果构造一个从二维到五维的映射 $\varphi(X)$，在五维的特征空间中，这个例子将有可能变成线性可分：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716220705227.png" alt="image-20220716220705227" style="zoom: 50%;" />

这样映射到五维空间后，$\varphi(x_1), \dots,\varphi(x_4)$ 变得线性可分了，因为：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716221145907.png" alt="image-20220716221145907" style="zoom: 50%;" />

这样就是 $X_{1}、X_{2}$ 是一类，另外两个是一类。

可以看到，通过人为指定一个二维到五维的映射 $\varphi(X)$ 后，线性不可分的数据集变成了线性可分的数据集。这件事情说明了一个**更一般的结论**：

::: theorem
假设在一个 **M 维空间**上随机取 N 个训练样本，同时随机地对每个训练样本赋予标签 +1 或 -1，同时假设这些训练样本线性可分的概率为 P(M)，则有当 M 趋于无穷大时，P(M) = 1。
:::

> 这个结论我们不再证明了，但可以直观理解一下。当 feature space 的 维度 $M$ 增加后，待估计参数 $(\omega, b)$ 的维度也会增加，也就是说整个算法模型的自由度也会增加，当然也就更有可能分开低维时候无法分开的数据集。

这个定理告诉我们，**将训练样本从低维映射到高维后，会增大线性可分的概率**。我们先放下对 $\varphi(X)$ 具体形式的探讨，并假设其已经确定下来，来看看 SVM 的优化问题将做出什么样的改变：

::: theorem 改造后的 SVM 的优化问题
+ minimize：$\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i$ 或 $\frac{1}{2} ||\omega||^2 + C\sum^N_{i=1}\delta_i^2$
+ 限制条件：
  1. $\delta_i \ge 0，i = 1 \sim N$
  2. $y_i[\omega^T \color{red}{\varphi(X_i)}+b] \ge 1-\delta_i，i=1 \sim N$
:::

+ 主要改变是 $X_i$ 被 $\varphi(X_i)$ 替换；
+ 之前是 $\omega$ 维度与 $X_i$ 维度相同，而**现在与 $\varphi(X_i)$ 维度相同**。

可以看到，**高维情况下优化问题的解法和低维情况是完全类似的**。

现在还剩下一个问题：在 SVM 中，低维到高维的映射 $\varphi(X)$ 取什么样的形式呢？

### 3.3 Kernel Function 的定义

本节具体研究 $\varphi(X)$ 的形式，并引入 Kernel Function 的定义。

SVM 的创始人 Vapnik 对回答 $\varphi(X)$ 的具体形式这一问题是非常有创意的。他指出，<u>我们可以不用知道 $\varphi(X)$ 的具体形式，取而代之，如果对于两个 vector $X_1、X_2$，我们知道 $K(X_1,X_2)=\varphi(X_1)^T\varphi(X_2)$，那么我们仍然能够通过一些技巧获得一个测试样本 $X$ 的类别信息，从而完成测试样本类别的预测</u>。

我们定义 $K(X_1,X_2)$ 为 <mark>Kernel Function</mark>，它的值是一个实数，这从它的等式右边就可以看出来。

我们举两个例子来说明 Kernel Function 与低维到高维的映射 $\varphi(X)$ 之间的相互关系。

::: note Example：已知映射求 Kernel Function
:watermelon: 首先举一个已知映射 $\varphi$ 求 Kernel Function $K$ 的例子：

假设 $\varphi(X)$ 是一个将二维向量映射为三维向量的映射，例如：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716225615547.png" alt="image-20220716225615547" style="zoom:50%;" />

我们看一下与这个 $\varphi$ 相对应的 Kernel Function 的形式。假设有两个二维向量：$X_1=[x_{11}, x_{12}]^T, X_2=[x_{21}, x_{22}]^T$，此时有：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716225837642.png" alt="image-20220716225837642" style="zoom:60%;" />
:::

::: note Example：已知 Kernel Function 求映射
:watermelon: 然后举一个已知 Kernel Function 求映射 $\varphi$ 的例子：

假设 X 是一个 2 dim vector，$X_1=[x_{11}, x_{12}]^T, X_2=[x_{21}, x_{22}]^T$，然后假设：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716231016401.png" alt="image-20220716231016401" style="zoom:60%;" />

根据定义 $K(X_1,X_2)=\varphi(X_1)^T\varphi(X_2)$，那么 $\varphi(X)$ 的形式就是：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716231103538.png" alt="image-20220716231103538" style="zoom:60%;" />

值得一提的是，如果 $\varphi(X)$ 是这种形式，那 $K(X_1,X_2)$ 也就一定是上面那种形式，这可以看出两者具有一一对应的关系。

:::

从上面例子中可以看出，**Kernel Function $K$ 与映射 $\varphi$ 是一一对应的关系**。还要指出的是，**Kernel Function 的形式不能随意的取**，它必须要满足一定的条件才能分解为两个 $\varphi$ 内积的形式。

有大佬提出了如下定理：

::: theorem Mercer’s Theorem
$K(X_1,X_2)$ 能写成 $\varphi(X_1)^T\varphi(X_2)$ 的充要条件是：

1. $K(X_1,X_2) = K(X_2,X_1)$  <font color=blue>【交换性】</font>
2. $\forall C_i(i=1 \sim N)$, $\forall N$ 有 $\sum^N_{i=1} \sum^N_{j=1} C_i C_j K(X_i X_j) \ge 0$    <font color=blue>【半正定性】</font>
:::

**只要 K 满足交换性和半正定性，那么它就能写成 $\varphi$ 内积的形式**。比如：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220716233130866.png" alt="image-20220716233130866" style="zoom:60%;" />

但在这个例子中却不能显式写出 $\varphi(X)$ 的具体表达式。

 虽然我们无法知道 $\varphi(X)$ 的具体形式，但是我们却可以通过一些方法知道 $\omega^T \varphi(X) + b$ 的值，进而可以知道一个测试样本 X 所属的类别。

::: warning 本节总结
在这一节中，我们定义了 Kernel Function $K(X_1,X_2)$，同时指出了它和低维到高维的映射 $\varphi(X)$ 的相互决定关系。
:::

下一节我们主要讲如何在已知 K 而不知 $\varphi(X)$ 的情况下去求解 SVM 的优化问题。

### 3.4 原问题和对偶问题

