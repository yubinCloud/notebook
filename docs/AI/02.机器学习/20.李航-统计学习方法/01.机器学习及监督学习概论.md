---
title: 机器学习及监督学习概论
date: 2022-07-27 10:51:49
permalink: /pages/ml/lh/supervised-learning-overview/
categories:
  - AI
  - 机器学习
  - 李航-统计学习方法
tags:
  - 
---

## 1. 统计学习方法的定义与分类

### 1.1 统计学习的概念

统计学习已经应用到了生活中的众多领域，比如人工智能、模式识别、数据挖掘、自然语言处理、语音处理、计算视觉、信息检索、生物信息等。

<mark>统计学习</mark>（**Statistical Machine Learning**）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。它所研究对象就是数据，假设同类的数据具有一定的统计规律，那么就可以利用概率统计方法对其处理，从而达到一个对数据分析和预测的目的。

-  以计算机和网络为<u>平台</u> 
-  以数据为<u>研究对象</u> 
-  以预测和分析数据为<u>目的</u> 
-  以方法为<u>中心</u> 
-  是多领域交叉的<u>学科</u>

简而言之，统计学习实现了一个从已知到未知的过程，利用已知数据和各种学科理论来对未知的新数据进行一个预测和分析。

### 1.2 统计学习的步骤

::: theorem Procedures
1. 得到一个有限的训练数据集合
2. 确定学习模型的集合【模型】
3. 确定模型选择的准则【策略】
4. 实现求解最优模型的算法【算法】
5. 通过学习方法选择最优模型
6. 利用学习的最优模型对新数据进行预测或分析
:::

第一步是要得到一个有限的训练数据集合，也就是用来训练模型的。接下来，确定学习模型的集合，这个集合称之为**假设空间**。然后，选择模型，而选择模型需要一定的评价准则，这就是第三步中确定模型选择的准则，我们称之为**策略**。第四步是实现求解最优模型的算法，也就是根据第三步的策略，通过算法实现模型的选择。最后，通过学习方法也就是第 2-4 步，选择出一个最优模型，再将用以预测的数据代入到最优模型中，进行一个预测和分析。

这里注意一下，第二步中的模型，第三步里的策略，还有第四步的算法，是统计学习的三要素，这三个要素一起构成了学习系统：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727110435742.png" alt="image-20220727110435742" style="zoom:67%;" /></center>

如上图所示，首先给定一个训练集，这里面假设包含N个样本，然后放入到学习系统里面，学习系统就包含了模型、策略和算法，然后通过学习系统对于训练集中信息的不断学习，得到了一个最优模型，也就是对应了之前第五步。最后，输入一个新的实例，代入到最优模型中，通过预测系统得到了一个新的输出，也就是对于新数据进行的预测和分析。这就是统计学习方法的一个大概步骤。

### 1.3 统计学习方法的分类

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/640" alt="图片" style="zoom:67%;" /></center>

从模型角度来进行分类，可以分为概率模型与非概率模型，线性模型与非线性模型，参数化模型与非参数化模型。

+ 关于概率模型与非概率模型，概率模型就是用条件概率分布的形式表达的模型 $f(Y|X)$ ，而非概率模型，则是用函数形式表达的 $y=f(x)$。常见的决策树、朴素贝叶斯都属于概率模型，而感知机、支持向量机、神经网络，这些就属于非概率模型了。
+ 关于线性模型与非线性模型，如果模型函数是线性的，那么就是线性模型，反之，是非线性模型。
+ 参数化模型，就是说模型的参数维度是固定的，可以由有限为的参数来刻画。那么非参数化模型就对应着参数维度不固定，所以参数化模型它更适用于简单问题，而非参数化模型，更适用于比较复杂的现实问题。
+ 从算法的角度来分类，分为在线学习和批量学习。在线学习也就是大家熟知的Online Learning。每次接受一个样本，然后预测学习模型，之后不断重复这个步骤。批量学习就是Batch Learning，一次接受所有的数据，然后学习模型进行预测。
+ 按技巧分类，按技巧它分为贝叶斯学习和核方法，贝叶斯学习就是基于贝叶斯定理的一个学习方法，而核方法则是基于核函数的。

## 2. 统计学习方法的基本分类

监督学习、无监督学习、半监督学习，是根据学习的数据中数据所包含的标记信息来区分的。

- 所学习的数据具有标注信息——监督学习；
- 所学习的数据不具有标注信息——无监督学习；
- 所学习的数据，只含有少量标注信息，大多数没有——半监督学习；

这一篇中，我们主要讲解一下监督学习和无监督学习，强化学习这一部分，带着大家简单了解一下。

### 2.1 监督学习

<mark>监督学习</mark>（**Supervised Learning**）是指从标注数据中学习预测模型的机器学习问题，其本质是学习输入到输出的映射的统计规律。

几个概念：

+ **输入空间**（Input Space）：输入的所有可能取值的集合
+ **实例**（Instance）：每一个具体的输入，通常由特征向量（Feature Vector）表示
+ **特征空间**（Feature Space）：所有特征向量存在的空间
+ **输出空间**（Output Space）：输出的所有可能取值的集合

大多时候，输入空间和特征空间是相同的。既然说到大多时候，那就肯定存在不同的时候了，比如支持向量机中，核技巧的基本思想，就是通过一个非线性变换，将输入空间对应到特征空间上。

根据变量的不同类型，要解决的问题可以分为回归问题、分类问题和标注问题：

- 输入变量与输出变量均为连续变量的预测问题——**回归问题**
- 输出变量为有限个离散变量的预测问题——**分类问题**
- 输入变量与输出变量均为变量序列的预测问题—— **标注问题**

以下是相应的**符号表示**：

+ 输入变量 $X$；输入变量的取值 $x$
+ 输出变量 $Y$；输出变量的取值 $y$
+ 输入实例 x 的 feature vector 表示：$x = (x^{(1)},x^{(2)},\dots,x^{(n)})^T$
+ 以 $x_i$ 表示多个输入变量中的第 i 个变量：$x_i = (x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T$
+ 样本容量为 N 的训练集：$T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}$

对于监督学习，主要就是研究输入到输出之间的统计规律，所以要有关于输入和输出的基本假设：

+ 监督学习的基本假设：X 和 Y 具有联合概率分布 $P(X,Y)$
+ 监督学习的目的：学习一个输入到输出的映射，这一映射以模型表示
+ 模型的形式：条件概率分布 $P(Y|X)$ 或决策函数 $Y=f(X)$
+ 假设空间（Hypothesis Space）：所有这些可能模型的集合

**映射关系以模型来表示的话，主要有两种，一种是条件概率分布的形式，一种是决策函数的形式**。条件概率分布的形式其实就是概率模型，而决策函数的形式就是非概率模型。对具体的输入进行相应的输出预测时，表达为：

$$P(y|x) \ 或 \ y=f(x)$$

流程图如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727142802263.png" alt="image-20220727142802263" style="zoom:75%;" /></center>

通过学习系统学习到的模型用加一个帽子表示，即 $\hat f$ 或 $\hat P$。然后，给一个新的实例，这个输出也可以表达成决策函数或者条件概率分布预测值的形式，之所以通过 argmax，选择使条件概率分布最大的y值，就是因为**我们预测的思想就是想找到出现的可能性最大的值**。

### 2.2 无监督学习

<mark>无监督学习</mark>（**Unsupervised Learning**）是指从无标注数据中学习预测模型的机器学习问题，其本质是学习数据中统计规律或潜在结构。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727143255267.png" alt="image-20220727143255267" style="zoom:75%;" /></center>

对于无监督学习，无标注数据，自然直接得到的数据只有输入，输出是隐含的潜在内容。

几个概念：

+ 输入空间：$\mathcal{X}$
+ 隐式结构空间：$\mathcal{Z}$
+ 模型：函数 $z=g(x)$ **或**条件概率分布 $P(z|x)$ **或**条件概率分布 $P(x|z)$
+ 假设空间（Hypothesis Space）：所有这些可能模型的集合
+ 目的：选出在给定评价标准下的最优模型
+ 样本容量为 N 的训练集：$U = {x_1, x_2,\dots,x_N}$

因为无监督学习，本质是研究数据中的潜在结构内容，也就是需要学习隐含在数据结构内部的信息，所以无监督学习对应的不是输出空间，而是隐式结构空间。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727143737519.png" alt="image-20220727143737519" style="zoom:75%;" /></center>

### 2.3 强化学习

强化学习其实要强调一个互动，互动是指的是智能系统与环境之间的一个连续互动，通过这个互动，学习一个最优的行为策略。可以参见李宏毅讲解的一章。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727144002687.png" alt="image-20220727144002687" style="zoom:67%;" /></center>

强化学习可以基于策略，也可以基于价值，基于策略的则是选择最优策略，基于价值是选择最优价值，都可以得到一个最优模型。

## 3. 统计学习方法的三要素

构成统计学习方法的三要素——**模型、策略、算法**。对于监督学习、无监督学习、强化学习，这三要素都是必备的，只不过形式不同。

对于监督学习，处理的是有标注的数据，数据中输出空间的类型已知，所以相应的模型、策略以及算法都是比较具体的。但对于无监督学习，处理的数据是无标注信息的，我们希望找到隐含在数据内部的结构信息，这时候的三要素——模型、策略、算法就不那么具体了。

### 3.1 监督学习的三要素

#### 3.1.1 监督学习的模型

对于监督学习，模型主要可以表达成两种形式，一个是条件概率分布的形式，一个是决策函数的形式。条件概率分布的形式，即概率模型；而决策函数的形式则是非概率模型。

<mark>假设空间</mark>（Hypothesis Space）是所有可能的条件概率分布或决策函数，用 $\mathcal{F}$ 表示。

**如果模型是由决策函数组成的集合**，那么假设空间将是所有可能决策函数的集合。每一个决策函数由一个参数向量决定，而假设空间是由参数向量所决定的函数组构成。我们称所有可能的参数向量组成的空间为**参数空间**，那么这个假设空间就应该是由参数空间决定的了。

+ 若 Hypothesis Space 是决策函数的集合：$\mathcal{F}=\{ f | Y=f(X)\}$
+ $\mathcal{F}$ 由一个参数向量决定的函数族构成：$\mathcal{F} = \{ f|Y=f_\theta(X), \theta \in \bold{R}^n\}$
+ 参数空间：$\Theta = \{ \theta| \theta \in \bold{R}^n\}$

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727145646811.png" alt="image-20220727145646811" style="zoom:75%;" /></center>

**如果模型表示为条件概率分布的形式**，那么假设空间就是由所有可能的条件概率分布组成的集合。对于每一个条件概率分布，它由一个参数向量来决定的，所以假设空间也可以说成是由一个参数向量决定的条件概率分布族构成的。此处，所有可能的参数构成参数空间。

+ 若 Hypothesis Space 是决策函数的集合：$\mathcal{F} = \{ P|P(Y|X)\}$
+ $\mathcal{F}$ 由一个参数向量决定的条件概率分布族构成：$\mathcal{F} = \{ P|P_\theta(Y|X), \theta \in \bold{R}^n\}$

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727150023099.png" alt="image-20220727150023099" style="zoom:75%;" /></center>

#### 3.1.2 监督学习的策略

如何在 Hypothesis Space 里面选择一个最优模型呢？这里就需要用到第二个要素策略。策略其实就是一种学习准则，用来选择最优模型的。想要选择模型，那么一定要知道如何度量模型的好坏。所以，这里先要引入几个概念。

+ <mark>损失函数</mark>：度量模型一次预测的好坏，记作 $L(Y, f(X))$，这里的 Y 是 label
+ <mark>风险函数</mark>：度量平均意义下模型预测的好坏

$$R_{exp}(f) = E_P[L(Y,f(X))]=\int_{\mathcal{X}\times \mathcal{Y}}L(y,f(x))P(x,y)dxdy$$

> $R_{exp}(f)$ 中 exp 表示 expectation，R 表示 Risk，f 表示模型，其计算就是对损失函数求了一下概率期望。如果对于假设空间中的每一个模型，我们都求一下风险函数值，选择一个最小的风险函数值所对应的模型就是我们想要的最优模型了。但是联合分布 $P(x,y)$ 并不是已知的，因此风险函数也就不能直接求出，所以我们需要一个经验值（估计值）来替代这个函数，也就引出了“经验风险”。

+ <mark>经验风险</mark>：模型 $f(X)$ 关于训练集的平均损失 $R_{emp}(f) = \frac{1}{N} \sum^N_{i=1}L(y_i,f(x_i))$，其中训练集 $T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}$

> emp 表示 empirical

下面我们看几种常见的损失函数：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727152000844.png" alt="image-20220727152000844" style="zoom:80%;" /></center>

**根据大数定律，当样本容量 N 趋于无穷大的时候，经验损失就会趋于风险函数**。所以，在一定程度上用经验损失作为风险函数的估计值是合理的。公式如下：

$$R_{emp}(f) \to R_{exp}(f), \qquad N \to \infty$$

可是在现实生活中，样本容量N一般是有限的，有的时候甚至会很小。因此，**仅仅用经验风险来估计风险函数效果并不理想，需要对其进行一定的矫正**。这里就涉及到监督学习的两个基本策略，一个是经验风险最小化策略，一个是结构风险最小化策略：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220727152419887.png" alt="image-20220727152419887" style="zoom:90%;" /></center>

<u>当样本容量 N 足够大的时候</u>，可以认为经验风险是风险函数的一个估计值，此时，只需选取使经验风险最小的模型即可。<u>但当样本容量 N 比较小的时候</u>，仅仅经验风险最小化，容易造成过拟合的现象，于是引入结构风险的概念。结构风险是在经验风险的基础上加了一个惩罚项，惩罚项针对的是模型的复杂度，也就是这里的模型越复杂，J(f) 就越大，当然模型越简单，J(f) 就越小。**结构风险的惩罚系数，可以平衡经验风险和模型的复杂度**。结构风险最小化，则是选取一个使结构风险最小的模型。

关于监督学习的策略，追根究底，就是选取一个目标函数，可以是经验风险，或者是结构风险，然后通过优化这个目标函数，达到学习模型的目的。

#### 3.1.3 监督学习的算法

在假设空间里面，根据策略去选择最优模型，需要一个具体的操作方案，操作方案也就是算法，是用来求解最优模型的。

如果这个最优模型存在显式解析解，那么简单了，直接把这个结果写出来即可。但是往往这个显式解是不存在的，所以需要一定的数值计算方法，比如梯度下降法。

### 3.2 无监督学习的三要素

无监督学习处理的是无标注的数据，所以我们希望在数据内部找到隐含的结构信息。

+ 模型：函数 $z=g_\theta(x)$ **或**条件概率分布 $P_\theta(z|x)$ **或**条件概率分布 $P_\theta(x|z)$
+ 策略：优化目标函数
+ 算法：通常是迭代算法

模型中涉及到的 z 来自于隐式结构空间，此时模型有三种表达方式，一种是函数形式，另外两种是条件概率分布的形式。

假设空间：

- 所有可能的函数所组成的集合
- 给定 x 的条件下，所有可能的 z 的条件概率分布组成的集合
- 给定 z 的情况下，所有可能的 x 的条件概率分布组成的集合

参数空间，则是由所有可能的参数组成的。

对于无监督学习，策略同样是优化目标函数。当然，因为无监督学习处理的数据是无标注信息的，更具有多变性，相应的目标函数会根据数据的不同而发生变化。