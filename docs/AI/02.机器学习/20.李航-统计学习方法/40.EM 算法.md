---
title: EM 算法
date: 2022-12-19 18:57:00
permalink: /pages/ml/statistical-learning-method/EM/
categories:
  - AI
  - 机器学习
  - 李航-统计学习方法
tags:
  - 
---

极大似然估计要解决的问题是，当有一堆观测数据 $y_1, y_2, \dots, y_m$，并知道它们服从的概率分布形式（比如服从二项分布） $P(Y|\theta)$ 时，求出使得 likelihood 最大化的 $\hat{\theta}$，即 $\hat{\theta} = \arg\max_\theta P(y_1|\theta) \cdot P(y_2|\theta) \cdot ... \cdot P(y_m|\theta)$。而 **EM 算法解决的就是含有 latent variable 时的极大似然估计问题**。

## 1. EM 算法的引入

### 1.1 三硬币模型的例子

一个介绍 EM 算法的例子。

***<u>三硬币模型</u>***	假设有 3 枚硬币：A、B、C，它们正面朝上的概率分别是 $\pi、p、q$，然后做如下投硬币的实验：先扔 A，若 A 朝上则再扔 B，朝下则再扔 C，也就是说第二步仍谁是取决于 A 朝上还是朝下；第二步掷硬币的结果作为本次实验的观察结果，正面记为 1，反面记为 0。独立重复 n 次试验（这里 n = 10），观察结果如下：

<center>1, 1, 0, 1, 0, 0, 1, 0, 1, 1</center>

我们只能观察最终的结果，但不知道每一次实验中是 A 朝上还是朝下，也就是不知道投的是 B 硬币还是 C 硬币。现在的任务是估计三枚硬币分别朝上的概率 $\pi、p、q$。

***<u>解</u>***	我们知道将一枚硬币抛出，朝上朝下是服从二项分布的。现在我们：

+ 令 Y 表示 **observable variable**，表示一次试验的结果是 0 或 1。
+ 硬币 A 的投掷结果记为 Z，表示 **latent variable**，它无法直接被观测到。
+ 模型参数 $\theta = (\pi, p, q)$，即这三枚硬币朝上的概率，我们的任务就是要估计它的值。

注意，随机变量 Y 是可以观测的，而 Z 无法被观测到，也就是说我们不知道在一次试验中 A 的投掷结果，只知道最终 B 或 C 的投掷结果。

 观测到的数据表示为 $Y = (y_1, y_2, \dots, y_n)$，未观测到的数据表示为 $Z = (z_1, z_2, \dots, z_n)$，那么似然函数就是：

$$P(y_1, \dots, y_n | \theta) = \prod_{j=1}^n [\pi p^{y_j}(1-p)^{1-y_j}+(1-\pi)q^{y_j}(1-q)^{1-y_j}]$$

上面的式子就是对每个观测数据的连乘。按照极大似然估计的方法，往往转换成对数的形式，因此对数似然函数写成：

$$L(\theta)= \log P(y_1, \dots, y_n | \theta) = \sum^n_{j=1}\log[...]$$

而我们的目标就是找出能够最大化 $L(\theta)$ 的 $\hat{\theta}$：

$$\hat{\theta} = \arg \max_\theta L(\theta)$$

由于里面存在 latent variable，这个问题是没有解析解的，只有通过迭代的方法来求解。EM 算法就是可以用于求解这个问题的一种迭代算法。

> 无法直接求解的原因在于，这里的 P(Y) 根本不知道是什么，因为所观测到的样本非常复杂，所以我们可以先假定它服从某个模型，即有一个生成模型，z -> y，然后 $P(Y)=\int_Z P(Y,Z)dz$。通过人为引入一个 latent variable z，然后做出 Z -> Y 的假设，这样求 $P(Y)=\frac{P(Y,Z)}{P(Z|Y)}$ 就使得问题具体化了。

### 1.2 EM 算法

一般地，用 Y 表示观测随机变量的数据，Z 表示隐随机变量的数据。Y 和 Z 连在一起称为<mark>完全数据</mark>（complete-data），观测数据 Y 又称为<mark>不完全数据</mark>（incomplete-data）。

EM 算法通过迭代求出极大似然估计 $\hat{\theta} = \arg \max_\theta L(\theta)$ 的结果。每轮迭代分成 E 步和 M 步。

首先介绍一个概念：Q function：

::: theorem Q function
完全数据的对数似然函数 $\log P(Y,Z|\theta)$ 关于在给定观测数据 Y 和当前估计的参数值 $\theta^{(i)}$ 下，对未观测数据 Z 的条件概率分布 $P(Z|Y, \theta^{(i)})$ 的期望称为 <mark>Q function</mark>，即：

$$Q(\theta, \theta^{(i)}) = E_Z [\log P(Y,Z|\theta)|Y, \theta^{(i)}]= \sum_Z \log P(Z|Y, \theta^{(i)}) P(Y,Z|\theta)$$
:::

> + Q function 是 EM 算法的核心，最好把 Q function 的定义背下来。
> + 这里之所以对 Z 积分，主要是可以实现将隐变量 z 给去掉。

下面正式介绍 EM 算法：

::: theorem EM Algorithm
**input**：观测变量数据 Y，隐变量数据 Z，联合分布 $P(Y,Z|\theta)$，条件分布 $P(Z|Y,\theta)$

**output**：模型参数 $\theta$ 的估计值

1. 选择参数的初值 $\theta^{(0)}$，开始迭代。
2. ***<u>Expectation</u>***：记 $\theta^{(i)}$ 为第 i 论迭代参数 $\theta$ 的估计值，在第 i+1 次迭代的 E 步，写出 Q function：$Q(\theta, \theta^{(i)})$。
3. ***<u>Maximization</u>***：求能够 maximize $Q(\theta, \theta^{(i)})$ 的 $\theta$，作为本轮（即第 i+1 论）的参数估计值 $\theta^{(i+1)}$：

$$\theta^{(i+1)}=\arg\max_\theta Q(\theta, \theta^{(i)})$$

4. 重复 E 步和 M 步，直到收敛。
:::

下面对 EM 算法做几点说明：

+ EM 算法是**初值敏感**的，即选择不同的初值可能得到不同的参数估计值。
+ Q function 中，$Q(\theta, \theta^{(i)})$ 的第一个变元表示要极大化的参数，是一个变量，而第二个变元表示参数的当前估计值，是一个常数。
+ 可以证明，这个迭代过程是**可以收敛**的。
+ EM 算法的计算过程中，迭代停止的条件一般是设置一个阈值，当 $\theta$ 的估计值变化小于阈值时停止，或者：

$$||Q(\theta^{(i+1)}, \theta^{(i)}) - Q(\theta^{(i)}, \theta^{(i)})|| \lt threshold$$

### 1.3 EM 算法的导出

见原书

## 2. EM 算法的收敛性

反正是可以收敛的，证明也不难，见原书。