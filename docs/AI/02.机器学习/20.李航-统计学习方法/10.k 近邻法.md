---
title: k 近邻法
date: 2022-07-28 16:22:43
permalink: /pages/ml/statistical-learning-method/knn/
categories:
  - AI
  - 机器学习
  - 李航-统计学习方法
tags:
  - 
---

## 1. k 近邻法简介

### 1.1 KNN 的直观理解

k 近邻（K-Nearest-Neighbor，**KNN**）的简单想法是，我们如果对某一个新的实例感兴趣时，根据它最近的 k 个邻居来判断即可。

<mark>k 近邻法</mark>（KNN）是一种基本的分类与回归方法。主要思想是：假定给定一个训练数据集，其中实例标签已定，当输入新的实例时，可以根据其最近的 k 个训练实例的标签，预测新实例对应的标注信息。

+ *用于分类问题*：对新的实例，根据与之相邻的 k 个训练实例的类别，通过多数表决等方式来进行预测，哪一类别的占比达，新的实例点就属于哪个类别；
+ *用于回归问题*：对新的实例，根据与之相邻的 k 个训练实例的标签，通过均值计算进行预测。

举个例子，下面的图形给定了 11 个观测值，包含 6 个蓝色正方形和 5 个红色三角形，这就是我们的训练数据集：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728220429410.png" alt="image-20220728220429410" style="zoom:80%;" /></center>

假如现在给定一个新的实例点（也即是中间那个绿色的圆点），它该属于哪个类别呢？<u>当选定 k = 3 时</u>，最近的 3 个邻居中红色三角形占得最多，因此预测新实例点为红色三角形类；<u>当选定 k = 5 时</u>，最近的 5 个邻居中蓝色矩形占得最多，因此预测新实例点为蓝色矩形类。

所以，多少个邻居，怎么计算距离，如何通过邻居的情况反映目标点的信息，都是非常关键的问题。

### 1.2 KNN 的具体算法

**输入**：训练集 $T=\{(x_1, y_1),(x_2, y_2),\dots,(x_N, y_N)\}$，其中 $x_i \in \mathcal{X} \subseteq \bf{R}^n$，$y \in \mathcal{Y} = \{c_1,c_2,\dots,c_K \}$

**输出**：新实例 x 所属的类别 y

具体来说，算法包含三部曲：

1. 给出「距离度量」的方式，计算新的 x 与训练集 T 中每个点的距离；
2. 找出与 x 最相近的 k 个点，将涵盖这 k 个点的 x 的邻域记作 $N_k(x)$；
3. 根据「分类决策规则」来决定 x 所属的类别 y，即：

$$y= argmax_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j),\ i=1,2,\dots,N; j=1,2,\dots,K$$

其中 $I(y_i=c_j)$ 是一个示性函数，当 $y_i=c_j$ 值为 1，否则为 0。所以上面那个 argmax 的式子就是想说，希望找到一个类别 $c_j$ 使得它所对应的样本个数在这个邻域里面所占的比例最大。

我们刚刚强调的三个事情：距离度量、k 个邻居、和分类决策规则就是 k 近邻法的三要素。注意：任意一者的变化，都可能导致实例 x 所属的类别发生变化。

### 1.3 KNN 的误差率

> 本部分的证明过程可以先略过

**训练集**：$T=\{(x_1, y_1),(x_2, y_2),\dots,(x_N, y_N)\}$

**类别集合**：$\mathcal{Y}=\{ c_1,c_2,\dots,c_K \}$

**考虑最近邻法**：就是指 k=1 的情况

对于新实例 $x$，$x_i$ 是距离最近的那个点，两者所属的真正类别分别记作   a 和 b。很明显，如果分类正确则 a=b，如果不正确则 $a \neq b$。

计算分类不正确时的误差率：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728222954731.png" alt="image-20220728222954731" style="zoom:80%;" /></center>

因为事件 $a=c_j$ 和 $b \neq c_j$ 两个相互独立，所以可以展开为两个概率的乘积，又因 $b \neq c_j$ 概率还可以写作 $1-P(b=c_j|x_i)$，就得到了最终的误差率公式。

接下来看一下 $Err(x,x_i)$ 所对应的上下界：

当 $K \to \infty$ 时，也就是类别有无穷多个时，

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728223631955.png" alt="image-20220728223631955" style="zoom:80%;" /></center>

此时，有：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728223831373.png" alt="image-20220728223831373" style="zoom:80%;" /></center>

可见,第一个求和式等于 1，那么公式可写成：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728223938229.png" alt="image-20220728223938229" style="zoom:80%;" /></center>

假设 x 的真实类别为 $c^*$：

$$c^*= \arg\max_{c_j \in \mathcal{Y}}P(c_j|x)$$

也就是说,希望找到一个 $c_j$，使得条件概率最大，那么这个 $c_j$ 就是我们要找到类别，这符合了最优决策规则，或者说最大概率的思想。

对应的贝叶斯误差率为：$P^*(err|x)=1-P(c^*|x)$

回到最前所讲的 k 近邻法误差率第二项：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728224324848.png" alt="image-20220728224324848" style="zoom:80%;" /></center>

如果想使这个式子的第二项 $\sum_{c_j\ne c^* \cdot P^2(c_j|x)}$ 最小，那么需要使 K-1 个项相等，对应的概率为 $\frac{1-P(c^*|x)}{K-1}$，就得到了最终的公式。

**误差率**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729084331475.png" alt="image-20220729084331475" style="zoom:80%;" /></center>

当 $P^*$ 较小时，$Err(x,x_i)$ 上界近似于 $2P^*$。于是，上下界就得到了：

$$P^* \le Err(x,x_i) \le 2P^*$$

此处给大家说的这个误差率考虑的是最近邻法，也就是 k=1 的时候。如果推广到 k 近邻法，可以令 N 趋近于无穷大，K 也无穷大，也就是训练数据集中包含了无穷多个样本，而且输出空间也包含了无穷多个类别。

此时的误差率应该趋于 $P^*$ 的，也就是贝叶斯误差率，它所对应的就是最优决策的误差。因此，如果 K 足够大，在大样本的情况下，可以认为 k 近邻法是近似于最优决策。

## 2. k 近邻法的三要素

k 近邻法虽然没有显性模型，但也是具有决定模型的三要素的。由于 k 近邻法就是要找到目标点的 k 个邻居，然后再做决策，所以找多少个邻居，怎么找邻居，以及找到邻居后怎么办是非常重要的。

实际上，这就涉及到空间的划分，俗称划地盘，具体来说就是先拿到训练数据集，然后对特征向量空间进行划分。

### 2.1 k 近邻法的 feature space

举个大家都熟悉的例子，二维平面，也就是数据集对应的特征空间是二维的。

假如训练数据集中包含两类数据，一个是「点类」，一个是「叉类」，我们可以给每个训练实例都划分出个小地盘出来，也就是图中形状各异的格子。我们通过最简单的最近邻法来说明，也就是当 k=1 时的情况。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729090953929.png" alt="image-20220729090953929" style="zoom:80%;" />

得到这些小格子后，咱们就可以对每一个新的实例点做出判断，预测它的类别了。现在我们以「红点为例」，距离它最近的训练实例点是下边的小叉叉，那么「它的类别属于叉类」，同样，以「黄色小三角为例」，距离它最近的训练实例是圆点，「类别属于点类」。

那么这些小格子怎么画出来的呢？或者说这一个个子空间怎么确定出来的呢？这就得考虑 k 近邻法的三要素了：**距离度量、 值的选择、分类决策规则**。

### 2.2 三要素之一：距离度量

$L_p$ 的定义如下：

::: theorem Lp 距离
feature space $\mathcal{X}$ 假设为 $\bf{R}^n$，$\forall x_i,x_j \in \mathcal{X}$，$x_i=(x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T$，$x_j=(x_j^{(1)},x_j^{(2)},\dots,x_j^{(n)})^T$，则有：

$$L_p(x_i,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$$
:::

特征空间中任意的 $x_i$ 和 $x_j$ 都是一个 n 维向量，上标 $l$ 代表的是第 $l$ 个特征，p 决定了不同的范数距离。

:sparkles: 1. **p = 2 时**，称为<mark>欧式距离</mark>（常见的勾股定理就是二维时的它）：

$$L_2(x_i,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}$$

:sparkles: 2. **p = 1 时**，称为<mark>曼哈顿距离</mark>（也就是计算所有特征下的绝对距离，再求个和）：

$$L_2(x_i,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|)$$

:sparkles: 3. **p = $\infty$ 时**，称为<mark>切比雪夫距离</mark>（也就是找到所有特征下的绝对距离的最大值）：

$$L_{\infty}(x_i,x_j)=\max_l |x_i^{(l)}-x_j^{(l)}|$$

下面我们通过一个二维的例子来说明当 p=1、p=2、p=$\infty$ 时，对应的 $L_p$ 距离下的图形是分别什么。假设有两个点，一个是原点 $x_0$，另一个点 $x_i$，我们看一下当距离为 1 时所有 $x_i$ 所构成的图形：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729093231705.png" alt="image-20220729093231705" style="zoom:100%;" /></center>

由此可见，当采用的不同的 p 时，距离也会发生变化，那么这时候能找到的最近的邻居自然也不同了。

### 2.3 三要素之一： k 值的选择

对于一个新实例点，当 k 值不同时，所属的分类也会不同。例子之前讲过：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728220429410.png" alt="image-20220728220429410" style="zoom:80%;" /></center>

+ 对于新实例点绿色圆圈，按照多数表决规则：当 k=3 时，它是红三角类型；当 k=5 时，它是蓝正方形类型。

**如果选择的 k 值比较小**，相当于就在一个比较小的邻域里面对训练集内的实例进行预测，所以近似误差小（也就是说**对 training set 的拟合误差小**），但是如果新增一个实例时，超过了范围则会导致估计误差增大，即**对噪声很敏感**。这就是只对训练集友好，对新的实例点不友好的情况——过拟合。与之相对，k 值较大时，就会出现欠拟合。整理如下：

|   特点   | k 值较小 | k 值较大 |
| :------: | :------: | :------: |
| 近似误差 |    小    |    大    |
| 估计误差 |    大    |    小    |
|  敏感性  |    强    |    弱    |
|   模型   |   复杂   |   简单   |
| 拟合程度 |  过拟合  |  欠拟合  |

::: note Tips
1. k 值可通过交叉验证选择。
2. k 值一般低于训练集样本量的平方根。
:::

### 2.4 三要素之一：分类决策规则

**多数表决规则**：由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。

**分类函数**：$f: \bf{R}^n \to \{c_1,c_2,\dots,c_K\}$

这个函数的意思是 f 每吞进去一个 n 维向量的实例，就得吐出来一个类别。

为了制定一个规则决定这个分类函数，我们需要比较损失情况，因为是分类问题，这时候最常用的损失函数是 **0-1 损失函数**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729095557163.png" alt="image-20220729095557163" style="zoom:80%;" /></center>

其中 $f(X)$ 是根据 k 个训练实例所属的类别得出的预测类别，$Y$ 是对应的真实类别。

那么误分类的概率就表达出来啦。**0-1 误分类概率**：

$$P(Y \neq f(X)) = 1-P(Y=f(X))$$

假如给定实例 $x\in \mathcal{X}$，相应的 k 邻域 $N_k(x)$，在 k 近邻法中有一个示性函数 $I(y_i=c_j)$，这里 $y_i$ 是真实的实例对应的输出类别，$c_j$ 是 k 类别中的任何一个类别，假设在 k 类别中有一个 $c_j$ 等于 $y_i$，那就意味着 $y_i$ 的类别找对了，所以邻域 $N_k(x)$ 中类别找对的实例越多，误分类就越小。

可以将所有的 $c_j$ 从 $c_1$、$c_2$ 一直到 $c_K$ 与邻域 $N_k(x)$ 中 $y_i$ 进行逐一的比对和计算，哪个误分类率最小，那么就会挑出这个 $c_j$。对应的**误分类率**可以继续写成：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729100443417.png" alt="image-20220729100443417" style="zoom:80%;" /></center>

上面这个误分类率就是指对于某一个实例 $x_i$ 而言，将它所属的类别与 k 个训练实例所对应的类别 $c_1,c_2,\dots,c_k$ 逐一的比对，如果相等记为 1，如果不等记为 0，然后把这些数求和后取平均值，可以看出，当这个结果越大说明分类越正确，也就意味着误分类率也可表达成：

$$\arg \max \sum_{x_i \in N_k(x)}I(y_i=c_j)$$

这就体现出了多数表决规则啦！

