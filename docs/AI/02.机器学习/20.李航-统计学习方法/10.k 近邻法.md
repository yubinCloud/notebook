---
title: k 近邻法
date: 2022-07-28 16:22:43
permalink: /pages/ml/statistical-learning-method/knn/
categories:
  - AI
  - 机器学习
  - 李航-统计学习方法
tags:
  - 
---

## 1. k 近邻法简介

### 1.1 KNN 的直观理解

k 近邻（K-Nearest-Neighbor，**KNN**）的简单想法是，我们如果对某一个新的实例感兴趣时，根据它最近的 k 个邻居来判断即可。

<mark>k 近邻法</mark>（KNN）是一种基本的分类与回归方法。主要思想是：假定给定一个训练数据集，其中实例标签已定，当输入新的实例时，可以根据其最近的 k 个训练实例的标签，预测新实例对应的标注信息。

+ *用于分类问题*：对新的实例，根据与之相邻的 k 个训练实例的类别，通过多数表决等方式来进行预测，哪一类别的占比达，新的实例点就属于哪个类别；
+ *用于回归问题*：对新的实例，根据与之相邻的 k 个训练实例的标签，通过均值计算进行预测。

举个例子，下面的图形给定了 11 个观测值，包含 6 个蓝色正方形和 5 个红色三角形，这就是我们的训练数据集：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728220429410.png" alt="image-20220728220429410" style="zoom:80%;" /></center>

假如现在给定一个新的实例点（也即是中间那个绿色的圆点），它该属于哪个类别呢？<u>当选定 k = 3 时</u>，最近的 3 个邻居中红色三角形占得最多，因此预测新实例点为红色三角形类；<u>当选定 k = 5 时</u>，最近的 5 个邻居中蓝色矩形占得最多，因此预测新实例点为蓝色矩形类。

所以，多少个邻居，怎么计算距离，如何通过邻居的情况反映目标点的信息，都是非常关键的问题。

### 1.2 KNN 的具体算法

**输入**：训练集 $T=\{(x_1, y_1),(x_2, y_2),\dots,(x_N, y_N)\}$，其中 $x_i \in \mathcal{X} \subseteq \bf{R}^n$，$y \in \mathcal{Y} = \{c_1,c_2,\dots,c_K \}$

**输出**：新实例 x 所属的类别 y

具体来说，算法包含三部曲：

1. 给出「距离度量」的方式，计算新的 x 与训练集 T 中每个点的距离；
2. 找出与 x 最相近的 k 个点，将涵盖这 k 个点的 x 的邻域记作 $N_k(x)$；
3. 根据「分类决策规则」来决定 x 所属的类别 y，即：

$$y= argmax_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j),\ i=1,2,\dots,N; j=1,2,\dots,K$$

其中 $I(y_i=c_j)$ 是一个示性函数，当 $y_i=c_j$ 值为 1，否则为 0。所以上面那个 argmax 的式子就是想说，希望找到一个类别 $c_j$ 使得它所对应的样本个数在这个邻域里面所占的比例最大。

我们刚刚强调的三个事情：距离度量、k 个邻居、和分类决策规则就是 k 近邻法的三要素。注意：任意一者的变化，都可能导致实例 x 所属的类别发生变化。

### 1.3 KNN 的误差率

> 本部分的证明过程可以先略过

**训练集**：$T=\{(x_1, y_1),(x_2, y_2),\dots,(x_N, y_N)\}$

**类别集合**：$\mathcal{Y}=\{ c_1,c_2,\dots,c_K \}$

**考虑最近邻法**：就是指 k=1 的情况

对于新实例 $x$，$x_i$ 是距离最近的那个点，两者所属的真正类别分别记作   a 和 b。很明显，如果分类正确则 a=b，如果不正确则 $a \neq b$。

计算分类不正确时的误差率：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728222954731.png" alt="image-20220728222954731" style="zoom:80%;" /></center>

因为事件 $a=c_j$ 和 $b \neq c_j$ 两个相互独立，所以可以展开为两个概率的乘积，又因 $b \neq c_j$ 概率还可以写作 $1-P(b=c_j|x_i)$，就得到了最终的误差率公式。

接下来看一下 $Err(x,x_i)$ 所对应的上下界：

当 $K \to \infty$ 时，也就是类别有无穷多个时，

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728223631955.png" alt="image-20220728223631955" style="zoom:80%;" /></center>

此时，有：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728223831373.png" alt="image-20220728223831373" style="zoom:80%;" /></center>

可见,第一个求和式等于 1，那么公式可写成：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728223938229.png" alt="image-20220728223938229" style="zoom:80%;" /></center>

假设 x 的真实类别为 $c^*$：

$$c^*= \arg\max_{c_j \in \mathcal{Y}}P(c_j|x)$$

也就是说,希望找到一个 $c_j$，使得条件概率最大，那么这个 $c_j$ 就是我们要找到类别，这符合了最优决策规则，或者说最大概率的思想。

对应的贝叶斯误差率为：$P^*(err|x)=1-P(c^*|x)$

回到最前所讲的 k 近邻法误差率第二项：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728224324848.png" alt="image-20220728224324848" style="zoom:80%;" /></center>

如果想使这个式子的第二项 $\sum_{c_j\ne c^* \cdot P^2(c_j|x)}$ 最小，那么需要使 K-1 个项相等，对应的概率为 $\frac{1-P(c^*|x)}{K-1}$，就得到了最终的公式。

**误差率**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729084331475.png" alt="image-20220729084331475" style="zoom:80%;" /></center>

当 $P^*$ 较小时，$Err(x,x_i)$ 上界近似于 $2P^*$。于是，上下界就得到了：

$$P^* \le Err(x,x_i) \le 2P^*$$

此处给大家说的这个误差率考虑的是最近邻法，也就是 k=1 的时候。如果推广到 k 近邻法，可以令 N 趋近于无穷大，K 也无穷大，也就是训练数据集中包含了无穷多个样本，而且输出空间也包含了无穷多个类别。

此时的误差率应该趋于 $P^*$ 的，也就是贝叶斯误差率，它所对应的就是最优决策的误差。因此，如果 K 足够大，在大样本的情况下，可以认为 k 近邻法是近似于最优决策。

## 2. k 近邻法的三要素

k 近邻法虽然没有显性模型，但也是具有决定模型的三要素的。由于 k 近邻法就是要找到目标点的 k 个邻居，然后再做决策，所以找多少个邻居，怎么找邻居，以及找到邻居后怎么办是非常重要的。

实际上，这就涉及到空间的划分，俗称划地盘，具体来说就是先拿到训练数据集，然后对特征向量空间进行划分。

### 2.1 k 近邻法的 feature space

举个大家都熟悉的例子，二维平面，也就是数据集对应的特征空间是二维的。

假如训练数据集中包含两类数据，一个是「点类」，一个是「叉类」，我们可以给每个训练实例都划分出个小地盘出来，也就是图中形状各异的格子。我们通过最简单的最近邻法来说明，也就是当 k=1 时的情况。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729090953929.png" alt="image-20220729090953929" style="zoom:80%;" />

得到这些小格子后，咱们就可以对每一个新的实例点做出判断，预测它的类别了。现在我们以「红点为例」，距离它最近的训练实例点是下边的小叉叉，那么「它的类别属于叉类」，同样，以「黄色小三角为例」，距离它最近的训练实例是圆点，「类别属于点类」。

那么这些小格子怎么画出来的呢？或者说这一个个子空间怎么确定出来的呢？这就得考虑 k 近邻法的三要素了：**距离度量、 值的选择、分类决策规则**。

### 2.2 三要素之一：距离度量

$L_p$ 的定义如下：

::: theorem Lp 距离
feature space $\mathcal{X}$ 假设为 $\bf{R}^n$，$\forall x_i,x_j \in \mathcal{X}$，$x_i=(x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T$，$x_j=(x_j^{(1)},x_j^{(2)},\dots,x_j^{(n)})^T$，则有：

$$L_p(x_i,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$$
:::

特征空间中任意的 $x_i$ 和 $x_j$ 都是一个 n 维向量，上标 $l$ 代表的是第 $l$ 个特征，p 决定了不同的范数距离。

:sparkles: 1. **p = 2 时**，称为<mark>欧式距离</mark>（常见的勾股定理就是二维时的它）：

$$L_2(x_i,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}$$

:sparkles: 2. **p = 1 时**，称为<mark>曼哈顿距离</mark>（也就是计算所有特征下的绝对距离，再求个和）：

$$L_2(x_i,x_j)=(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|)$$

:sparkles: 3. **p = $\infty$ 时**，称为<mark>切比雪夫距离</mark>（也就是找到所有特征下的绝对距离的最大值）：

$$L_{\infty}(x_i,x_j)=\max_l |x_i^{(l)}-x_j^{(l)}|$$

下面我们通过一个二维的例子来说明当 p=1、p=2、p=$\infty$ 时，对应的 $L_p$ 距离下的图形是分别什么。假设有两个点，一个是原点 $x_0$，另一个点 $x_i$，我们看一下当距离为 1 时所有 $x_i$ 所构成的图形：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729093231705.png" alt="image-20220729093231705" style="zoom:100%;" /></center>

由此可见，当采用的不同的 p 时，距离也会发生变化，那么这时候能找到的最近的邻居自然也不同了。

### 2.3 三要素之一： k 值的选择

对于一个新实例点，当 k 值不同时，所属的分类也会不同。例子之前讲过：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220728220429410.png" alt="image-20220728220429410" style="zoom:80%;" /></center>

+ 对于新实例点绿色圆圈，按照多数表决规则：当 k=3 时，它是红三角类型；当 k=5 时，它是蓝正方形类型。

**如果选择的 k 值比较小**，相当于就在一个比较小的邻域里面对训练集内的实例进行预测，所以近似误差小（也就是说**对 training set 的拟合误差小**），但是如果新增一个实例时，超过了范围则会导致估计误差增大，即**对噪声很敏感**。这就是只对训练集友好，对新的实例点不友好的情况——过拟合。与之相对，k 值较大时，就会出现欠拟合。整理如下：

|   特点   | k 值较小 | k 值较大 |
| :------: | :------: | :------: |
| 近似误差 |    小    |    大    |
| 估计误差 |    大    |    小    |
|  敏感性  |    强    |    弱    |
|   模型   |   复杂   |   简单   |
| 拟合程度 |  过拟合  |  欠拟合  |

::: note Tips
1. k 值可通过交叉验证选择。
2. k 值一般低于训练集样本量的平方根。
:::

### 2.4 三要素之一：分类决策规则

**多数表决规则**：由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。

**分类函数**：$f: \bf{R}^n \to \{c_1,c_2,\dots,c_K\}$

这个函数的意思是 f 每吞进去一个 n 维向量的实例，就得吐出来一个类别。

为了制定一个规则决定这个分类函数，我们需要比较损失情况，因为是分类问题，这时候最常用的损失函数是 **0-1 损失函数**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729095557163.png" alt="image-20220729095557163" style="zoom:80%;" /></center>

其中 $f(X)$ 是根据 k 个训练实例所属的类别得出的预测类别，$Y$ 是对应的真实类别。

那么误分类的概率就表达出来啦。**0-1 误分类概率**：

$$P(Y \neq f(X)) = 1-P(Y=f(X))$$

假如给定实例 $x\in \mathcal{X}$，相应的 k 邻域 $N_k(x)$，在 k 近邻法中有一个示性函数 $I(y_i=c_j)$，这里 $y_i$ 是真实的实例对应的输出类别，$c_j$ 是 k 类别中的任何一个类别，假设在 k 类别中有一个 $c_j$ 等于 $y_i$，那就意味着 $y_i$ 的类别找对了，所以邻域 $N_k(x)$ 中类别找对的实例越多，误分类就越小。

可以将所有的 $c_j$ 从 $c_1$、$c_2$ 一直到 $c_K$ 与邻域 $N_k(x)$ 中 $y_i$ 进行逐一的比对和计算，哪个误分类率最小，那么就会挑出这个 $c_j$。对应的**误分类率**可以继续写成：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729100443417.png" alt="image-20220729100443417" style="zoom:80%;" /></center>

上面这个误分类率就是指对于某一个实例 $x_i$ 而言，将它所属的类别与 k 个训练实例所对应的类别 $c_1,c_2,\dots,c_k$ 逐一的比对，如果相等记为 1，如果不等记为 0，然后把这些数求和后取平均值，可以看出，当这个结果越大说明分类越正确，也就意味着误分类率也可表达成：

$$\arg \max \sum_{x_i \in N_k(x)}I(y_i=c_j)$$

这就体现出了多数表决规则啦！

## 3. kd 树的构造和搜索

假如我们现在有一个数据集，当给定一个新的实例时，最简单粗暴的办法就是计算它和所有点的距离，然后找到 k 个最近邻，最后根据多数表决的规则判断这个实例所属的类。

但是如果这个数据集中的训练实例非常多且密集呢？如果数据集中有成千上万个点，那每运算一个点的类型都要经过巨大的额运算，这样的运算量是极大的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729103824138.png" alt="image-20220729103824138" style="zoom:80%;" /></center>

这时，我们就可以用一个更快速的计算办法 —— kd 树。

### 3.1 什么是 kd 树？

::: theorem
kd 树是一种对 k 维空间中的实例点进行储存以便对其进行快速检索的树形数据结构。
:::

从本质上来看，kd 树是一个二叉树，根据这个结构对 k 维空间进行不断的划分，每一个节点就代表了 k 维超矩形区域。

> 注意，这里 kd 树的 k 指的是是特征的个数，也就是数据的维度，而之前 k 近邻法中的 k 指的是距离新实例点最近的 k 个邻居。

举个例子，二维（k=2）的划分区域：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729104834120.png" alt="image-20220729104834120" style="zoom:80%;" />

三维（k=3）的划分区域：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729104912436.png" alt="image-20220729104912436" style="zoom:80%;" />

### 3.2 如何构造 kd 树？

「**输入**」：k 维空间数据集 $T=\{x_1,x_2,\dots,x_N \}$，其中 $x_i=(x_i^{(1)},x_i^{(2)},\dots,x_i^{(k)})^T$

「**输出**」：kd 树

:footprints: 1. **开始：构造根节点**

以 $x^{(1)}$ 作为坐标轴，选 $x^{(1)}$ 方向上 N 个数据在该维度上的值的中位数作为**切分点**，将超矩形区域切割为两个子区域，由根节点生出深度为 1 的左右子节点，左节点的坐标小于切分点，右节点坐标大于切分点。

:footprints: 2. **重复：剩余特征的选取与切割**

继续对深度为 j 的结点，选择 $x^{(l)}$ 为切分坐标轴，$l = j (\mod k) + 1$，以该结点区域中所有实例 $x^{(l)}$ 坐标的中位数作为切分点，将区域不断分割为两个子区域。

:footprints: 3. **停止：得到 kd 树**

直到两个子区域没有实例时停止分割，即得到一棵 kd 树。

### 3.3 kd 树构造示例

现在我们按照上面的思路一起来构建一棵二维的 kd 树。

「**输入**」：$T = \{(2,3),(5,4),(9,6),(4,7),(8,1),(7,2) \}$

数据的散点图如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729133628224.png" alt="image-20220729133628224" style="zoom:80%;" />

因为该训练数据集的维度为 2，那么任选一个特征即可。不妨选择 $x^{(1)}$ 为坐标轴，将 $x^{(1)}$ 中的数据按照从小到大排序，分别是：2,4,5,7,8,9。中位数是 5,7，不妨选 7，即以 (7,2) 为根节点进行切分：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729133825394.png" alt="image-20220729133825394" style="zoom:80%;" />

左边的就是在 $x^{(1)}$ 小于 7 的子节点，右边的是大于 7 的子节点。

下面进行第 2 次切分，以 $x^{(2)}$ 为坐标轴，第一次切分后的左边区域所有节点的 $x^{(2)}$ 上的值排序后是 2,3,4,7，中位数选 4，即切分点坐标是 (5,4)，于是画一条横线进行第二次切分。

同样的，对第一次切分后的右边区域而言也进行如此切分，选择 (9,6) 为切分点进行第二次切分。切分后的图为：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729134159173.png" alt="image-20220729134159173" style="zoom:80%;" />

我们将切分后的四个区域分别以 A B C D 命名：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729134252348.png" alt="image-20220729134252348" style="zoom:80%;" />

可以看到 ACD 三个区域中还有一个实例点，那么过这些点继续画一条垂直于 $x^{(1)}$ 轴的线进行切分，而 B 区域内已经满足了区域内无实例点则无需切分的结束条件。

至此，切分均已完毕：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729134407440.png" alt="image-20220729134407440" style="zoom:80%;" />

根据这个层次可以绘制出 kd 树：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729134438268.png" alt="image-20220729134438268" style="zoom:80%;" />

### 3.4 如何搜索 kd 树？

「**输入**」：已构造的 kd 树，目标点是 x

「**输出**」：x 的最近邻

:footprints: 1. **寻找“当前最近点”**

从根结点出发，递归访问 kd 树，找出所划分的区域内包含 x 的叶结点，以此叶结点为“当前最近点”。

:footprints: 2. **回溯**

以目标点和“当前最近点”的距离沿树根部进行回溯和迭代，当前最近点一定存在于该结点一个子结点对应的区域，检查子结点的父结点的另一子结点对应的区域是否有更近的点，如果更近则更新“当前最近点”。

当回退到根结点时，搜索结束，最后的“当前最近点”即为 x 的最近邻点。

看不懂没关系，接下来看到例题就都明白啦！

### 3.5 搜索 kd 树示例

还是以刚才生成的那棵 kd 树为例：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729134438268.png" alt="image-20220729134438268" style="zoom:80%;" />

#### Example 1

「**输入**」：kd 树，目标点 x=(2.1, 3.1)；

「**输出**」：该目标点的最近邻点

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729142830759.png" alt="image-20220729142830759" style="zoom:80%;" />

1. <u>寻找当前最近点</u>：从根节点开始，x=(2.1, 3.1) 在根节点 (7, 2) 的左子区域内，继续到 (5,4) 所确定的左子区域内，继续到 (2, 3) 的右子区域中，(2, 3) 就是当前最近邻点。

2. <u>回溯</u>：我们以 (2.1, 3.1) 为圆心，以两点之间的距离为半径画一个圆，这个区域内没有其他的点，那就证明 (2, 3) 是 x=(2.1, 3.1) 的最近邻点。

#### Example 2

「**输入**」：kd 树，目标点 x=(2, 4.5)；

「**输出**」：该目标点的最近邻点

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729143258514.png" alt="image-20220729143258514" style="zoom:80%;" />

1. <u>寻找当前最近点</u>：从根节点开始，x=(2, 4.5) 在根节点 (7,2) 的左子区域内，继续到 (5,4) 所确定的上子区域内，继续到 (4,7) 的左子区域中，(4, 7) 就是当前最近邻点。
2. <u>回溯</u>：我们以 (2, 4.5) 为圆心，以 (2, 4.5) 到 (4,7) 两点之间的距离为半径画一个圆，这个区域内有两个节点，分别是 (2,3) 和 (5,4)，通过计算 (2, 4.5) 到这两点的距离，得出到 (2, 3) 的距离最近，那么 (2, 3) 就是最近邻点。接着，再以 (2, 4.5) 为圆心，以 (2, 4.5) 到 (2, 3) 两点之间的距离为半径画一个圆，此时圆里没有其他的节点，说明可以确认 (2, 3) 就是 (2, 4.5) 的最近邻点。