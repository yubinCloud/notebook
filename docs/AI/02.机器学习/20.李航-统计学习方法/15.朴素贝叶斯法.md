---
title: 朴素贝叶斯法
date: 2022-07-29 14:42:08
permalink: /pages/ml/statistical-learning-method/naive-Bayes/
categories:
  - AI
  - 机器学习
  - 李航-统计学习方法
tags:
  - 
---

贝叶斯思维和我们人类做判断的思路是相似的，都是按照**主观判断**、**添加新信息**、**最终结论**三步骤进行，换成数学思维里就是**先验概率**、**调整因子**、**后验概率**。

> 比如今天是冬至，你走到餐馆里，老板根据今天的节气做出一个主观判断，会问你“要不要来盘饺子”，因为大多数人在冬至这一天会选择吃饺子。但如果你用宝儿姐的四川话回一句“我今天不吃饺子”，那这句话其实相当于在原有的基础上提供了新的信息，老板根据新提供的信息可能会判断出你是个四川人，于是会问你“要不要来碗羊汤”，因为四川人在冬至这一天会选择喝羊汤。如果你说好，那就是老板成功预测出今天你打算喝羊汤这个最终结论。

上面例子的整个过程用 Bayes 思维表述就是：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729145509166.png" alt="image-20220729145509166" style="zoom:80%;" /></center>

## 1. 朴素贝叶斯定理

### 1.1 Bayes 定理

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729150934386.png" alt="image-20220729150934386" style="zoom:80%;" /></center>

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729151020298.png" alt="image-20220729151020298" style="zoom:90%;" /></center>

### 1.2 Bayes 分类

存在 K 类：$c_1,c_2,\dots,c_K$，给定一个新的实例 $x=(x^{(1)},x^{(2)},\dots,x^{(n)})$，问：该实例归属于哪一类？

由 Bayes 公式知 $P(Y=c_i|X=x) = \frac{P(X=x|Y=c_i) \cdot P(Y=c_i)}{\sum^K_{i=1}P(X=x|Y=c_i) \cdot P(Y=c_i)}$，所以我们可以分别计算 $P(Y=c_1|X=x), P(Y=c_2|X=x), \dots, P(Y=c_K|X=x)$ 的值，然后找到最大的那一个，从而就可以确定归属的类了。

公式也可以写成：

$$\arg\max_{c_i} P(X=x|Y=c_i)\cdot P(Y=c_i)$$

那么什么是朴素 Bayes 呢？

### 1.3 Naive Bayes 

朴素贝叶斯和贝叶斯相比，做了一个“朴素”的假设：**特征与特征之间是相互独立、互不影响的**。

这样我们回到 Bayes 分类的 $$\arg\max_{c_i} P(X=x|Y=c_i)\cdot P(Y=c_i)$$ 公式中，因为所有的 $x^{(i)}$ 与 $x^{(j)}$ 在 $i \neq j$ 时相互独立，那么 $P(X=x|Y=c_i)$ 就可以写成：

$$P(X=x|Y=c_i) = \prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)$$

所以最终只需要求出：

$$\arg\max_{c_i}P(Y=c_i) \prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)$$

从而找到了归属的类别，完成了朴素贝叶斯法的分类。

## 2. 朴素贝叶斯法

之前介绍了朴素贝叶斯法，它常用于解决分类问题，所以就有了朴素贝叶斯分类器，从根本上来说它就是一种「分类方法」。这一节我们继续说说，朴素贝叶斯**因何而朴素**，它的判断准则又是什么呢？

### 2.1 如何理解“朴素”？

首先来看一个训练数据集，计算得出一个联合概率分布。

已知训练数据集 $T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}$

**输入**：$\mathcal{X} \subseteq \bf{R}^n$，$x \in \mathcal{X}$

**输出**：$\mathcal{Y}=\{c_1,c_2,\dots,c_K \}$，$y \in \mathcal{Y}$

**生成方法**：学习联合概率分布 $P(X,Y)$

> 注意这里是用了“生成方法”，这种方法首先学习联合概率分布 $P(X,Y)$，得到条件概率分布 $P(Y|X)$，即生成模型，比如朴素贝叶斯法和隐马尔可夫模型。

接下来，我们就一起看如何求出联合概率分布。

X 是 feature，Y 是所属类别，$P(Y=c_i)$ 叫做<mark>先验概率分布</mark>，$P(X=x|Y=c_i)$ 叫做<mark>条件概率分布</mark>，而 $P(Y=c_i|X=x)$ 叫做<mark>后验概率分布</mark>。

联合概率分布可以通过先验概率分布乘以条件概率分布得到。

为什么要“朴素”呢？

### 2.2 为什么要有条件独立性假设？

以“帅是怎么定义的”为例，可以根据“身高、体重、脸型、鼻型”列一个表格，在“身高”中，高于 170cm 就是“高”，否则就是“矮”，其余 feature 定义类似。假如每种特征都有两种情况，并且最后的属性“帅/不帅”也有两种，那么计算联合概率分布时，会有 $2 \cdot 2^4 =2^5$ 种组合。

如果用数学语言表达就是：

1. $x^{(j)}$ 的可能取值有 $S_j$ 个，组合数就是 $S_1 \cdot S_2 \dots S_n$
2. $y$ 的可能取值有 K 个
3. 总的组合数就是 $K \prod^n_{j=1}S_j$

惊人的组合数，随着指数级的增加而递增。

因此，在朴素贝叶斯法中，需要加了一个特征相互独立的假设，不然我们计算不出来这么大的联合概率分布，这样就可以转化为简单的形式：

$$P(X=x|Y=c_i)=\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)$$

+ 其中 n 是 $x$ 的维度

于是，**后验概率**就是：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20B2C3DAE5408766BCE0F505C7029045.jpg" alt="img" style="zoom: 50%;" /></center>

### 2.3 后验概率最大化准则

了解了朴素贝叶斯法的重要性，那么我们再来看看它的具体计算方法是什么？

假如存在 K 类 $c_1,c_2,\dots,c_K$，现在给定一个新的实例 $x=(x^{(1)},x^{(2)},\dots,x^{(n)})$，问：该实例归属于哪一类？

现在这个问题对我们来说并不陌生，在前面已经给出了答案：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/20B2C3DAE5408766BCE0F505C7029045.jpg" alt="img" style="zoom: 50%;" /></center>

由于分母都相同，因此只要计算出分子的最大值就可找出这个实例所对应的类：

$$y=\arg\max_{c_i}P(Y=c_i)\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_i)$$

那么对应的判断准则又怎么界定呢？如果得到模型有多个，那个模型更好呢，如何进行比较？在 k 近邻法中，为了制定规则来决定这个分类函数的好与坏，我们需要比较分类问题的损失情况，在朴素贝叶斯方法中继续用到 **0-1 损失函数**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220729095557163.png" alt="image-20220729095557163" style="zoom:80%;" /></center>

+ $f(X)$ 是分类决策函数，$Y$ 是对应的分类，输出空间为 $\{c_1,c_2,\dots,c_K \}$

+ 当两者相等，证明分类正确，没有损失，反之记为1，有损失。

这个损失函数对应的期望是：$R(f)=E[L(Y,f(X))]$

由于朴素贝叶斯法用到的是条件概率的方式，因此**对于损失函数的期望也以条件概率进行表达**，将损失函数对应的期望最小值可以写成：

$$\min E_{Y|X} \sum^K_{i=1}L(c_i,f(X))P(c_i|X)$$

期望的下标 $Y|X$，表示这里的期望对应的是已知 X 下 Y 的条件概率分布。根据数学期望的定义，上面这个期望最小值表达式，就可以理解为是先计算出**每个分类的损失函数** $L(c_i,f(X))$ 乘以**对应这个分类发生的概率** $P(c_i|X)$，再对所有计算值进行**求和**后的结果。

根据这个思路，我们就可以考虑所有的特征后，进行期望最小值的计算，最小的期望风险所对应的那个模型就是我们想要的那一个。

如果，这时候有一个新的实例 $X=x$，我们如何得到它的分类决策函数 $f(x)$ 呢？**仍然去找使损失期望最小值的那个**！于是，之前的式子可以写为：

$$f(x)=\arg\max_{y\in\mathcal{Y}}\sum^K_{i=1}L(c_i,f(x))P(c_i|X=x)$$

而我们知道当 $Y \neq f(X)$ 时，损失记为 1，反之为 0，也就意味着只有当 $y\neq c_i$ 时才会有损失值，否则都为 0，这样就可以对这个函数进行简化：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220730104048781.png" alt="image-20220730104048781" style="zoom:80%;" /></center>

因此上式中的第三行等式去掉了求和符号，意味着对 $P(y=c_1|X=x),P(y=c_2|X=x),\dots,P(y=c_K|X=x)$ 计算后，找到其中的最大值，记为 $P(y=c_i|X=x)$，**此时所得到的 $c_i$ 就是通过朴素贝叶斯法求出的类**。

至此，就实现了**将期望风险最小化转为后验概率最大化**，这就是朴素贝叶斯法的判断准则。