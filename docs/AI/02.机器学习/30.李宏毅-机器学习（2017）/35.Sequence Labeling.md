---
title: Sequence Labeling
date: 2022-11-13 15:26:29
permalink: /pages/ml/lhy/sequence-labeling/
categories:
  - AI
  - 机器学习
  - 李宏毅-机器学习（2017）
tags:
  - 
---

## 1. Sequence Labeling Problem

Sequence Labeling 的 problem 是什么呢？它也是一个 seq2seq 的问题：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117155714244.png" alt="image-20221117155714244" style="zoom:50%;" />

RNN 其实也可以解这个问题，但是还存在其他的基于 structured learning 的方法（two steps, three problems）。

今天用 POS tagging 任务（词性标注任务）来作为例子进行讲解。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117160243786.png" alt="image-20221117160243786" style="zoom:72%;" /></center>

POS tagging 对于接下来的 syntactic parsing、word sense disambiguation 等任务都是很有用的。

Outline：

+ Hidden Markov Model（HMM）
+ Conditional Random Field（CRF ）
+ Structured Perceptron/SVM
+ Towards Deep Learning

## 2. Hidden Markov Model (HMM)

### 2.1 HMM 的 assumption

How do you generate a sentence? HMM 做了一个 assumption：先产生一个 POS sequence，在由这个 POS seq 去产生一个 sentence：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117161700358.png" alt="image-20221117161700358" style="zoom:67%;" /></center>

下面看一下这两个步骤：

:arrow_forward: **Step 1: Generate a POS sequence based on the grammar**

HMM 假设这个 grammar 就是 Markov Chain：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117162107620.png" alt="image-20221117162107620" style="zoom:50%;" /></center>

根据这个 Markov Chain，一个 POS sequence 的概率就可以这么算：

$$P("PN \ V \ D \ N") = 0.4 \times 0.8 \times 0.25 \times 0.95 \times 0.1$$

:arrow_forward: **Step 2: Generate a sentence based on the POS sequence**

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117164128139.png" alt="image-20221117164128139" style="zoom:60%;" /></center>

HMM 做的事情就是在帮助我们描述我们是怎样说出一句话的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117164417415.png" alt="image-20221117164417415" style="zoom:67%;" /></center>

这些概率的计算也都是蛮单纯的。我们用更一般化的方式来描述一下这些概率的计算：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117164650876.png" alt="image-20221117164650876" style="zoom:67%;" /></center>

### 2.2 如何计算这些概率？

接下来的问题就是，我们要怎么算出这些概率呢？我们首先需要收集一大堆的 training data：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117164815692.png" alt="image-20221117164815692" style="zoom:67%;" /></center>

有了这些 training data，这些概率就很好计算了：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117165403220.png" alt="image-20221117165403220" style="zoom:67%;" /></center>

+ 所以就是通过 count 来计算 probabilities，So simple！

> 在语音处理中使用 HMM 时，这里的计算其实要用到 EM 算法来做，但在 POS tagging 中，只需要 count 就好啦，这是没有问题的。
>
> 出现这种差别的原因在于，在一般 HMM 模型中，训练样本对应着的是一个可见的观测样本，一个隐状态序列，这个隐马尔可夫链的状态并不能像这个例子一样可以显性的写出来，因此需要 EM 算法来联合确定。 

### 2.3 HMM 的 inference

有了上面的概率，那我们该如何做 POS Tagging 呢？别忘了我们可以计算 P(x, y)，所以：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221117170216243.png" alt="image-20221117170216243" style="zoom:67%;" /></center>

这里的求解 $y = {\arg\max}_{y \in \mathbb{Y}}P(x, y)$ 可以遍历所有可能的 y 取值，然后代入到 $P(x, y)$ 中，看看哪个计算出来的概率最大。

但这样的计算真的需要我们去遍历所有可能的 y 吗？假如我们是通过遍历 y 来做的话：

+ 假如有 |S| 个 tags，并且 sequence y 的长度是 L
+ 那么就一共有 $|S|^L$ 中可能的 y

幸运的是，我们有 Viterbi Algorithm，它可以把上面的问题在复杂度 $O(L|s|^2)$ 的情况下解出来。如果不知道这个算法也没关系，就可以把它当成是一个 function，你告诉它这个 $P(x, y)$ 怎么算，然后你 call 这个 function 后，就可以知道哪个 y 可以让这个 $P(x, y)$ 最大。

### 2.4 Summary

HMM 其实也是 structured learning 的一种方法，而我们讲过，structured learning 就是要回答三个 problem，那这个 HMM 是如何回答这三个 problem 的呢？

+ **Problem 1: Evaluation**
  + HMM 中的 evaluation function 就是 x 与 y 的 joint probablties：$F(x, y)=P(x, y)=P(y)P(x|y)$
+ **Problem 2: Inference**
  + 就是要解 $\tilde{y} = \arg \max_{y\in \mathbb{Y}}P(x, y)$，HMM 中就是用的 Viterbi algorithm
+ **Problem 3: Training**
  + $P(y)$ and $P(x|y)$ can be simply obtained from training data

### 2.5 Drawbacks



