---
title: 基础概念
date: 2022-06-01 22:25:01
permalink: /pages/ml/AndrewNg/basic-concept
categories:
  - AI
  - 机器学习
  - 吴恩达-机器学习
tags:
  - 
---

## 1. 监督学习与无监督学习

+ **监督学习**是已经知道数据的 label，比如预测房价问题，给出了房子的面积和价格。
  + 回归问题是预测连续值的输出，例如预测房价。
  + 分类问题是预测离散值输出，例如判断肿瘤是良性还是恶性。
+ **无监督学习**是没有数据的 label，比如对于分类问题无监督学习可以得到多个不同的聚类，从而实现预测的功能。

## 2. 代价函数

之后的课程对符号做如下约定：

+ $m$ 表示训练样本的数目
+ $x$ 表示特征/输入变量
+ $y$ 代表目标变量/输出变量
+ $(x, y)$ 代表训练集中的实例
+ $(x^{(i)}, y^{(i)})$ 代表第 $i$ 个观察实例
+ $h$ 代表学习算法的解决方案或函数，也称为假设（hypothesis）

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/ad0718d6e5218be6e6fce9dc775a38e6.png" alt="img" style="zoom:80%;" /></center>

上图是一个监督学习算法的工作方式，把训练集喂给学习算法后输出一个函数，通常表示用 $h$ 来表示，$h$ 代表 **hypothesis**（**假设**），它是一个函数，输入是房屋尺寸大小（$x$），输出是预测的价格（$y$）。

> 也许 hypothesis 这个名字不是很合适，但一开始就是这么叫的，所以也就流传下来了。

对于我们的房价预测问题，该如何表达 $h$ 呢？一种可能的表达方式为：$h_\theta(x) = \theta_0 + \theta_1x$，因为只含有一个特征/输入变量，因此这样的问题叫作**单变量线性回归问题**。

### 2.1 什么是 cost function？

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/d385f8a293b254454746adee51a027d4.png" alt="img" style="zoom:80%;" /></center>

在线性回归中我们有一个像这样的训练集，训练样本数量 m=47，而我们的 hypothesis function 形式是这样：$h_\theta(x) = \theta_0 + \theta_1x$。接下来我们要做的是为模型选择合适的 **parameters** $\theta$，在这个例子中就是直线的斜率 $\theta_1$ 和 y 轴上的截距 $\theta_0$。

我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/6168b654649a0537c67df6f2454dc9ba.png" alt="img" style="zoom:80%;" />

我们的目标便是选择出可以使得建模误差能够最小的模型参数，也就是使得代价函数 $J(\theta)$ 最小化，可以写作 $minimize_{\theta}J(\theta)$，其中 $J(\theta)=J(\theta_0, \theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$，它也是 square error function。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/10ba90df2ada721cf1850ab668204dc9.png" alt="img" style="zoom:80%;" /></center>

我们绘制一个等高线图，三个坐标分别是 $\theta_0$、$\theta_1$ 和 $J(\theta_0,\theta_1)$：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/27ee0db04705fb20fab4574bb03064ab.png" alt="img" style="zoom: 80%;" />

在一个固定数据集 $X$ 中，每一个 parameter $\theta$ 确定一个 hypothesis function $h(\theta)$，进而确定了一个 cost function $J(\theta)$，上图中的点是在这个数据集 $X$ 上通过函数 $J(\theta)$ 计算出来的一个 error value。

### 2.2 cost function 的直观理解

将上面三维空间的图画成等高线图可以表示为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/86c827fe0978ebdd608505cd45feb774.png" alt="img" style="zoom:80%;" /></center>

我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数 $J$ 取最小值的参数 $\theta_0$ 和 $\theta_1$ 来。

## 3. 梯度下降

### 3.1 批量梯度下降

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/13176da01bb25128c91aca5476c9d464.png" alt="img" style="zoom:72%;" /></center>

+ $\alpha$ 表示学习率

> 我们使用 `:=` 表示 Assignment，`=` 表示 Assert Truth。

对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220602104500338.png" alt="image-20220602104500338" style="zoom:67%;" /></center>

则算法改写成：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220602104536047.png" alt="image-20220602104536047" style="zoom:67%;" /></center>

刚刚使用的算法称为**批量梯度下降**，指的是<u>在梯度下降的每一步中，我们都用到了所有的训练样本</u>。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。

### 3.2 multi-feature 的梯度下降

我们对以下符号做出约定：

+ $n$：feature 的数量
+ $x^{(i)}$：代表第 i 个训练实例，是 feature matrix 的第 i 行，是一个 vector
+ $x_j^{i}$：代表 feature matrix 的第 i 行的第 j 个特征，也就是第 i 个训练实例的第 j 个特征

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/591785837c95bca369021efa14a8bb1c.png" alt="img" style="zoom:67%;" /></center>

支持 multi-feature 的 hypothesis $h$ 可以表示为：$h_\theta(x)=\theta_0x_0+\theta_1x_1+\dots+\theta_nx_n$，其中 $x_0 \equiv 0$。此时模型的参数是一个 n+1 维的 vector，任何一个训练实例也都是 n+1 维的 vector，feature matrix $X$ 的维度是 $m \times (n+1)$，因此公式可以简化为：$h_\theta(x) = \theta^TX$。

在多变量线性回归中，cost function 也可以写成 $J(\theta_0,\dots,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$，此时多变量线性回归的批量梯度下降算法为：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/41797ceb7293b838a3125ba945624cf6.png" alt="img" style="zoom:80%;" /></center>

求导的步骤计算后即：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/dd33179ceccbd8b0b59a5ae698847049.png" alt="img" style="zoom: 67%;" /></center>

我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。

### 3.3 Feature Scaling

**Idea**: <u>Make sure features are on a similar scale</u>.

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20200927122159371.png" alt="image-20200927122159371" style="zoom:60%;" /></center>

最简单的方法是令 $x_n=\frac{x_n-\mu_n}{s_n}$，其中 $\mu_n$ 是平均值，$s_n$ 是标准差。

### 3.4 Learning Rate

+ If $\alpha$ is too small: slow convergence.
+ If $\alpha$ is too large: $J(\theta)$ may not decrease on every iteration; may not converge.

To choose $\alpha$, try $\dots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \dots$

