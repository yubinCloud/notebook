---
title: 协同过滤
date: 2022-07-20 16:55:18
permalink: /pages/ml/AndrewNg/collaborative-filtering/
categories:
  - AI
  - 机器学习
  - 吴恩达-机器学习
tags:
  - 
---

## 1. Problem Formulation

本章主要讲一下推荐系统（Recommender Systems），它是机器学习中的一个重要的应用，在学术界也具有一定的份额。借助推荐系统，我们还将领略一下特征学习的思想，即不手动设计 feature 而是采用一个算法来进行学习。

我们从一个例子开始定义推荐系统的问题。假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220720185838047.png" alt="image-20220720185838047" style="zoom:67%;" /></center>

前三部电影是爱情片，后两部则是动作片，我们可以看出 Alice 和 Bob 更倾向于爱情片，而 Carol 和 Dave 更倾向于动作片，并且没有一个用户给所有的电影都打过分。**我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据**。

下面先引入一些标记：

+ $n_u$：用户的数量
+ $n_m$：电影的数量
+ $r(i,j)$：如果用户 j 给电影 i 评过分，则 $r(i,j)=1$
+ $y^{(i,j)}$：代表用户 j 给电影 i 的评分
+ $m_j$：代表用户 j 评过分的电影的总数

## 2. 基于内容的推荐系统

在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。

在我们的例子中，我们可以假设每部电影都有两个 feature，$x_1$ 代表电影的浪漫程度，$x_2$ 代表电影的动作程度：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220720220144087.png" alt="image-20220720220144087" style="zoom:80%;" />

每部电影都有一个 feature vector，如第一部电影 $x^{(1)} = [0.9,  0]^T$。

下面我们要基于这些特征来构建一个推荐系统算法。假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如 $\theta^{(1)}$ 是第一个用户的模型的参数。先定义一下几个符号：

+ $\theta^{(j)}$ 是用户 j 的参数向量，$\theta^{(j)} \in \mathcal{R}^{n+1}$，n 是电影的 number of feature，在上例中是 n=2。
+ $x^{(i)}$ 是电影 i 的特征向量，$x^{(i)} \in \mathcal{R}^{n+1}$，n 是电影的 number of feature，在上例中是 n=2，加 1 是加了个偏置 1，即 $x^{(i)}_0 \equiv 0$。

**预测 user $j$ 会给 movie $i$ 打分：$(\theta^{(j)})^T x^{(i)}$**。

比如上例中，$x^{(3)} = [1, 0.99, 0]^T$，并假设 $\theta^{(1)}=[0,5,0]^T$，那么 Alice 对 *Cute puppies of love* 这部电影的打分可以预测为 $(\theta^{(1)})^T x^{(3)} = 5 \times 0.99 = 4.95$，这是合理的。

我们把问题正式的表达出来：为了 learn $\theta^{(j)}$，我们是要寻找 $\theta^{(j)}$ 能够最小化下面这个式子：

$$\frac{1}{2m^{(j)}} \sum_{i: r(i,j)=1}[(\theta^{(j)})^T x^{(i)} - y^{(i,j)}]^2 + \color{blue}{\frac{\lambda}{2m^{(j)}} \sum^n_{k=1}(\theta_k^{(j)})^2}$$

+ 式子的后半部分是 regularization
+ 前后两部分系数中都有 $m^{(j)}$，因此它可以作为一个常数而去掉。 

所以，Optimization objective 可以写成下面这样：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220720222010471.png" alt="image-20220720222010471" style="zoom:70%;" /></center>

下面的就是将所有 user 的参数一块算，用 gradient descent 来做的话就是：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220720223900528.png" alt="image-20220720223900528" style="zoom: 80%;" /></center>

上面所介绍的算法就是<mark>基于内容的推荐算法</mark>，因为我们假设变量是已有的，即我们有描述电影内容的特征量，比如这个电影的爱情程度怎么样。但是对于许多电影来说，我们并没有或者很难获取这些的特征量。

下一节我们将介绍不是基于内容的算法，即不去假设我们已经得到这些所有的电影的特征。

## 3. 协同过滤算法

在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。**相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征**。

假设我们知道一个用户的喜好，他比较喜欢爱情片，那他的 $\theta^{(1)} = [0,5,0]^T$，其余类似。对于电影 j 的 feature vector $x^{(j)}$，我们希望的是 $(\theta^{(i)})^T x^{(j)}$ 的值与 $y^{(i,j)}$ 越接近越好：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220720234027758.png" alt="image-20220720234027758" style="zoom:67%;" />

优化过程可以写成如下：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220720234146360.png" alt="image-20220720234146360" style="zoom:75%;" /></center>

+ 下面是用于计算所有电影的 feature vector 的式子。

我们先看一个基本的 Collaborative filtering 算法：

+ **Given $x^{(1)}, \dots, x^{(n_m)}$ and movie ratings, we can estimate $\theta^{(1)}, \dots, \theta^{(n_u)}$**
+ **Given $\theta^{(1)}, \dots, \theta^{(n_u)}$, we can estimate $x^{(1)},\dots,x^{(n_m)}$**

所有我们可以先随机地猜取一些 $\theta$ 值，然后就可以估计出 $x$，再用这些 $x$ 可以计算出更好的 $\theta$… 来回重复地迭代计算，最终将得到一个收敛的结果，这个结果就会是一组合理的电影特征以及一组对不同用户的参数的合理估计：

$$\theta \to x \to \theta \to x \to \dots$$

这是一个基本的协同过滤算法，但不是我们要最终应用的。下面我们来改进这个算法。

考虑如果我们既没有用户的参数，也没有电影的特征，那上面提到的两种方法都不可行了。协同过滤算法可以同时学习这两者。

我们的优化目标便改为同时针对 $x$ 和 $\theta$ 进行：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220720235543400.png" alt="image-20220720235543400" style="zoom:80%;" /></center>

+ 可以看到，紫色部分都一样，因此可以合并
+ 其实，当固定 $\theta$ 或 $x$ 的其中一个来估计另一个时，所做的优化过程其实与最下面这个合起来的式子做的事情是一样的，因为一旦固定一个，这个合起来的式子总会有一项的值为常数。

这里与前面的区别在于，$\theta$ 和 $x$ 都是 $\in R^n$，因为我们把之前向量中的第 0 个元素的偏置给去掉了，因为如果需要这个偏置，算法会自动学习出来。

::: theorem Collaborative filtering algorithm
1. Initialize $x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}$ to small random values.
2. Minimize $J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)})$ using gradient descent (or an advanced optimization algorithm). E.g. for every $j=1, \dots, n_u; i = 1, \dots, n_m$:

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220721000332891.png" alt="image-20220721000332891" style="zoom:80%;" /></center>

3. For a user with parameters $\theta$ and a movie with (learned) features $x$, predict a star rating of $\theta^T x$.
:::

上面介绍的就是 Collaborative filtering 算法，它可以同时算出所有电影的 feature vector 和用户的 params vector，并能预测出用户未评价部分的分数。

