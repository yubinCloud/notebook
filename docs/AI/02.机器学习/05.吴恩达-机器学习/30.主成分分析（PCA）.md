---
title: 主成分分析（PCA）
date: 2022-07-18 18:26:39
permalink: /pages/ml/AndrewNg/PCA/
categories:
  - AI
  - 机器学习
  - 吴恩达-机器学习
tags:
  - 
---

主成分分析（PCA）是一种数据压缩的算法，他将数据压缩到 K 维度，并使得所有数据投影到新维度的距离最小。

本章学习第二种类型的无监督学习问题，称为 <mark>Dimensionality Reduction</mark>（降维）。主成分分析（PCA）是一种数据压缩的算法，他将数据压缩到 K 维度，并使得所有数据投影到新维度的距离最小。

## 1. Motivation

### 1.1 Motivation 1：Data Compression

什么是 Dimensionality Reduction？用一个例子来解释，我们收集的数据集有两个 feature：$x_1$ 是长度，用厘米表示；$x_2$ 是用英寸表示同一物体的长度。所以这里的“长度”被冗余表示了，这两个基本的长度度量，也许我们想要做的是减少数据到一维，只用一个数测量这个长度：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220718195536164.png" alt="image-20220718195536164" style="zoom:67%;" /></center>

图中由于 inches 和 cm 两个单位在度量同一物体时出于四舍五入的精度问题，导致可能不完全是线性的。我们希望通过 Dimensionality Reduction，将原来 2D 的 $\boldsymbol{x}$ 转换成了 1D 的 $\boldsymbol{z}$。

从这件事情我看到的东西发生在工业上的事：有时可能有几个不同的工程团队，也许一个工程队给你二百个特征，第二工程队给你另外三百个的特征，第三工程队给你五百个特征，**一千多个特征都在一起来描述一个 object 时，很有可能存在一些特征是高度冗余的**。

我们再看一个将 3D 的 feature vector 降至 2D feature vector 的例子：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220718200257266.png" alt="image-20220718200257266" style="zoom:60%;" /></center>

在最左边的图中，这些样本点虽然处于三维空间中，但实际上大部分都处于一个平面，因此可以进行降维。

这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将 1000 维的特征降至 100 维。

### 1.2 Motivation 2：Visualization

在许多机器学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。

假使我们有有关于许多不同国家的数据，每一个特征向量都有 50 个特征（如 GDP，人均 GDP，平均寿命等）。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220719012805280.png" alt="image-20220719012805280" style="zoom: 90%;" /></center>

这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。

## 2. PCA Problem

### 2.1 PCA Problem Formulation

主成分分析（PCA）是最常见的降维算法。

PCA 中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射误差的平方和能尽可能地小：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220719013647437.png" alt="image-20220719013647437" style="zoom:67%;" /></center>

+ <mark>方向向量</mark>是一个经过原点的 vector。
+ <mark>投射误差</mark>（**Projected Error**）也就是投影距离，是从样本点向该方向向量作垂线的长度。

下面给出主成分分析问题的描述：

**问题**是要将 n 维数据降至 k 维；**目标**是找到 vectors $u^{(1)}, u^{(2)}, \dots, u^{(k)}$ 使得 projected error 的平方和最小。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220719013854556.png" alt="image-20220719013854556" style="zoom: 80%;" />

### 2.2 PCA  v.s.  Linear Regression

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220719014431797.png" alt="image-20220719014431797" style="zoom:67%;" /></center>

+ Linear Regression 的目的是预测结果，而 PCA 不做任何预测
+ 左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）