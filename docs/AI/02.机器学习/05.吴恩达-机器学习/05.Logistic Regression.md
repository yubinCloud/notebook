---
title: Logistic Regression
date: 2022-06-02 14:52:51
permalink: /pages/ml/AndrewNg/logistic-regression
categories:
  - AI
  - 机器学习
  - 吴恩达-机器学习
tags:
  - 
---

## 1. Classification Problem

在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。我们从二元的分类问题开始讨论。

我们将因变量（**dependent variable**）可能属于的两个类分别称为负向类（**negative class**）和正向类（**positive class**），则 dependent var $y \in \{0, 1\}$，其中 0 表示负向类，1 表示正向类。

**为什么不能用 Linear Regression 的 hypothesis 来解 classification 问题**？对于 classification，y range from 0 or 1，但如果使用 Linear Regression，那 hypothesis func 的 output 可能远大于 1 或 远小于 0，而 label 取值 0 或 1，这样就会感觉很奇怪。因此我们需要研究 Logistic Regression，其 output 永远在 0~1 之间，它适用于 label y 取值离散的情况，比如 0 1 1 0。

> Logistic Regression 是一个 classification 算法，不要被名字里面的 “regression” 给迷惑了。

## 2. Hypothesis Representation

本节展示 logistic regression 的 hypothesis func 表达式。我们希望它的值在 0 和 1 之间，此时就有了：

+ 当 $h_\theta(x) \ge 0.5$ 时，预测 $y=1$；
+ 当 $h_\theta(x) \lt 0.5$ 时，预测 $y=0$。

Logistic Regression 模型的 hypothesis 是： 

$$h_\theta(x) = g(\theta^TX)$$

其中 X 代表特征向量，g 代表 **logistic function**，它也是 sigmoid function，公式为 $g(z)=\frac{1}{1+e^{-z}}$，图像为：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1073efb17b0d053b4f9218d4393246cc.jpg" alt="img" style="zoom:67%;" />

$h_\theta(x)$ 的作用是：<u>对于给定的 input x，根据选择的参数 $\theta$ 计算出 output = 1 的可能性</u>，即 $h_\theta(x)=P(y=1|x;\theta)$。例如给定 x，计算 $h_\theta(x)=0.7$，那表示有 70% 的概率 y 为 positive class，30% 的概率为 negative class。

## 3. Decision Boundary

在 Logistic Regression 中，我们预测：

+ 当 $h_\theta(x) \ge 0.5$ 时，预测 $y=1$；
+ 当 $h_\theta(x) \lt 0.5$ 时，预测 $y=0$。

 根据 logistic function 的 S 型函数图像，我们知道：

+ z = 0 时 g(z) = 0.5
+ z > 0 时 g(z) > 0.5
+ z < 0 时 g(z) < 0.5

又 $z=\theta^Tx$，即：$\theta^Tx \ge 0$ 时预测 y=1，$\theta^Tx \lt 0$ 时预测 y=0。这样此时 Decision Boundary 是坐标轴的竖直方向的轴。

现在假设我们又有一个模型：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/58d098bbb415f2c3797a63bd870c3b8f.png" alt="img" style="zoom:67%;" /></center>

现在假设参数 $\theta$ 是 vector $[-3,1,1]^T$，那么当 $-3+x_1+x_2 \ge 0$ 时，即 $x_1+x_2 \ge 3$ 时，模型将预测 y=1，这时我们绘制直线 $x_1+x_2=3$，这条线便是我们模型的分界线，将预测为 1 的区域和预测为 0 的区域分隔开：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/f71fb6102e1ceb616314499a027336dc.jpg" alt="img" style="zoom:67%;" />

对于更复杂的数据分布情况，z 可能需要是二次方特征来划分出 Decision Boundary：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/197d605aa74bee1556720ea248bab182.jpg" alt="img" style="zoom:67%;" />

<u>我们可以用非常复杂的模型来适应非常复杂形状的判定边界</u>。

## 4. Cost Function

本节介绍如何拟合 logistic regression 的 param $\theta$，具体来说就是定义用来拟合 param 的优化目标，或者叫 cost function。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/f23eebddd70122ef05baa682f4d6bd0f.png" alt="img" style="zoom:80%;" /></center>

对于线性回归模型中，我们定义的代价函数是所有模型误差的平方和。理论上我们对 logistic regression 也可以沿用这个定义，但问题在于将 logistic regression 的 $h_{\theta}(x)$ 代入这样的 cost function 后会得到一个非凸函数（**non-convex function**），从而难以通过 gradient descent 找到 global minima，如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/8b94e47b7630ac2b0bcb10d204513810.jpg" alt="img" style="zoom:75%;" />

我们先看一下 logistic regression 的 loss function（error function），它可以用来衡量算法的运行情况：

$$L(\hat{y}, y) = -(y \log{\hat{y}} + (1-y)\log(1-\hat{y}))$$

+ if y=1, $L(\hat{y}, y)=-\log\hat{y}$, then you want $\hat{y}$ large.
+ if y=0, $L(\hat{y}, y)=-\log(1-\hat{y})$, then you want $\hat{y}$ small.

**loss function 是在单个 training example 上定义的，它衡量了你在单个 training example 上的表现。而 cost function 衡量的是在全体 training example 上的表现**。这样我们可以定义 cost function $J$：

$$J(w, b)=\frac{1}{m} \sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})$$

在得到这样一个 cost function 以后，我们便可以用 gradient descent 算法来求得能使 cost function 最小的 param 了。

To fit parameters $\theta$, we need $\min_\theta J(\theta)$。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220607201529507.png" alt="image-20220607201529507" style="zoom:67%;" /></center>

## 5. 将 logistic regression 用于多类别分类

本节主要谈如何使用 logistic regression 来解决多类别分类问题，具体来说是通过一个叫做“一对多”（**one-vs-all**）的分类算法。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/54d7903564b4416305b26f6ff2e13c04.png" alt="img" style="zoom:80%;" /></center>

我用3种不同的符号来代表 3 个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？使用逻辑回归可以将数据集一分为二为正类和负类，而下面这个叫做“one-vs-all”或“one-vs-rest”的方法可用于多分类。

现在我们有一个训练集，好比上图表示的有 3 个类别。先从用三角形代表的类别 1 开始，我们可以创建一个新的"伪"训练集，类型 2 和类型 3 定为负类，类型 1 设定为正类，然后可以拟合出一个合适的分类器：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/b72863ce7f85cd491e5b940924ef5a5f.png" alt="img" style="zoom: 50%;" />

这里的三角形是正样本，而圆形代表负样本，这样可以训练一个标准的逻辑回归分类器，我们将这个 y=1 标为正向类而训练的 model 记作 $h_\theta^{(1)}(x)$。以此类推，将 y=2 记为正向类而训练的 model 记作 $h_\theta^{(2)}(x)$。最后就可以得到一系列的模型：$h_\theta^{(i)}(x)$，其中 $i=(1,2,3,\dots,k)$。

在我们训练完这些分类器后，为了做出预测，给输入一个新的 x 值，我们要做的就是在我们三个分类器里面分别输入 x，然后我们选择一个让 $h_\theta^{(i)}(x)$ 最大的 $i$。

通过这个小方法，现在也可以将逻辑回归分类器用在多类分类的问题上了。