---
title: 数据复制（冗余）
date: 2023-01-15 15:08:00
permalink: /pages/DDIA/note/replication/
categories:
  - 数据科学
  - 数据密集型应用系统设计
  - 读书笔记
tags:
  - 
---

本书第一部分讲单机数据系统，现在进入的第二部分讲多机数据系统。

[[toc]]

<mark>冗余</mark>（**Replication**）是指将同一份数据复制多份，放到通过网络互联的多个机器上去。其好处有：

1. **降低访问延迟**：可以在地理上同时接近不同地区的用户。
2. **提高可用性**：当系统部分故障时仍然能够正常提供服务。
3. **提高读吞吐**：扩展至多台机器以同时提供数据访问服务。

> 本章假设我们的数据系统中所有数据能够存放到一台机器中，则本章只需考虑多机冗余的问题，而不考虑分片的问题。

<u>如果数据是只读的，则冗余很好做，直接复制到多机即可</u>。我们有时可以利用这个特性，使用分治策略，将数据分为只读部分和读写部分，则只读部分的冗余就会容易处理的多，甚至可以用 EC 方式做冗余，减小存储放大的同时，还提高了可用性。

> - 想想 EC 牺牲了什么？以计算换存储。

但<u>难点就在于，数据允许数据变更时，如何维护多机冗余且一致</u>。常用的冗余控制算法有：

1. single leader，主从复制
2. multi-leader，多主节点复制
3. leaderless，无主节点复制

这需要在多方面做取舍：

+ 使用同步复制还是异步复制
+ 如何处理失败的副本

数据库冗余问题在学术界不是一个新问题了，但在工业界，大部分人都是新手——分布式数据库是近些年才大规模的在工业界落地的。

## 1. 主节点与从节点

冗余存储的每份数据称为<mark>副本</mark>（**replica**）。多副本所带来的最主要的一个问题是：如何保证所有数据被同步到了所有副本上？

**leader-based replication**（<mark>主从复制</mark>）的同步算法，是最常用解决办法。

1. 其中一个 replica 称为 leader，也称为**主副本**（primary、master）。主副本作为写入的协调者，所有写入都要发给主副本。
2. 其他 replica 称为 follower，也称为**只读副本**（read replicas）、**从副本**（slaves）、**次副本**（secondaries）、**热备**（hot-standby）。主副本将改动写到本地后，将其发送给各个从副本，从副本收变动到后应用到自己状态机，这个过程称为**日志同步**（replication log）、**变更流**（change steam）。
3. 对于读取，客户端可以从主副本和从副本中读取；但写入，客户端只能将请求发到主副本。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230115152251991.png" alt="image-20230115152251991" style="zoom: 50%;" />

> leader 也被称为**主节点**，follower 也被称为**从节点**，因此这种算法也被称为**主从复制**。不论名称叫什么，关键是理解其中的思想。根据习惯，下面通称主副本和从副本。

有很多数据系统都用了此模式：

1. 关系型数据库：PostgreSQL（9.0+）、MySQL 和 Oracle Data Guard 和 SQL Server 的 AlwaysOn
2. 非关系型数据库：MongoDB、RethinkDB 和 Espresso
3. 消息队列：Kafka 和 RabbitMQ。

### 1.1 同步复制与异步复制

**同步（synchronously）复制**和**异步（asynchronously）复制**和关键区别在于：请求何时返回给客户端。

+ 如果等待某副本写完成后，则该副本为<mark>同步复制</mark>。
+ 如果不等待某副本写完成，则该副本为<mark>异步复制</mark>。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230115152800522.png" alt="image-20230115152800522" style="zoom: 50%;" />

两者的对比：

+ 同步复制牺牲了响应延迟和部分可用性（在某些副本有问题时不能完成写入操作），换取了**所有副本的一致性**（但并不能严格保证）
+ 异步复制放松了一致性，而换来了**较低的写入延迟**和**较高的可用性**。

在实践中，会根据对一致性和可用性的要求，进行取舍。针对所有从副本来说，可以有以下选择：

1. **全同步**：所有的从副本都同步写入。如果副本数过多，可能性能较差，当然也可以做并行化、流水线化处理。
2. **半同步**：（**semi-synchronous**），有一些副本为同步，另一些副本为异步。
3. **全异步**：所有的从副本都异步写入。网络环境比较好的话，可以这么配置。

> 异步复制可能会造成副本丢失等严重问题，为了能兼顾一致性和性能，学术界也在不断研究新的复制方法。如，**链式复制（chain-replication）**。
>
> 多副本的一致性和共识性有诸多联系，本书后面章节会讨论。

### 1.2 配置新的从节点

> 其实，“节点”往往是在说提供 service 的机器，而“副本”是在指逻辑上的数据集。但在文中讲解时，两者会混用。

如果原副本是只读（read-only）的，只需要简单拷贝即可。但是如果是可写副本，则问题要复杂很多。因此，比较简单的一种解决方法是：**禁止写入，然后拷贝**。这在某些情况下很有用，比如夜间没有写入流量，同时一晚上肯定能复制完。但这会违反高可用的设计目标。

但如果要求不停机，则需要另一种解决方法：

1. 主副本在本地做**一致性快照**。何谓一致性？
2. 将快照复制到从副本节点。
3. 从主副本拉取快照之后的操作日志，应用到从副本，这个过程叫做**追赶**。如何知道快照与其后日志的对应关系？序列号。
4. 当从副本赶上主副本进度后，就可以正常跟随主副本了。

这个过程一般是自动化的，比如 Raft，具体操作步骤也因数据库系统而异。也可以手动化，比如写一些脚本。

### 1.3 宕机处理

系统中任何节点都可能在计划内或者计划外宕机（节点失效）。那么如何应对这些宕机情况，保持整个系统的可用性呢？我们的目标是，尽管个别节点会出现中断，但要保持系统总体的持续运行，并尽可能减小节点中断带来的影响。

#### 1.3.1 从节点宕机：追赶恢复

类似于新增从副本。如果落后的多，可以直接向主副本拉取快照+日志；如果落后的少，可以仅拉取缺失日志。

#### 1.3.2 主节点宕机：故障转移

处理相对麻烦，首先要选出新的主副本，然后要通知所有从副本变更。具体来说，包含下面步骤：

1. **确认主副本故障**。要防止由于网络抖动造成的误判。一般会用心跳探活，并设置合理超时（timeout）阈值，超过阈值后没有收到该节点心跳，则认为该节点故障。
2. **选择新的主副本**。新的主副本可以通过**选举**（共识问题）或者**指定**（外部控制程序）来产生。选主时，要保证备选节点数据尽可能的新，以最小化数据损失。
3. **让系统感知新主副本**。系统其他参与方，包括从副本、客户端和旧主副本。前两者不多说，旧主副本在恢复时，需要通过某种手段，让其知道已经失去领导权，避免**脑裂**。

主副本切换时，会遇到很多问题：

1. **新老主副本数据冲突**。新主副本在上位前没有同步完所有日志，旧主副本恢复后，可能会发现和新主副本数据冲突。
2. **相关外部系统冲突**。即新主副本，和使用该副本数据的外部系统冲突。书中举了 GitHub 数据库 MySQL 和缓存系统 redis 冲突的例子。
3. **新老主副本角色冲突**。即新老主副本都以为自己才是主副本，称为**脑裂（split brain）**。如果他们两个都能接受写入，且没有冲突解决机制，数据会丢失或者损坏。有的系统会在检测到脑裂后，关闭其中一个副本，但设计的不好可能将两个主副本都关闭调。反正需要让那些不是 leader 但还自认为是 leader 的节点意识到自己不是 leader 了。
4. **超时阈值选取**。如果超时阈值选取的过小，在不稳定的网络环境中（或者主副本负载过高）可能会造成主副本频繁的切换；如果选取过大，则不能及时进行故障切换，且恢复时间也增长，从而造成服务长时间不可用。

所有上述问题，在不同需求、不同环境、不同时间点，都可能会有不同的解决方案。因此在系统上线初期，不少运维团队更愿意手动进行切换；等积累一定经验后，再进行逐步自动化。

节点故障、不可靠网络、副本一致性、持久化、可用性和延迟之间各种细微的权衡，都是设计分布式系统时，所面临的**分布式系统核心的基本问题**。根据实际情况，对这些问题进行艺术化的取舍，便是分布式系统之美。

### 1.4 日志复制

在数据库中，基于领导者的多副本是如何实现的？在不同层次有多种方法，包括：

1. 语句层面的复制
2. 预写日志的复制
3. 逻辑日志的复制
4. 触发器的复制

对于一个**系统**来说，多副本同步的是什么？**增量修改**。

> 从另一个角度来看本节所讲述的四个方法的脉络：
>
> 具体到一个由数据库构成的**数据系统**，通常由数据库外部的**应用层**、数据库内部**查询层**和**存储层**组成。**修改**在查询层表现为：语句；在存储层表现为：存储引擎相关的预写日志、存储引擎无关的逻辑日志；修改完成后，在应用层表现为：触发器逻辑。

#### 1.4.1 基于语句的复制

主副本记录下所有更新语句：`INSERT`、`UPDATE` 或 `DELETE` 然后发给从库。主副本在这里类似于充当其他从副本的**伪客户端**。

但这种方法有一些问题：

1. **非确定性函数（nondeterministic）** 的语句可能会在不同副本造成不同改动。如 NOW()、RAND()
2. **使用自增列，或依赖于现有数据**。则不同用户的语句需要完全按相同顺序执行，当有并发事务时，可能会造成不同的执行顺序，进而导致副本不一致。
3. **有副作用**（触发器、存储过程、UDF）的语句，可能不同副本由于上下文不同（如环境变量），产生的副作用不一样。除非副作用是确定的输出。

当然也有解决办法：

1. 识别所有产生非确定性结果的语句。
2. 对于这些语句同步值而非语句。

但是 Corner Case 实在太多，需要考虑的情况太多，因此效果不好。

#### 1.4.2 基于预写日志（WAL）传输

我们发现主流的存储引擎都有**预写日志**（WAL，为了宕机恢复）：

1. 对于日志流派（LSM-Tree，如 LevelDB），每次修改先写入 log 文件，防止写入 MemTable 中的数据丢失。
2. 对于原地更新流派（B+ Tree），每次修改先写入 WAL，以进行崩溃恢复。

不管哪种情况，所有对数据库写入的字节序列都会被记入日志，也就是说，存储引擎通常都会维护一个具有如下特点的数据结构：

+ 追加写入
+ 可重放

这种结构，天然适合备份同步。本质是因为磁盘的读写特点和网络类似：磁盘是顺序写比较高效，网络是只支持流式写。具体来说，**主副本在写入 WAL 时，会同时通过网络发送对应的日志给所有从副本**。从节点收到日志后进行处理，建立和主节点内容完全相同的数据副本。**主要缺点**是日志描述的数据结果非常底层

书中提到一个数据库版本升级的问题：

1. 如果允许旧版本代码给新版本代码（应该会自然做到后向兼容）发送日志（前向兼容）。则在升级时可以先升级从库，再切换升级主库。
2. 否则（也就是要求多个节点的版本必须一致），只能进行停机升级软件版本。

#### 1.4.3 基于行的逻辑日志复制

这种在工业中用的比较多。

为了和具体的存储引擎物理格式解耦，在做数据同步时，可以使用不同的日志格式：<mark>逻辑日志</mark>。

对于关系型数据库来说，行是一个合适的粒度：

1. **对于插入行**：日志需包含所有列值。
2. **对于删除行**：日志需要包含待删除行标识，可以是主键，也可以是其他任何可以唯一标识行的信息。
3. **对于更新行**：日志需要包含待更新行的标志，以及所有列值（至少是要更新的列值）

对于多行修改来说，比如事务，可以在修改之后增加一条事务提交的记录。 MySQL 的 binlog 就是这么干的。

使用逻辑日志的**好处**有：

1. 方便新旧版本的代码兼容，更好的进行滚动升级。
2. 允许不同副本使用不同的存储引擎。
3. 允许导出变动做各种**变换**。如导出到数据仓库进行离线分析、建立索引、增加缓存等等。

之前分析过一种基于逻辑日志，统一各种数据系统的[文章](https://zhuanlan.zhihu.com/p/458683164)，很有意思。

#### 1.4.4 基于触发器的复制

前面所说方法，都是在**数据库内部**对数据进行多副本同步。

但有些情况下，可能需要用户决策，如何对数据进行复制：

1. 对需要复制的数据进行过滤，只复制一个子集。
2. 将数据从一种数据库复制到另外一种数据库。

有些数据库如 Oracle 会提供一些工具。但对于另外一些数据库，可以使用**触发器和存储过程**。触发器支持注册自己的应用层代码，使得当数据库系统发生数据更改时自动执行上述自定义代码。

基于触发器的复制，<u>开销更高</u>、<u>更容易出错</u>；但是给了用户<u>更多的灵活性</u>。

## 2. 复制滞后问题

容忍节点故障只是采用数据复制的其中一个原因，如前所述，使用多副本的好处有：

1. **可用性**：容忍部分节点故障
2. **可伸缩性**：增加读副本处理更多读请求
3. **低延迟**：让用户选择一个就近的副本访问

对于读多写少的场景，可以通过增加从副本的数量来均摊流量。但这种扩展方法有个条件：数据复制必须是异步复制。否则，同步复制情况下一个节点崩就全崩，节点越多，发生故障的概率就越高。

但若是异步复制，就会引入不一致问题：**某些副本进度可能落后于主副本**。如果应用恰好从这个落后的副本中读取数据，那应用可能会读到过期的信息。

如果此时不再有写入，经过一段时间后，多副本最终会达到一致：<mark>最终一致性</mark>。

> 这里的“最终”有点含糊不清，只说了最终，但没说什么时候算最终。实际中，网络通常比较快，副本滞后（replication lag）通常不会太久，但对于分布式系统，谁都不干打包票，在极端情况下，这个“最终”可能非常久。

这个一致性的要求其实非常低，只有对一致性没啥要求的系统才敢用最终一致性，对于这种最终一致的系统，在工程中，要考虑到由于副本滞后所带来的一致性问题。下面讲的就是复制滞后带来的魔幻问题：

### 2.1 读你所写

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230116153122987.png" alt="image-20230116153122987" style="zoom:80%;" /></center>

上图问题在于，在一个异步复制的分布式数据库里，同一个客户端，写入主副本后返回；稍后再去读一个落后的从副本，就会发现：读不到自己刚写的内容！**对用户来讲，看起来似乎是刚刚提交的数据丢失了**，显然用户不会高兴。

为了避免这种反直觉的事情发生，我们引入一种新的一致性：<mark>读写一致性</mark>（**read-after-write consistency**），或者**读你所写一致性**（**read-your-writes consistency**）。该机制保证，对于**单个客户端**来说，就一定能够读到其所写变动，但对其他用户则没有保证。也即，这种一致性是从**单个客户端**角度来看的一种因果一致性。

那么如何实现这种一致性呢？列举几种方案：

+ 【按内容分类】对于客户端可能修改的内容集，**只从主副本读取**。如社交网络上的个人资料，读自己的资料时，从主副本读取；但读其他人资料时，可以向从副本读。
+ 【按时间分类】如果每个客户端都能访问基本所有数据，则方案一就会退化成所有数据都要从主副本读取，这显然不可接受。此时，可以按时间分情况讨论，**近期内有过改动的数据，从主副本读，其他的，向从副本读**。像那种过去了一天的数据，应该可以假定已经同步完了，因此可以大胆地去从节点中拿。那这个区分是否最近的**时间阈值**（比如一分钟）如何选取呢？可以监控从副本一段时间内的最大延迟这个经验值，来设置。
+ 【利用时间戳】客户端记下本客户端上次改动时的时间戳，在读从副本时，利用此时间戳来看某个从副本是否已经同步了改时间戳之前内容。可以在所有副本中找到一个已同步了的；或者阻塞等待某个副本同步到改时间戳后再读取。时间戳可以是逻辑时间戳（例如序列号），也可以是物理时间戳（此时多机时钟同步非常重要）。

会有一些实际的复杂 case：

+ **数据分布在多个物理中心**。所有需要发送给主副本的请求都要首先路由到主副本所在的数据中心。
+ **一个逻辑用户有多个物理客户端**。此时要提供跨设备的写后读一致性。比如一个用户通过电脑、手机多终端同时访问，此时就不能用设备 id，而需要使用用户 id，来保证用户角度的读写一致性。但不同设备有不同物理时间戳，不同设备访问时可能会路由到不同数据中心。

### 2.2 单调读

异步复制可能带来的另外一个问题：对于一个客户端来说，系统可能会发生**时光倒流**（moving backward in time）。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230116163320539.png" alt="image-20230116163320539" style="zoom:75%;" /></center>

于是，我们再引入一种一致性保证：<mark>单调读一致性</mark>（Monotonic reads）。单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况。

- 读写一致性和单调读有什么区别？ 读你所写保证的是写后读顺序，单调读保证的是**多次读**之间的顺序。

如何实现单调读？

+ 确保每个用户总是从固定的同一副本执行读取。
+ 前面提到的时间戳机制。

### 2.3 前缀一致读

异步复制所带来的第三个问题：有时候会违反因果关系。

本质在于：如果数据库由多个分区（Partition）组成，而**分区间的事件顺序无法保证**。此时，如果有因果关系的两个事件落在了不同分区，则有可能会出现**果在前，因在后**。

> 在一个 partition 内，往往是可以保证数据写入的时间逻辑顺序，但在多个 partition 中却一般不太能保证这种顺序，比如 kafka 的不同 partition 之间就很难保证消息交付顺序，从而出现一些魔幻的现象。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20230116164100900.png" alt="image-20230116164100900" style="zoom:75%;" /></center>

如上图的例子，在 Observer 看来，是 Cake 夫人先回答了问题，Poon 先生才提出了问题，产生了奇怪的逻辑混乱。

为了防止这种问题，我们又引入了一种一致性：<mark>前缀一致读</mark>（consistent prefix reads）。额… 这个名字比较奇怪。

实现这种一致性保证的方法：

1. 不分区。
2. 让所有有因果关系的事件路由到一个分区。但如何追踪因果关系是个难题。

### 2.4 副本滞后的终极解决方案

事务！！

多副本异步复制所带来的一致性问题，都可以通过**事务**（transaction）来解决。单机事务已经存在了很长时间，但在数据库走向分布式时代，一开始很多 NoSQL 系统抛弃了事务，因为这更容易实现、有更好的性能、更好的可用性，于是复杂度被转移到了应用层。

> 所以 NoSQL 刚出现的时候，只是解决了大数据量的问题，并没有解决好用的问题，这其实是数据库刚刚进入大规模分布式（多副本、多分区）的一种妥协，在经验积累的够多之后，事务必然会被引回。

于是近年来越来越多的分布式数据库开始支持事务，是为**分布式事务**。