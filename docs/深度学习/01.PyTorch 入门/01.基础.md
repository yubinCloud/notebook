---
title: åŸºç¡€
date: 2022-01-16 17:38:57
permalink: /pages/4264b9/
categories:
  - æ·±åº¦å­¦ä¹ 
  - PyTorch å…¥é—¨
tags:
  - 
---

## 1. Tensor

å¯å‚è§å®˜ç½‘ï¼Œè¿™é‡Œåªä»‹ç»ä¸€äº›è¾ƒä¸ç†Ÿæ‚‰çš„éƒ¨åˆ†ã€‚

### shape

**dim** in PyTorch == **axis** in NumPy

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220123174701303.png" alt="image-20220123174701303" style="zoom:50%;" />

### Operators

#### â— Squeeze

> **Squeeze**: remove the specified dimension with length = 1.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220123183045139.png" alt="image-20220123183045139" style="zoom:67%;" />

+ å¦‚æœä¼ ç»™ `squueeze` çš„ dim åœ¨çŸ©é˜µä¸Š length â‰  1ï¼Œé‚£ä¹ˆä¸ä¼šåšä»»ä½•æ”¹å˜ã€‚

#### â— Unsqueeze

> **Unsqueeze**: expand a new dimension.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220123183354654.png" alt="image-20220123183354654" style="zoom:67%;" />

#### â— Cat

> **Cat**: concatenate multiple tensors.

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220123183738286.png" alt="image-20220123183738286" style="zoom: 67%;" />

## 2. Autograd

**Autograd** ä¸­æ–‡æ˜¯**è‡ªåŠ¨å¾®åˆ†**ï¼Œæ˜¯ç¥ç»ç½‘ç»œä¼˜åŒ–çš„æ ¸å¿ƒã€‚

### 2.1 å¾®åˆ†ç¤ºä¾‹

å‡è®¾ä¸€ä¸ªå‘é‡  $\vec{x} = [1, 1]^T$ ä½œä¸ºè¾“å…¥ï¼Œä¹˜ä»¥ 4 å¾—åˆ°å‘é‡ $\vec{z}$ ï¼Œæœ€åæ±‚å…¶é•¿åº¦å¾—åˆ°æ ‡é‡ $y$ï¼Œå€¼ä¸º 5.6569ï¼Œè¿™ä¸ªè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

$y = \sqrt{z_1^2+z_2^2} = \sqrt{(4x_1)^2 + (4x_2)^2} = 4 \sqrt{x_1^2+x_2^2}$

å› æ­¤ $y$ å…³äº $x_1$ çš„å¾®åˆ†æ˜¯ï¼š

$\frac{\partial y}{\partial x_1} = \frac{\partial (4 \sqrt{x_1^2+x_2^2})}{\partial x_1} \approx 2.8284$ 

å½“è¾“å…¥å’Œè®¡ç®—å˜å¾—æ›´ä¸ºå¤æ‚æ—¶ï¼ŒPyTorch çš„ Autograd æŠ€æœ¯å°±å¯ä»¥å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨å»æ±‚è¿™äº›å¾®åˆ†å€¼ã€‚

### 2.2 åŸºæœ¬åŸç†

ä¸Šé¢çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œ$\vec{x}ã€\vec{z}$ å’Œ $y$ éƒ½è¢«å½“åš**èŠ‚ç‚¹**ï¼Œè¿è¡Œè¿‡ç¨‹è¢«æŠ½è±¡ä¸º**ä¿¡æ¯æµ**ï¼Œå¤æ‚çš„è®¡ç®—ä¹Ÿå¯ä»¥è¢«æŠ½è±¡æˆä¸€å¼ **è®¡ç®—å›¾**ï¼š

 ![image-20220116182148630](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116182148630.png)

å¾®åˆ†ç¤ºä¾‹ä¸­ï¼Œ$\vec{x}$ æ˜¯å¶å­èŠ‚ç‚¹ï¼Œ$\vec{z}$ æ˜¯ä¸­é—´èŠ‚ç‚¹ï¼Œ$y$ æ˜¯è¾“å‡ºèŠ‚ç‚¹ï¼Œä»–ä»¬ä¸‰è€…éƒ½æ˜¯   Tensorã€‚

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116183156678.png" alt="image-20220116183156678" style="zoom:80%;"  />

<center>è®¡ç®—å›¾ï¼šç»¿è‰²æ˜¯å¶å­èŠ‚ç‚¹ï¼Œæ©™è‰²æ˜¯ä¸­é—´èŠ‚ç‚¹ï¼Œçº¢è‰²æ˜¯è¾“å‡ºèŠ‚ç‚¹ï¼Œè“è‰²ç®­å¤´è¡¨ç¤ºä¿¡æ¯æµ</center>

Tensor åœ¨è‡ªåŠ¨å¾®åˆ†æ–¹é¢æœ‰ä¸‰ä¸ªé‡è¦å±æ€§ï¼š

+ **requires_grad**ï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œé»˜è®¤ Falseï¼Œå½“å…¶ä¸º True æ—¶è¡¨ç¤ºè¯¥ Tensor éœ€è¦è‡ªåŠ¨å¾®åˆ†
+ **grad**ï¼šç”¨äºå­˜å‚¨ Tensor çš„å¾®åˆ†å€¼
+ **grad_fn**ï¼šç”¨äºå­˜å‚¨ Tensor çš„å¾®åˆ†å‡½æ•°

å½“å¶å­èŠ‚ç‚¹çš„ `requires_grad` ä¸º True æ—¶ï¼Œ**ä¿¡æ¯æµç»è¿‡è¯¥èŠ‚ç‚¹æ—¶ï¼Œæ‰€æœ‰ä¸­é—´èŠ‚ç‚¹çš„ `requires_grad`  å±æ€§éƒ½ä¼šå˜æˆ True**ï¼Œåªè¦åœ¨è¾“å‡ºèŠ‚ç‚¹è°ƒç”¨åå‘ä¼ æ’­å‡½æ•° `backward()`ï¼ŒPyTorch å°±ä¼šè‡ªåŠ¨æ±‚å‡ºå¶å­èŠ‚ç‚¹çš„å¾®åˆ†å€¼å¹¶æ›´æ–°å­˜å‚¨åœ¨å¶å­èŠ‚ç‚¹çš„ grad å±æ€§ã€‚æ³¨æ„ï¼Œ**åªæœ‰å¶å­èŠ‚ç‚¹çš„ `grad` å±æ€§èƒ½è¢«æ›´æ–°**ã€‚

### 2.3 å‰å‘ä¼ æ’­

Autograd æŠ€æœ¯å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä»å¶å­èŠ‚ç‚¹å¼€å§‹è¿½è¸ªä¿¡æ¯æµï¼Œè®°ä¸‹æ•´ä¸ªè¿‡ç¨‹ä½¿ç”¨çš„å‡½æ•°ï¼ŒçŸ¥é“è¾“å‡ºèŠ‚ç‚¹ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º**å‰å‘ä¼ æ’­**ã€‚

é¦–å…ˆåˆå§‹åŒ–å¶å­èŠ‚ç‚¹ $\vec{x}$ï¼š

```python
x = torch.one(2)
x.requires_grad
```

æ‰“å°å‡º Falseï¼Œå› ä¸ºé»˜è®¤æƒ…å†µä¸‹ Tensor çš„ `requires_grad` ä¸º Falseã€‚ä¸ºäº†è®© PyTorch å¸®æˆ‘ä»¬è‡ªåŠ¨æ±‚å¾®åˆ†ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è®¾ä¸º Trueï¼š

```python
>>> X.requires_grad = True
>>> X
tensor([1., 1.], requires_grad=True)
```

æ­¤æ—¶ $\vec{x}$  çš„ `grad` å’Œ `grad_fn` å±æ€§ä¸ºç©ºã€‚æ¥ä¸‹æ¥æˆ‘ä»¬è®¡ç®— $\vec{z}$ï¼š

```python
>>> z = 4 * X
>>> z
tensor([4., 4.], grad_fn=<MulBackward0>)
```

è¿™é‡Œé¢çš„ `grad_fn` æ˜¯<u>å¾®åˆ†å‡½æ•°</u>ï¼Œåœ¨æ­¤å¤„æ˜¯ä¹˜æ³•çš„åå‘å‡½æ•°ã€‚æœ€åæˆ‘ä»¬ç”¨ norm() å‡½æ•°æ±‚å…¶é•¿åº¦å¾—åˆ° yï¼š

```python
>>> y = z.norm()
>>> y
tensor(5.6569, grad_fn=<CopyBackwards>)
```

### 2.4 åå‘ä¼ æ’­

æ¥ä¸‹æ¥ï¼Œè°ƒç”¨è¾“å‡ºèŠ‚ç‚¹çš„ `backward()` å‡½æ•°å¯¹æ•´ä¸ªå›¾è¿›è¡Œåå‘ä¼ æ’­ï¼Œæ±‚å‡ºå¾®åˆ†å€¼ï¼š

```python
>>> y.backward()
>>> X.grad
tensor([2.8284, 2.8284])
```

è¿è¡Œåå¯ä»¥å‘ç° $\vec{x}$ çš„ grad å±æ€§æ›´æ–°ä¸º $\vec{x}$ çš„å¾®åˆ†å€¼ï¼Œè¿™ä¸ªç»“æœä¸æˆ‘ä»¬äººå·¥è®¡ç®—çš„ç»“æœä¸€è‡´ã€‚

å†æŸ¥çœ‹ä¸€ä¸‹ $\vec{z}$ å’Œ $y$ çš„ grad å€¼ï¼Œå‘ç°å¹¶æ²¡æœ‰æ”¹å˜ï¼Œå› ä¸ºä»–ä»¬éƒ½ä¸æ˜¯å¶å­èŠ‚ç‚¹ï¼š

```python
>>> z.grad
>>> y.grad
```

## 3. çº¿æ€§å›å½’

æœ¬èŠ‚æˆ‘ä»¬å°†å®ç°ä¸€ä¸ª**çº¿æ€§å›å½’**ï¼ˆLRï¼‰æ¨¡å‹ã€‚

### 3.1 ç†è®ºåˆ†æ

#### 1ï¼‰å‡†å¤‡æ•°æ®

```python
import torch
import matplotlib.pyplot as plt

x = torch.Tensor([1.4, 5, 11, 16, 21])
y = torch.Tensor([14.4, 29.6, 62, 85, 113.4])

plt.scatter(x.numpy(), y.numpy())
plt.show()
```

å¾—åˆ°å›¾ï¼š![image-20220116200141925](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116200141925.png)

#### 2ï¼‰ç›®æ ‡å‡½æ•°

å› ä¸ºæˆ‘ä»¬å‡è®¾ç”¨ä¸€æ¡ç›´çº¿å»æ‹Ÿåˆï¼Œæ‰€ä»¥å¯ä»¥å‡è®¾è¯¥å‡½æ•°ä¸ºï¼š$y=w_1x+w_0$ã€‚æˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°ä¸€ç»„åˆé€‚çš„ $(w_1, w_0)$ã€‚

æˆ‘ä»¬æŠŠä¸Šé¢ y æ”¹å†™ä¸€ä¸‹å¾—åˆ°ï¼š$\hat{y}=w_1x+w_0$ï¼Œè¿™æ · $\hat{y}^{(i)}$ æ˜¯ç”±æ ·æœ¬ä¸­çš„ $x^{(i)}$ ä¼ å…¥çº¿æ€§æ¨¡å‹åè®¡ç®—å¾—åˆ°çš„è¾“å‡ºï¼Œ$y^{(i)}$ æ˜¯æˆ‘ä»¬çœŸå®æ ·æœ¬å€¼ã€‚å› ä¸ºæµ‹é‡ä¼šäº§ç”Ÿè¯¯å·®ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå‡½æ•°æ¥è¡¡é‡ $\hat{y}^{(i)}$ å’Œ $y^{(i)}$ ä¹‹é—´çš„è¯¯å·®ï¼Œè¿™ä¸ªå‡½æ•°å°±æ˜¯**æŸå¤±å‡½æ•°**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å‡æ–¹è¯¯å·®å‡½æ•°ï¼ˆMean-Square Errorï¼Œ**MSE**ï¼‰ï¼š

$L(w_1, w_0) = \sum^5_{i=1}(\hat{y}^{(i)} - y^{(i)})^2$

å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ‰¾ä¸€ç»„åˆé€‚çš„ $(w_1, w_0)$ ä½¿å¾—æŸå¤±å‡½æ•°çš„ L å€¼æœ€å°ã€‚

#### 3ï¼‰ä¼˜åŒ–

ä¸ºäº†è®©æŸå¤±å‡½æ•°å€¼ L é™åˆ°æœ€å°ï¼Œæˆ‘ä»¬å°±è¦å¼€å§‹è°ƒæ•´å‚æ•° $(w_1, w_0)$ çš„å€¼äº†ï¼è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º**ä¼˜åŒ–**ã€‚è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç§**æ¢¯åº¦ä¸‹é™**çš„æ–¹æ³•æ¥å¯»æ‰¾è¿™ä¸ªå‡½æ•°çš„æœ€å°å€¼ã€‚

L çš„æ¢¯åº¦æ˜¯ï¼š$\nabla L = (\frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_0})$

è¿™æ ·ä¼˜åŒ–çš„è¿‡ç¨‹å°±æ˜¯åšè¿™æ ·çš„è¿ç®—ï¼š$\vec{w}^{t+1} = \vec{w}^t - \nabla L(\vec{w}^t) \times \delta$ï¼Œå…¶ä¸­ $\delta$ æ˜¯å­¦ä¹ ç‡ã€‚

#### 4ï¼‰æ‰¹é‡è¾“å…¥

ä¸Šé¢çš„è¡¨è¾¾å¼æ˜¯ä¸€æ¬¡ä¸€ä¸ªæ ·æœ¬çš„å½¢å¼ï¼Œåœ¨å®é™…çš„ä¼˜åŒ–ä¸­ï¼Œæˆ‘ä»¬æ˜¯è®©å¤šä¸ªæ ·æœ¬åŒæ—¶åœ¨ä¸€ä¸ªå…¬å¼ä¸­å‡ºç°ï¼Œæ‰€æœ‰å…¬å¼ä¸­çš„ $\vec{x}$ å’Œ $\hat{y}$ éƒ½è¦å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œ$\vec{x}$ å‡çº§ä¸ºçŸ©é˜µ $\boldsymbol{X}$ï¼Œ$\hat{y}$ å‡çº§ä¸º $\vec{\hat{y}}$ ï¼Œæœ€ç»ˆç»“æœä¸ºï¼š$\vec{\hat{y}} = \boldsymbol{X} \cdot \vec{w}$

æŸå¤±å‡½æ•° L å¯ä»¥è¡¨ç¤ºä¸ºï¼š$L(w_1, w_0) = |\vec{\hat{y}} - \vec{y}|^2$

#### 5ï¼‰è®­ç»ƒ

è®­ç»ƒå°±æ˜¯ä¸æ–­åœ°é€šè¿‡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œå¯¹å‚æ•° $\vec{w}$ è¿›è¡Œè°ƒä¼˜ï¼Œæœ€ç»ˆè®©æŸå¤±å‡½æ•°çš„æŸå¤±å€¼ L è¾¾åˆ°æœ€å°çš„è¿‡ç¨‹ï¼š

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116203409197.png" alt="image-20220116203409197" style="zoom:80%;" />

### 3.2 ä»£ç å®ç°

#### 1ï¼‰å‡†å¤‡æ•°æ®

xï¼Œy ä»ç„¶ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰çš„æ•°æ®ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥å˜é‡å’Œå„å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼š

```python
import torch
import matplotlib.pyplot as plt

# å‡†å¤‡æ•°æ®

# ç”ŸæˆçŸ©é˜µX
def Produce_X(x):
	x0 = torch.ones(x.numpy().size) # ç”¨onesäº§ç”Ÿåˆå§‹å€¼ä¸º1ï¼Œå¤§å°ä¸xç›¸åŒçš„å‘é‡
	X = torch.stack((x,x0),dim=1)   # stackå‡½æ•°å°†ä¸¤ä¸ªå‘é‡æ‹¼åˆ
	return X


x = torch.Tensor([1.4,5,11,16,21])
y = torch.Tensor([14.4,29.6,62,85.5,113.4])
X = Produce_X(x)

# å®šä¹‰æƒé‡wçš„å˜é‡
w = torch.rand(2,requires_grad=True)

inputs = X 
target = y
```

+ `Produce_X` å‡½æ•°å°† x ä¸ä¸€ä¸ªä¸ä¹‹ç›¸åŒå½¢çŠ¶çš„å…¨ 1 å‘é‡è¿›è¡Œåˆå¹¶å¾—åˆ°ä¸€ä¸ª Xï¼Œå®ƒçš„å®é™…æ•°æ®å¦‚ä¸‹ï¼š

  ```
  tensor([[ 1.4000,  1.0000],
          [ 5.0000,  1.0000],
          [11.0000,  1.0000],
          [16.0000,  1.0000],
          [21.0000,  1.0000]])
  ```

  è¿™æ · X ä¸ $\vec{w}$ çš„ä¹˜ç§¯ä¾¿ç›¸å½“äºä¸€ä¸ª $y = w_1x + w_0$ã€‚

+ ç”¨ `rand()` å‡½æ•°æ¥åˆå§‹åŒ–å‚æ•°å‘é‡ $\vec{w}$ï¼Œæ ¹æ® Autograd ä¸­æ‰€ä»‹ç»çš„ï¼Œå‚æ•° w å±äºè®¡ç®—å›¾çš„å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦è¿›è¡Œè‡ªåŠ¨å¾®åˆ†å¹¶åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°ï¼Œå› æ­¤éœ€è¦ä¸“é—¨å°† w çš„ `requires_grad` è®¾ç½®ä¸º Trueã€‚ 

#### 2ï¼‰è®­ç»ƒ

æ¯ä¸€è½®çš„è®­ç»ƒåˆ†æˆä¸¤éƒ¨åˆ†ï¼šå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­

```python
#è®­ç»ƒ
def train(epochs=1,learning_rate=0.01):
	for epoch in range(epochs):

		#å‰å‘ä¼ æ’­
		output = inputs.mv(w) #å…¬å¼ï¼šy=Xw
		loss = (output - target).pow(2).sum() # å…¬å¼ï¼šL = âˆ‘(y-y')^2

		#åå‘ä¼ æ’­
		loss.backward() 
		w.data -= learning_rate * w.grad  # æ›´æ–°æƒé‡wï¼Œå…¬å¼ï¼šw_(t+1)= w_(t) - ğœ¼*â–½J
		
		w.grad.zero_() # æ¸…ç©ºgradçš„å€¼

		if epoch % 80 == 0:
			draw(output,loss)

	#plt.savefig('plot1.png', format='png')

	return w, loss
```

+ æ³¨æ„ï¼Œ**æˆ‘ä»¬æ›´æ–°å®Œ w åï¼Œå¿…é¡»æ¸…ç©º w çš„ grad çš„å€¼ï¼Œå¦åˆ™ grad çš„å€¼ä¼šæŒç»­ç´¯åŠ **ã€‚æ‰€ä»¥ï¼Œè¿™é‡Œä½¿ç”¨ `zero_()` å‡½æ•°æ¥æ¸…ç©ºæ¢¯åº¦å€¼ã€‚

ä¸ºäº†èƒ½å¤Ÿè§‚å¯Ÿåˆ°è®­ç»ƒçš„å˜åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥è®©ç¨‹åºæ¯è¿›è¡Œ 80 æ¬¡å¾ªç¯å°±æ›´æ–°ä¸€æ¬¡å›¾åƒï¼Œäºæ˜¯å®šä¹‰ä¸€ä¸ª `draw()` å‡½æ•°ï¼š

```python
#ç»˜å›¾
def draw(output,loss):
	plt.cla() # æ¸…ç©ºå‡½æ•°å›¾åƒ
	plt.scatter(x.numpy(), y.numpy()) # ç»˜åˆ¶æ•£ç‚¹å›¾
	
	plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5) # ç»˜åˆ¶å‡ºå›å½’ç›´çº¿
	plt.text(0.5, 0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.text(3, 9,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.axis([10, 160, 0, 0.03])

	plt.pause(0.005)
```

äºæ˜¯ä¾¿å¯ä»¥è®­ç»ƒäº†ï¼š

```python
w, loss = train(10000,learning_rate = 1e-4)  #å­¦ä¹ ç‡è®¾ç½®ä¸º1x10^(-4)
```

è®­ç»ƒå®Œä¹‹åæ‰“å°æœ€ç»ˆç»“æœï¼š

```python
print("final loss:", loss.item())
print("weights:", w.data)
```

è¿è¡Œç»“æœï¼š

```
final loss: 8.2430419921875
weights: tensor([5.0840, 5.5849])
```

### 3.3 å¤§è§„æ¨¡æ•°æ®é›†å®ä¾‹

ä¹‹å‰è®­ç»ƒæ—¶æˆ‘ä»¬å°† 5 ä¸ªæ•°æ®æ ·æœ¬åŒæ—¶è¾“å…¥ç¨‹åºï¼Œè¿™ç§æ–¹å¼å«åš**æ‰¹è¾“å…¥**ï¼Œè¿™ç§æ–¹å¼æ˜¯å¿«é€Ÿè€Œæœ‰æ•ˆçš„ã€‚

äººä»¬é€šè¿‡å¯¹ç¥ç»å…ƒçš„ç ”ç©¶ï¼Œå¯¹å…¶è¿›è¡Œæ•°å­¦æŠ½è±¡å¾—åˆ°äº†**äººå·¥ç¥ç»å…ƒæ¨¡å‹**ï¼š

![image-20220117175443927](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220117175443927.png)

PyTorch ä¸ºæˆ‘ä»¬é¢„å…ˆç¼–å†™å¥½äº†æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å‡½æ•°ç­‰ï¼Œæˆ‘ä»¬å°†ä»£ç å†é‡æ–°ç¼–å†™ä¸€æ¬¡ï¼š

#### 1ï¼‰å‡†å¤‡æ•°æ®

```python
import torch
import matplotlib.pyplot as plt
from torch import nn, optim
from time import perf_counter

# ç”¨linspaceäº§ç”Ÿï¼ˆ-3ï¼Œ3ï¼‰åŒºé—´å†…çš„100000ä¸ªç‚¹ï¼Œå¹¶ä½¿ç”¨unsqueezeå‡½æ•°å¢åŠ ä¸€ä¸ªç»´åº¦
x = torch.unsqueeze(torch.linspace(-3,3,100000),dim=1)

# å‡è®¾çœŸå®å‡½æ•°æ˜¯y=xï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢å¢åŠ ä¸€äº›è¯¯å·®ï¼Œæ›´åŠ ç¬¦åˆå®é™…æƒ…å†µ
y = x + 1.2 * torch.rand(x.size())
```

#### 2ï¼‰å®šä¹‰æ¨¡å‹

å®šä¹‰ä¸€ä¸ªçº¿æ€§å›å½’çš„æ¨¡å‹ LRï¼Œå®ƒç»§æ‰¿è‡ª `nn.Module`ï¼Œå¹¶åœ¨å…¶ä¸­ä½¿ç”¨ `nn.Linear()` æ„é€ çº¿æ€§æ¨¡å‹ï¼š

```python
class LR(nn.Module):
    def __init__(self):
        super(LR,self).__init__()
        self.linear = nn.Linear(1,1)

    def forward(self,x):
        out = self.linear(x)
        return out
```

+ `nn.Linear()` çš„ç¬¬ä¸€ä¸ªå‚æ•°ä»£è¡¨è¾“å…¥æ•°æ®çš„ç»´åº¦ï¼Œç¬¬äºŒä¸ªå‚æ•°ä»£è¡¨è¾“å‡ºæ•°æ®çš„ç»´åº¦ã€‚è¿™é‡Œ x å’Œ y éƒ½æ˜¯ä¸€ç»´çš„ï¼Œå› æ­¤è®¾ç½®ä¸º `nn.Linear(1, 1)`ã€‚
+ `forward()` å‡½æ•°æ¥æ„é€ ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­çš„è®¡ç®—æ­¥éª¤ã€‚

#### 3ï¼‰å®ä¾‹åŒ–æ¨¡å‹

å¦‚æœå¹³å°æ”¯æŒ CUDAï¼Œå®ä¾‹åŒ– LR ç±»åéœ€è¦è°ƒç”¨ `cuda()` æ–¹æ³•ï¼š

```python
#å¦‚æœæ”¯æŒCUDAï¼Œåˆ™é‡‡ç”¨CUDAåŠ é€Ÿ
CUDA = torch.cuda.is_available()

if CUDA:
	LR_model = LR().cuda()
	inputs = x.cuda()
	target = y.cuda()
else:
	LR_model = LR()
	inputs = x
	target = y
```

#### 4ï¼‰æŸå¤±å‡½æ•°

nn æ¨¡å—ä¸­é¢„è®¾æœ‰å‡æ–¹è¯¯å·®å‡½æ•°ï¼š

```python
criterion = nn.MSELoss()
```

#### 5ï¼‰ä¼˜åŒ–å™¨

ä¸‹é¢é‡‡ç”¨â€œéšæœºæ¢¯åº¦ä¸‹é™â€çš„æ–¹æ³•æ¥æ›´æ–°æƒé‡ã€‚**éšæœºæ¢¯åº¦ä¸‹é™**å®é™…ä¸Šå°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•çš„æ”¹è‰¯ç‰ˆï¼Œä¸é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•ä¸­æŠŠå…¨éƒ¨æ•°æ®æ‹¿æ¥è®¡ç®—æ¢¯åº¦çš„æ–¹æ³•ï¼Œè€Œæ˜¯<u>æ¯æ¬¡éšæœºæŒ‘é€‰ä¸€ä¸ªæ•°æ®æ ·æœ¬è®¡ç®—æ¢¯åº¦å€¼ï¼Œå¹¶è¿›è¡Œæƒå€¼æ›´æ–°</u>ã€‚è¿™æ ·åšçš„**å¥½å¤„**æ˜¯å¯ä»¥é¿å…ä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨æ•°æ®å¯¼è‡´çš„å†…å­˜æº¢å‡ºé—®é¢˜ï¼Œè¿˜å¯ä»¥é˜²æ­¢ä¼˜åŒ–çš„æ—¶å€™é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ PyTorch é¢„è®¾çš„éšæœºæ¢¯åº¦ä¸‹é™å‡½æ•° `SGD()` è¿›è¡Œæ›´æ–°ï¼š

```python
optimizer = optim.SGD(LR_model.parameters(), lr=1e-4)
```

+ `SGD()` å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯**éœ€è¦ä¼˜åŒ–çš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‚æ•°**ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯**å­¦ä¹ ç‡**ã€‚

#### 6ï¼‰è®­ç»ƒ

å¼€å§‹ç¼–å†™ `train()` å‡½æ•°ï¼Œå…¶å‚æ•°ä¾æ¬¡æ˜¯è¢«è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè®­ç»ƒè½®æ•°ï¼š

```python
def draw(output,loss):
    """å¯è§†åŒ–"""
    if CUDA:
        output = output.cpu()
    plt.cla()
    plt.scatter(x.numpy(), y.numpy())
    plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5)
    plt.text(0.5,0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
    plt.pause(0.005)

def train(model, criterion, optimizer, epochs):
    global loss
    for epoch in range(epochs):
        # forward
        output = model(inputs)
        loss = criterion(output,target)

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


        if epoch % 80 == 0:
            draw(output,loss)

    return model, loss
```

+ å¦‚æœé‡‡ç”¨äº† CUDA åŠ é€Ÿï¼Œdraw å‡½æ•°çš„ output éœ€è¦è¿˜åŸæˆ CPU çš„æ•°æ®ç±»å‹æ‰èƒ½è¿›è¡Œç»˜å›¾
+ åœ¨**å‰å‘ä¼ æ’­**é˜¶æ®µï¼Œæˆ‘ä»¬å°† inputs è¾“å…¥ç¥ç»ç½‘ç»œæ¨¡å‹ model å¾—åˆ° outputï¼Œæ¥ä¸‹æ¥ç”¨å®šä¹‰çš„æŸå¤±å‡½æ•° criterion æ¥è®¡ç®—æŸå¤±å€¼ã€‚
+ åœ¨**åå‘ä¼ æ’­**é˜¶æ®µï¼Œå…ˆç”¨ optimizer.zero_grad() æ¸…ç©ºæƒå€¼çš„ grad å€¼ï¼Œéšåç”¨ backward() è®¡ç®—æ¢¯åº¦ï¼Œå¹¶ç”¨ä¼˜åŒ–å™¨ optimizer.stip() å‡½æ•°è¿›è¡Œæƒå€¼æ›´æ–°ã€‚ 

æ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰åˆè¯•æ—¶é—´ startï¼Œå¹¶ä¼ å…¥æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ä»¥åŠè®­ç»ƒè½®æ•°ï¼ˆ10 000 æ¬¡ï¼‰ï¼š

```python {2}
start = perf_counter()
LR_model,loss = train(LR_model,criterion,optimizer,10000)
finish = perf_counter()
time = finish - start

print("è®¡ç®—æ—¶é—´:%s" % time)
print("final loss:",loss.item())
print("weights:",list(LR_model.parameters()))
```

ä»£ç çš„è®­ç»ƒç»“æœå¦‚ä¸‹ï¼š

```
è®¡ç®—æ—¶é—´:164.62969759999942
final loss: 0.12093639373779297
weights: [Parameter containing:
tensor([[0.9995]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.5632], device='cuda:0', requires_grad=True)]
```

## 4. éçº¿æ€§å›å½’

éçº¿æ€§å°±æ˜¯è¯´æˆ‘ä»¬çš„æ‹Ÿåˆå‡½æ•°å¹¶éç›´çº¿æˆ–è€…å¹³é¢ï¼Œè€Œæ˜¯æ›´åŠ å¤æ‚çš„æ›²çº¿æˆ–æ›²é¢ã€‚

### 4.1 æ¿€æ´»å‡½æ•°

åœ¨äººå·¥ç¥ç»å…ƒå›¾ä¸­çš„ååŠæ®µè¿˜æœ‰ä¸€ä¸ªæ¿€æ´»å‡½æ•° fï¼Œä½†æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„çº¿æ€§å›å½’å¿½ç•¥äº†å®ƒã€‚åœ¨æ²¡æœ‰æ¿€æ´»å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œå¤šä¸ªç¥ç»å…ƒçš„å †å ç›¸å½“äºå¤šä¸ªçº¿æ€§æ¨¡å‹çš„å åŠ ï¼Œä»æ€»ä½“ä¸Šçœ‹ï¼Œå…¶ç¥ç»ç½‘ç»œæœ¬è´¨ä¸Šè¿˜æ˜¯ä¸ªçº¿æ€§æ¨¡å‹ã€‚**æ¿€æ´»å‡½æ•°**çš„å‡ºç°å°±æ˜¯ä¸ºäº†è®©ç¥ç»ç½‘ç»œå¯ä»¥**æ‹Ÿåˆå¤æ‚çš„éçº¿æ€§å‡½æ•°**ã€‚æ¿€æ´»å‡½æ•° f å®é™…ä¸Š**æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„éçº¿æ€§å‡½æ•°**ï¼Œä½†åªè¦å¤šä¸ªå¸¦æœ‰æ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒç»„åˆåœ¨ä¸€èµ·ï¼Œå°±å…·æœ‰æ‹Ÿåˆå¤æ‚éçº¿æ€§å‡½æ•°çš„å¼ºå¤§èƒ½åŠ›ã€‚

> å„ç±»æ¿€æ´»å‡½æ•°å¯ä»¥ç™¾åº¦ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨ ReLU å‡½æ•°ã€‚
>
> ReLU å‡½æ•°ï¼š$ReLU(x) = x ï¼ˆx > 0ï¼‰; = 0 (x \le 0)$

å› æ­¤æ•´ä¸ªç¥ç»å…ƒçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š$y = ReLU(\vec{x} \cdot \vec{w})$

### 4.2 äººå·¥ç¥ç»ç½‘ç»œ

ä¸ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç½‘ç»œåˆ†æˆä¸‰å±‚ï¼šè¾“å…¥å±‚ã€éšå«å±‚ï¼ˆå¯ä»¥æœ‰å¤šå±‚ï¼‰å’Œè¾“å‡ºå±‚ã€‚éšå«å±‚çš„å±‚æ•°å¤§äºç­‰äº 2 çš„ç¥ç»ç½‘ç»œç§°ä¹‹ä¸º**æ·±åº¦ç¥ç»ç½‘ç»œ**ã€‚

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220118002521894.png" alt="image-20220118002521894" style="zoom:67%;" />

#### 1ï¼‰å‡†å¤‡æ•°æ®

æˆ‘ä»¬æ ¹æ®ä¸€å…ƒä¸‰æ¬¡æ–¹ç¨‹è‡ªåŠ¨ç”Ÿæˆä¸€æ‰¹æ•°æ®æ ·æœ¬ï¼Œéšåä½¿ç”¨ä»–ä»¬æ¥æ¼”ç¤ºç¥ç»ç½‘ç»œçš„éçº¿æ€§å›å½’ï¼š

```python
import torch
import matplotlib.pyplot as plt

x = torch.unsqueeze(torch.linspace(-3,3,10000),dim=1)
y = x.pow(3)+1.3*torch.rand(x.size())

plt.scatter(x.numpy(), y.numpy(),s=0.01)
plt.show()
```

#### 2ï¼‰å»ºç«‹æ¨¡å‹

æˆ‘ä»¬ä½¿ç”¨ä»…å«æœ‰ä¸€å±‚éšå«å±‚çš„ç¥ç»ç½‘ç»œæ¥å¤„ç†ä¸Šé¢çš„æ•°æ®ï¼š

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220118003356722.png" alt="image-20220118003356722" style="zoom:67%;" />

```python
from torch import nn,optim
import torch.nn.functional as F

class Net(nn.Module):  # ç»§æ‰¿ torch.nn çš„ Module
    def __init__(self, input_feature, num_hidden, outputs):
        super(Net, self).__init__()     # ç»§æ‰¿ __init__ 
        # å®šä¹‰æ¯å±‚ç¥ç»å…ƒçš„ç»“æ„ä¸æ•°ç›®
        self.hidden = nn.Linear(input_feature, num_hidden)   # çº¿æ€§éšå«å±‚
        self.out = nn.Linear(num_hidden, outputs)   # è¾“å‡ºå±‚
 
    def forward(self, x):  
        # å‰å‘ä¼ æ’­è¾“å…¥å€¼
        x = F.relu(self.hidden(x))      # æ¿€åŠ±å‡½æ•°ReLUå¤„ç†éšå«å±‚çš„è¾“å‡º
        x = self.out(x)             # æœ€ç»ˆè¾“å‡ºå€¼
        return x
```

+ æ¿€æ´»å‡½æ•°çš„ä½¿ç”¨ç›´æ¥è°ƒç”¨ `torch.nn.functional.F.relu` å³å¯ã€‚

å®ä¾‹åŒ– Netï¼Œè®¾ç½®è¾“å…¥ä¸º 1 ç»´ï¼Œéšå«å±‚èŠ‚ç‚¹æ•°ä¸º 20ï¼Œè¾“å‡ºä¸º 1 ç»´ï¼š

```python
CUDA = torch.cuda.is_available()

if CUDA:
	#åˆå§‹åŒ–è¾“å…¥ç¥ç»å…ƒæ•°ç›®ä¸º1ï¼Œéšå«å±‚æ•°ç›®ä¸º20ï¼Œè¾“å‡ºç¥ç»å…ƒæ•°ç›®ä¸º1çš„ç¥ç»ç½‘ç»œæ¨¡å‹
	net = Net(input_feature=1, num_hidden=20, outputs=1).cuda()
	inputs = x.cuda()
	target = y.cuda()
else:
	net = Net(input_feature=1, num_hidden=20, outputs=1)
	inputs = x
	target = y
```

ä¸çº¿æ€§å›å½’ä¸€æ ·ï¼Œä¼˜åŒ–å™¨é€‰æ‹©éšæœºæ¢¯åº¦ä¸‹é™ï¼ŒæŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®ï¼š

```python
optimizer = optim.SGD(net.parameters(), lr=0.01)  # ä¼ å…¥ net çš„æ‰€æœ‰å‚æ•°, å­¦ä¹ ç‡
criterion = nn.MSELoss()      # é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„è¯¯å·®è®¡ç®—å…¬å¼ (å‡æ–¹å·®)
```

#### 3ï¼‰è®­ç»ƒ

è®­ç»ƒå‡½æ•° train() ä¸ä¹‹å‰ç±»ä¼¼ï¼Œä¹Ÿæ˜¯**åˆ†æˆå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸¤ä¸ªæ­¥éª¤**ï¼š

```python
def draw(output,loss):
	if CUDA:
		output = output.cpu()
	plt.cla()
	plt.scatter(x.numpy(), y.numpy())
	plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5)
	plt.text(-2,-20,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	plt.pause(0.005)


def train(model,criterion,optimizer,epochs):
	for epoch in range(epochs):
		# forward
		output = model(inputs)
		loss = criterion(output,target)

		# backward
		optimizer.zero_grad()
		loss.backward()
		optimizer.step()
		
		if epoch % 80 == 0:
			draw(output,loss)

	return model,loss

```

æˆ‘ä»¬è®­ç»ƒ 10000 æ¬¡ï¼Œå¹¶æ‰“å°æœ€ç»ˆç»“æœï¼š

```python
net,loss = train(net,criterion,optimizer,10000)

print("final loss:", loss.item())
```

## 5. é€»è¾‘å›å½’

çº¿æ€§å›å½’å’Œéçº¿æ€§å›å½’çš„è¾“å‡ºéƒ½æ˜¯è¿ç»­çš„ï¼Œè€Œ**é€»è¾‘å›å½’çš„è¾“å‡ºæ˜¯äºŒå…ƒç¦»æ•£çš„**ï¼Œå³è¾“å‡º y åªæœ‰ä¸¤ç§ç»“æœï¼Œå› æ­¤ï¼Œ<u>é€»è¾‘å›å½’ä¹Ÿå¸¸å¸¸è¢«å½“ä½œäºŒå…ƒåˆ†ç±»é—®é¢˜</u>ã€‚

### 5.1 sigmoid å‡½æ•°

éçº¿æ€§ sigmoid å‡½æ•°ï¼ˆå¸¸ç®€å†™ä¸º **sigm**ï¼‰ï¼š$sigm(x) = \frac{1}{1 + e^{-x}}$ <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220118144558229.png" alt="image-20220118144558229" style="zoom:67%;" />

äºŒå…ƒåˆ†ç±»çš„æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220118144717708.png" alt="image-20220118144717708" style="zoom:67%;" />

è¿™æ ·å°†è¾“å…¥ $\vec{x}$ ç»è¿‡ç½‘ç»œåï¼Œé€šè¿‡ $sigm(\vec{x} \cdot \vec{w})$ æ˜ å°„ä¸ºé›†åˆ ï¼ˆ0ï¼Œ 1ï¼‰ ä¸­çš„ä¸€ä¸ªå®æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæœ€ç»ˆè¾“å‡ºå½“ä½œ y=1 çš„æ¦‚ç‡ $P(u=1, \vec{w}, \vec{x})$ ã€‚

sigmoid å·²é¢„ç½®åœ¨ PyTorch çš„ `torch.sigmoid()` ä¸­ï¼Œè¯¥å‡½æ•°å¯ä»¥å°†è¾“å…¥çš„ Tensor è¾“å‡ºæˆï¼ˆ0,1ï¼‰ä¹‹é—´çš„æ•°ï¼Œä¸”å’Œä¸º 1ï¼š

```python
>>> a = torch.randn(2)
>>> a
tensor([-0.2128,  0.5412])
>>> torch.sigmoid(a)
tensor([0.4470, 0.6321])
```

### 5.2 äº¤å‰ç†µæŸå¤±å‡½æ•°

**äº¤å‰ç†µæŸå¤±å‡½æ•°**æ˜¯åˆ†ç±»é—®é¢˜ä¸­å¸¸ç”¨çš„æŸå¤±å‡½æ•°ã€‚åœ¨ PyTorch ä¸­å·²ç»é¢„ç½®åœ¨ `nn.CrossEntropyLoss()` ä¸­äº†ã€‚

> ä¸ºä»€ä¹ˆç”¨äº¤å‰ç†µå‡½æ•°ï¼Ÿæˆ‘ä»¬å¯ä»¥å°†äºŒå…ƒåˆ†ç±»é—®é¢˜æŠ½è±¡æˆæ•°å­¦ä¸­çš„ä¼¯åŠªåˆ©æ¨¡å‹ï¼Œç„¶åå¯¹å…¶ä½¿ç”¨æœ€å¤§ä¼¼ç„¶æ³•å¾—åˆ°ä¼¼ç„¶åº¦çš„è®¡ç®—å…¬å¼ï¼Œé€šè¿‡å¯¹å…¶è¿›è¡Œè½¬åŒ–ï¼Œå¯ä»¥å°†æ±‚ä¼¼ç„¶åº¦çš„æœ€å¤§å€¼è½¬åŒ–ä¸ºæ±‚ä¸€ä¸ªæŸå¤±å‡½æ•°çš„æœ€å°å€¼ï¼Œè¿™é‡Œçš„æŸå¤±å‡½æ•°ä¾¿æ˜¯äº¤å‰ç†µå‡½æ•°ã€‚

### 5.3 é€»è¾‘å›å½’ç¤ºä¾‹

#### 1ï¼‰å‡†å¤‡æ•°æ®

```python
import torch
import matplotlib.pyplot as plt
from torch import nn,optim

means = torch.ones(500, 2) #oneså‡½æ•°ç”Ÿæˆ500x2çš„æ•°æ®
data0 = torch.normal(4 * means, 2) #æ„é€ ä¸€ä¸ªå‡å€¼ä¸º4ï¼Œæ ‡å‡†å·®ä¸º2çš„æ•°æ®ç°‡
data1 = torch.normal(-4 * means, 2) #æ„é€ ä¸€ä¸ªå‡å€¼ä¸º-4ï¼Œæ ‡å‡†å·®ä¸º2çš„æ•°æ®ç°‡
label0 = torch.zeros(500) #500ä¸ªæ ‡ç­¾0
label1 = torch.ones(500) #500ä¸ªæ ‡ç­¾1

x = torch.cat((data0, data1), ).type(torch.FloatTensor) 
y = torch.cat((label0, label1), ).type(torch.LongTensor)

plt.scatter(x.numpy()[:,0], x.numpy()[:, 1], c=y.numpy(), s=10, lw=0, cmap='RdYlGn')
plt.show()
```

![image-20220118151820205](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220118151820205.png)

#### 2ï¼‰å»ºç«‹æ¨¡å‹

```python
class Net(nn.Module):     # ç»§æ‰¿ torch çš„ Module
    def __init__(self):
        super(Net, self).__init__()     # ç»§æ‰¿ __init__ åŠŸèƒ½
        self.linear = nn.Linear(2,2)

    def forward(self, x):
        x = self.linear(x)
        x = torch.sigmoid(x)
        return x

CUDA = torch.cuda.is_available()

if CUDA:
    net = Net().cuda()
    inputs = x.cuda()
    target = y.cuda()
else:
    net = Net()
    inputs = x
    target = y
```

+ `nn.Linear(2, 2)` çš„è¾“å…¥åŒ…æ‹¬ä¸¤ä¸ªç‰¹å¾ $\vec{x} = (x_1, x_2)$ï¼Œåˆ†åˆ«ä»£è¡¨æ¨ªè½´å’Œçºµè½´ï¼›è¾“å‡ºçš„æ˜¯ä¸¤ä¸ªç±»â€œå¾—åˆ†â€æƒ…å†µï¼Œæˆ‘ä»¬å‡è®¾å“ªä¸€ç±»åˆ†æ•°é«˜ï¼Œå°±å±äºå“ªä¸€ç±»ã€‚**å¯ä»¥ä½¿ç”¨ sigmoid å‡½æ•°æ¥ç”Ÿæˆä¸¤ä¸ªç±»çš„æ¦‚ç‡**ã€‚

æˆ‘ä»¬ä»ç„¶ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ¥ä¼˜åŒ–ï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨äº¤å‰ç†µå‡½æ•°ï¼š

```python
optimizer = optim.SGD(net.parameters(), lr=0.02)
criterion = nn.CrossEntropyLoss()
```

#### 3ï¼‰è®­ç»ƒ

```python
def draw(output):
    if CUDA:
        output=output.cpu()
    plt.cla()
    output = torch.max((output), 1)[1] 
    pred_y = output.data.numpy().squeeze()
    target_y = y.numpy()
    plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=pred_y, s=10, lw=0, cmap='RdYlGn')
    accuracy = sum(pred_y == target_y)/1000.0  
    plt.text(1.5, -4, 'Accuracy=%s' % (accuracy), fontdict={'size': 20, 'color':  'red'})
    plt.pause(0.1)


def train(model,criterion,optimizer,epochs):
    for epoch in range(epochs):
        #forward
        output = model(inputs)
        loss = criterion(output, target)

        #backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 40 == 0:
            draw(output)


train(net,criterion,optimizer,1000)
```

+ ç¬¬ 5 è¡Œçš„ `torch.max()` ä¼šè¿”å› output ä¸­æ¦‚ç‡æœ€å¤§çš„ä¸€ç»„æ•°å€¼ä¸è¯¥ç±»åˆ«çš„æ ‡ç­¾ã€‚

::: tip torch.max()

`torch.max()` ä¼šè¿”å›é€‰å®šç»´åº¦ä¸­çš„æœ€å¤§å€¼å’Œåºåˆ—å·ï¼Œä¾‹å¦‚ `torch.max(b, 1)` ä¼šè¿”å›ç¬¬ä¸€ç»´çš„æœ€å¤§å€¼å’Œåºåˆ—å·ï¼š

```python
>>> b = torch.randn(5, 2)
>>> b
tensor([[ 0.4022, -0.7402],
        [ 1.4084,  0.0899],
        [-1.5336,  0.8580],
        [ 0.7828, -0.5670],
        [-0.1495,  0.0716]])
>>> torch.max(b, 1)
torch.return_types.max(
values=tensor([0.4022, 1.4084, 0.8580, 0.7828, 0.0716]),
indices=tensor([0, 0, 1, 0, 1]))
```

:::

## 6. å¤šå…ƒåˆ†ç±»

é€»è¾‘å›å½’æ˜¯äºŒå…ƒåˆ†ç±»ï¼Œå±äºå¤šå…ƒåˆ†ç±»çš„ç‰¹æ®Šæƒ…å†µã€‚

### 6.1 softmax å‡½æ•°

å¤šå…ƒåˆ†ç±»ä¸äºŒå…ƒåˆ†ç±»ç±»ä¼¼ï¼ŒåŒºåˆ«åœ¨äºç”¨ softmax ä»£æ›¿ sigmoidã€‚**å¤šå…ƒåˆ†ç±»çš„ç¥ç»ç½‘ç»œè¦æ±‚è¾“å‡ºå±‚çš„ç¥ç»å…ƒæ•°ç›®ä¸æ‰€éœ€åˆ†ç±»çš„ç±»åˆ«æ•°ä¿æŒä¸€è‡´**ã€‚**softmax èƒ½å°†æ‰€æœ‰åˆ†ç±»çš„åˆ†æ•°å€¼ $(\eta_1, \eta_2, ..., \eta_k)$ è½¬åŒ–ä¸ºæ¦‚ç‡ $(\pi_1, \pi_2, ..., \pi_k)$ï¼Œä¸”å„æ¦‚ç‡å’Œä¸º 1**ã€‚

å¤šå…ƒåˆ†ç±»æ¨¡å‹çš„ç»“æ„ï¼š

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220118165313230.png" alt="image-20220118165313230" style="zoom:80%;" />

softmax å‡½æ•°ï¼š$\pi_l = \frac{e^{\eta_l}}{\sum_{j=1}^k{e^{\eta_j}}}$

softmax å·§å¦™åœ°å°†å¤šä¸ªåˆ†ç±»çš„åˆ†æ•°è½¬åŒ–ä¸ºï¼ˆ0,1ï¼‰çš„å€¼å¹¶ä¸”å’Œä¸º 1ï¼š$\sum^k_{i=1}{\pi_i} = 1$

### 6.2 å¤šå…ƒåˆ†ç±»ç¤ºä¾‹

#### 1ï¼‰å‡†å¤‡æ•°æ®

```python
import torch
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torch import nn,optim
 
# ç”Ÿæˆæ•°æ®
means = torch.ones(500, 2) 
data0 = torch.normal(4*means, 2)      
data1 = torch.normal(-4*means, 1)    
data2 = torch.normal(-8*means, 1)     
label0 = torch.zeros(500)
label1 = torch.ones(500)                
label2 = label1*2  #500ä¸ªæ ‡ç­¾2
 
x = torch.cat((data0, data1, data2), ).type(torch.FloatTensor)  
y = torch.cat((label0, label1, label2), ).type(torch.LongTensor)    
 
plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=y.numpy(), s=10, lw=0, cmap='RdYlGn')
plt.show()
```

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220118170525699.png" alt="image-20220118170525699" style="zoom:80%;" />

#### 2ï¼‰å»ºç«‹æ¨¡å‹

```python
class Net(nn.Module):    
    def __init__(self, input_feature, num_hidden,outputs):
        super(Net, self).__init__()     
        self.hidden = nn.Linear(input_feature, num_hidden)   # çº¿æ€§éšå«å±‚
        self.out = nn.Linear(num_hidden, outputs)       # è¾“å‡ºå±‚

    def forward(self, x):
        x = F.relu(self.hidden(x))      # æ¿€åŠ±å‡½æ•°ReLUå¤„ç†éšå«å±‚çš„è¾“å‡º
        x = self.out(x)
        x = F.softmax(x)     #ä½¿ç”¨softmaxå°†è¾“å‡ºå±‚çš„æ•°æ®è½¬æ¢æˆæ¦‚ç‡å€¼           
        return x

CUDA = torch.cuda.is_available()

if CUDA:
    net = Net(input_feature=2, num_hidden=20,outputs=3).cuda()
    inputs = x.cuda()
    target = y.cuda()
else:
    net = Net(input_feature=2, num_hidden=20,outputs=3)
    inputs = x
    target = y

optimizer = optim.SGD(net.parameters(), lr=0.02)
criterion = nn.CrossEntropyLoss()
```

#### 3ï¼‰è®­ç»ƒ

```python
ef draw(output):
    if CUDA:
        output=output.cpu()
    plt.cla()
    output = torch.max((output), 1)[1] 
    pred_y = output.data.numpy().squeeze()
    target_y = y.numpy()
    plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=pred_y, s=10, lw=0, cmap='RdYlGn')
    accuracy = sum(pred_y == target_y)/1500.0  
    plt.text(1.5, -4, 'Accuracy=%s' % (accuracy), fontdict={'size': 20, 'color':  'red'})
    plt.pause(0.1)

def train(model,criterion,optimizer,epochs):
    for epoch in range(epochs):
        #forward
        output = model(inputs)
        
        loss = criterion(output,target)

        #backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 40 == 0:
            draw(output)

train(net,criterion,optimizer,10000)
```

## 7. å·ç§¯ç¥ç»ç½‘ç»œ

### 7.1 å¼•å…¥å·ç§¯

ç»è¿‡å¯¹çŒ«çš„è„‘çš®å±‚ç ”ç©¶å‘ç°ï¼Œ<u>è§†è§‰ç³»ç»Ÿçš„ä¿¡æ¯å¤„ç†æ˜¯åˆ†çº§çš„</u>ã€‚å·ç§¯ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘çš„è§†è§‰å¤„ç†æœºåˆ¶ï¼Œé‡‡ç”¨åˆ†çº§æå–ç‰¹å¾çš„åŸç†ï¼Œæ¯ä¸€çº§çš„ç‰¹å¾å‡ç”±ç½‘ç»œå­¦ä¹ æå–ã€‚

åœ¨å‰é¢ä»‹ç»çš„ç¥ç»ç½‘ç»œä¸­ï¼Œè¾“å…¥å±‚è¢«æè¿°ä¸ºä¸€åˆ—ç¥ç»å…ƒã€‚è€Œåœ¨å·ç§¯ç¥ç»ç½‘ç»œé‡Œï¼Œæˆ‘ä»¬**æŠŠè¾“å…¥å±‚çœ‹åšäºŒç»´çš„ç¥ç»å…ƒ**ï¼Œå¦‚æœè¾“å…¥çš„æ˜¯åƒç´ å¤§å°ä¸º 28 * 28 çš„å›¾ç‰‡ï¼Œåˆ™å¯ä»¥çœ‹åš 28 * 28 çš„äºŒç»´ç¥ç»å…ƒï¼Œ**å®ƒçš„æ¯ä¸€ä¸ªèŠ‚ç‚¹å¯¹åº”å›¾ç‰‡åœ¨è¿™ä¸ªåƒç´ ç‚¹çš„ç°åº¦å€¼**ã€‚

åœ¨ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬ä¼šæŠŠè¾“å…¥å±‚çš„èŠ‚ç‚¹ä¸éšå«å±‚çš„èŠ‚ç‚¹é‡‡ç”¨å…¨è¿æ¥ï¼Œè€Œåœ¨ CNN ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨â€œ**å±€éƒ¨æ„ŸçŸ¥**â€çš„æ–¹æ³•ï¼Œå³ä¸å†æŠŠè¾“å…¥å±‚çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½è¿æ¥åˆ°éšå«å±‚çš„æ¯ä¸€ä¸ªç¥ç»å…ƒèŠ‚ç‚¹ä¸Šï¼Œè€Œæ˜¯ç”¨ä¸€ä¸ªå±€éƒ¨æ„ŸçŸ¥åŸŸï¼ˆè¿‡æ»¤å™¨ï¼‰ä¸æ–­ç§»åŠ¨è¿›è¡Œå·ç§¯ï¼š

![image-20220119151430747](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220119151430747.png)

åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™ç§éšå«å±‚ä¹Ÿè¢«ç§°ä¸º**ç‰¹å¾å›¾**ã€‚

::: details ä¸ºä»€ä¹ˆå·ç§¯ï¼Ÿ

åœ¨ä¼ ç»Ÿå…¨è¿æ¥çš„ç¥ç»ç½‘ç»œä¸­ï¼Œå¦‚æœè¦å¯¹ä¸€å¼ å›¾ç‰‡è¿›è¡Œåˆ†ç±»ï¼Œè¿æ¥æ–¹å¼å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬æŠŠä¸€å¼ å¤§å°ä¸º 100Ã—100 çš„å›¾ç‰‡çš„æ¯ä¸ªåƒç´ ç‚¹éƒ½è¿æ¥åˆ°æ¯ä¸€ä¸ªéšå«å±‚çš„èŠ‚ç‚¹ä¸Šï¼Œå¦‚æœéšå«å±‚çš„èŠ‚ç‚¹æ•°ä¸º 10000ï¼Œé‚£ä¹ˆè¿æ¥çš„æƒé‡æ€»æ•°åˆ™ä¸º $10^8$ ä¸ªã€‚å½“å›¾ç‰‡åƒç´ æ›´å¤§ï¼Œéšå«å±‚çš„èŠ‚ç‚¹æ•°ç›®æ›´å¤šæ—¶ï¼Œåˆ™éœ€è¦æ›´åŠ åºå¤§çš„æƒé‡æ•°ç›®ã€‚

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220119152111741.png" alt="image-20220119152111741" style="zoom: 50%;" />

åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬ä¸å†éœ€è¦å¦‚æ­¤åºå¤§çš„æƒé‡æ•°ç›®ã€‚ï¼Œåœ¨åˆ©ç”¨ 10Ã—10 çš„è¿‡æ»¤å™¨å¯¹ 100Ã—100 çš„åŸå›¾è¿›è¡Œå·ç§¯æ—¶ï¼Œè¯¥è¿‡æ»¤å™¨åœ¨ä¸æ–­æ»‘åŠ¨çš„è¿‡ç¨‹ä¸­å¯¹åº”ç”Ÿæˆä¸€å¼ ç‰¹å¾å›¾ï¼Œå³ä¸€ä¸ªè¿‡æ»¤å™¨(100ä¸ªæƒé‡å€¼)å¯å¯¹åº”ä¸€å¼ ç‰¹å¾å›¾ã€‚å¦‚æœæˆ‘ä»¬æœ‰ 100 å¼ ç‰¹å¾å›¾ï¼Œåˆ™ä¸€å…±åªéœ€è¦ 104 ä¸ªæƒé‡å€¼ã€‚<u>å¦‚æ­¤ä¸€æ¥ï¼Œåœ¨ä¸€ä¸ªéšå«å±‚çš„æƒ…å†µä¸‹ï¼Œå·ç§¯ç¥ç»ç½‘ç»œçš„æƒé‡æ•°ç›®å¯ä»¥å‡å°è‡³å…¨è¿æ¥ç¥ç»ç½‘ç»œæƒé‡æ•°ç›®çš„ä¸€ä¸‡åˆ†ä¹‹ä¸€ï¼Œå¤§å¤§å‡å°‘è®¡ç®—é‡ï¼Œæé«˜è®¡ç®—æ•ˆç‡</u>ã€‚

å¦å¤–ä¸€ä¸ªåŸå› ï¼šæƒ³è±¡ä¸€ä¸‹ï¼Œå‡è®¾ä½ æƒ³ä»ä¸€å¼ å›¾ç‰‡ä¸­æ‰¾åˆ°æŸä¸ªç‰©ä½“ã€‚ åˆç†çš„å‡è®¾æ˜¯ï¼šæ— è®ºå“ªç§æ–¹æ³•æ‰¾åˆ°è¿™ä¸ªç‰©ä½“ï¼Œéƒ½åº”è¯¥å’Œç‰©ä½“çš„ä½ç½®æ— å…³ã€‚ ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿåˆ©ç”¨å¸¸è¯†ï¼šçŒªé€šå¸¸ä¸åœ¨å¤©ä¸Šé£ï¼Œé£æœºé€šå¸¸ä¸åœ¨æ°´é‡Œæ¸¸æ³³ã€‚ ä½†æ˜¯ï¼Œå¦‚æœä¸€åªçŒªå‡ºç°åœ¨å›¾ç‰‡é¡¶éƒ¨ï¼Œæˆ‘ä»¬è¿˜æ˜¯åº”è¯¥è®¤å‡ºå®ƒã€‚ æˆ‘ä»¬å¯ä»¥ä»å„¿ç«¥æ¸¸æˆâ€æ²ƒå°”å¤šåœ¨å“ªé‡Œâ€ä¸­å¾—åˆ°çµæ„Ÿï¼š åœ¨è¿™ä¸ªæ¸¸æˆä¸­åŒ…å«äº†è®¸å¤šå……æ–¥ç€æ´»åŠ¨çš„æ··ä¹±åœºæ™¯ï¼Œè€Œæ²ƒå°”å¤šé€šå¸¸æ½œä¼åœ¨ä¸€äº›ä¸å¤ªå¯èƒ½çš„ä½ç½®ï¼Œè¯»è€…çš„ç›®æ ‡å°±æ˜¯æ‰¾å‡ºä»–ã€‚ å°½ç®¡æ²ƒå°”å¤šçš„è£…æ‰®å¾ˆæœ‰ç‰¹ç‚¹ï¼Œä½†æ˜¯åœ¨çœ¼èŠ±ç¼­ä¹±çš„åœºæ™¯ä¸­æ‰¾åˆ°ä»–ä¹Ÿå¦‚å¤§æµ·æé’ˆã€‚ ç„¶è€Œæ²ƒå°”å¤šçš„æ ·å­å¹¶ä¸å–å†³äºä»–æ½œè—çš„åœ°æ–¹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªâ€œæ²ƒå°”å¤šæ£€æµ‹å™¨â€æ‰«æå›¾åƒã€‚ è¯¥æ£€æµ‹å™¨å°†å›¾åƒåˆ†å‰²æˆå¤šä¸ªåŒºåŸŸï¼Œå¹¶ä¸ºæ¯ä¸ªåŒºåŸŸåŒ…å«æ²ƒå°”å¤šçš„å¯èƒ½æ€§æ‰“åˆ†ã€‚ å·ç§¯ç¥ç»ç½‘ç»œæ­£æ˜¯å°†*ç©ºé—´ä¸å˜æ€§*ï¼ˆspatial invarianceï¼‰çš„è¿™ä¸€æ¦‚å¿µç³»ç»ŸåŒ–ï¼Œä»è€ŒåŸºäºè¿™ä¸ªæ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„å‚æ•°æ¥å­¦ä¹ æœ‰ç”¨çš„è¡¨ç¤ºã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä¸Šè¿°æƒ³æ³•æ€»ç»“ä¸€ä¸‹ï¼Œä»è€Œå¸®åŠ©æˆ‘ä»¬è®¾è®¡é€‚åˆäºè®¡ç®—æœºè§†è§‰çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼š

1. *å¹³ç§»ä¸å˜æ€§*ï¼ˆtranslation invarianceï¼‰ï¼šä¸ç®¡æ£€æµ‹å¯¹è±¡å‡ºç°åœ¨å›¾åƒä¸­çš„å“ªä¸ªä½ç½®ï¼Œç¥ç»ç½‘ç»œçš„å‰é¢å‡ å±‚åº”è¯¥å¯¹ç›¸åŒçš„å›¾åƒåŒºåŸŸå…·æœ‰ç›¸ä¼¼çš„ååº”ï¼Œå³ä¸ºâ€œå¹³ç§»ä¸å˜æ€§â€ã€‚
2. *å±€éƒ¨æ€§*ï¼ˆlocalityï¼‰ï¼šç¥ç»ç½‘ç»œçš„å‰é¢å‡ å±‚åº”è¯¥åªæ¢ç´¢è¾“å…¥å›¾åƒä¸­çš„å±€éƒ¨åŒºåŸŸï¼Œè€Œä¸è¿‡åº¦åœ¨æ„å›¾åƒä¸­ç›¸éš”è¾ƒè¿œåŒºåŸŸçš„å…³ç³»ï¼Œè¿™å°±æ˜¯â€œå±€éƒ¨æ€§â€åŸåˆ™ã€‚æœ€ç»ˆï¼Œå¯ä»¥èšåˆè¿™äº›å±€éƒ¨ç‰¹å¾ï¼Œä»¥åœ¨æ•´ä¸ªå›¾åƒçº§åˆ«è¿›è¡Œé¢„æµ‹ã€‚

:::

è¿›è¡Œå·ç§¯æ“ä½œä¹‹å‰éœ€è¦å®šä¹‰ä¸€ä¸ª**è¿‡æ»¤å™¨**ï¼Œå…¶ä¸­æ¯ä¸€æ ¼éƒ½æœ‰ä¸€ä¸ªæƒé‡å€¼ã€‚å·ç§¯çš„è¿‡ç¨‹å°±æ˜¯å°†æ ¼å­ä¸­çš„æƒé‡å€¼ä¸å›¾ç‰‡å¯¹åº”çš„åƒç´ å€¼ç›¸ä¹˜å¹¶ç´¯åŠ ã€‚å¾—åˆ°çš„éšå«å±‚çš„ç»“æœå°±æ˜¯æˆ‘ä»¬é€šè¿‡å·ç§¯ç”Ÿæˆçš„**ç‰¹å¾å›¾**ï¼š

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220119152953200.png" alt="image-20220119152953200" style="zoom: 40%;" />

é‡è¦æœ¯è¯­ï¼šå·ç§¯è®¡ç®—ã€è¿‡æ»¤å™¨ï¼ˆå·ç§¯æ ¸ï¼‰ã€æ­¥é•¿ï¼ˆstrideï¼‰ã€å¡«å……ï¼ˆpaddingï¼‰

### 7.2 æ± åŒ–

**æ± åŒ–**çš„ç›®çš„æ˜¯é™ä½æ•°æ®çš„ç»´åº¦ï¼Œè¿‡ç¨‹å®é™…ä¸Šå°±æ˜¯**ä¸‹é‡‡æ ·**ï¼š

![image-20220119155254565](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220119155254565.png)

å›¾ç¤ºä¸ºæœ€å¤§æ± åŒ–ï¼Œå®é™…åº”ç”¨ä¸­ç”Ÿæˆæ± åŒ–ç‰¹å¾çš„æ–¹å¼ä¸€èˆ¬æœ‰ä¸¤ç§ï¼š

+ **æœ€å¤§å€¼æ± åŒ–**ï¼ˆMax-Poolingï¼‰ï¼šå°†æ± åŒ–çª—å£å†…çš„æœ€å¤§å€¼ä½œä¸ºæ± åŒ–ç»“æœçš„ç‰¹å¾å€¼
+ **å¹³å‡å€¼æ± åŒ–**ï¼ˆMean-Poolingï¼‰ï¼šå°†æ± åŒ–çª—å£å†…çš„æ‰€æœ‰å€¼çš„å¹³å‡å€¼ä½œä¸ºæ± åŒ–ç»“æœçš„ç‰¹å¾å€¼