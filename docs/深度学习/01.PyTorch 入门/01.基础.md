---
title: 基础
date: 2022-01-16 17:38:57
permalink: /pages/4264b9/
categories:
  - 深度学习
  - PyTorch 入门
tags:
  - 
---

## 1. Autograd

**Autograd** 中文是**自动微分**，是神经网络优化的核心。

### 1.1 微分示例

假设一个向量  $\vec{x} = [1, 1]^T$ 作为输入，乘以 4 得到向量 $\vec{z}$ ，最后求其长度得到标量 $y$，值为 5.6569，这个计算过程如下：

$y = \sqrt{z_1^2+z_2^2} = \sqrt{(4x_1)^2 + (4x_2)^2} = 4 \sqrt{x_1^2+x_2^2}$

因此 $y$ 关于 $x_1$ 的微分是：

$\frac{\partial y}{\partial x_1} = \frac{\partial (4 \sqrt{x_1^2+x_2^2})}{\partial x_1} \approx 2.8284$ 

当输入和计算变得更为复杂时，PyTorch 的 Autograd 技术就可以帮助我们自动去求这些微分值。

### 1.2 基本原理

上面的计算过程中，$\vec{x}、\vec{z}$ 和 $y$ 都被当做**节点**，运行过程被抽象为**信息流**，复杂的计算也可以被抽象成一张**计算图**：

 ![image-20220116182148630](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116182148630.png)

微分示例中，$\vec{x}$ 是叶子节点，$\vec{z}$ 是中间节点，$y$ 是输出节点，他们三者都是   Tensor。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116183156678.png" alt="image-20220116183156678" style="zoom:80%;"  />

<center>计算图：绿色是叶子节点，橙色是中间节点，红色是输出节点，蓝色箭头表示信息流</center>

Tensor 在自动微分方面有三个重要属性：

+ **requires_grad**：一个布尔值，默认 False，当其为 True 时表示该 Tensor 需要自动微分
+ **grad**：用于存储 Tensor 的微分值
+ **grad_fn**：用于存储 Tensor 的微分函数

当叶子节点的 `requires_grad` 为 True 时，**信息流经过该节点时，所有中间节点的 `requires_grad`  属性都会变成 True**，只要在输出节点调用反向传播函数 `backward()`，PyTorch 就会自动求出叶子节点的微分值并更新存储在叶子节点的 grad 属性。注意，**只有叶子节点的 `grad` 属性能被更新**。

### 1.3 前向传播

Autograd 技术可以帮助我们从叶子节点开始追踪信息流，记下整个过程使用的函数，知道输出节点，这个过程被称为**前向传播**。

首先初始化叶子节点 $\vec{x}$：

```python
x = torch.one(2)
x.requires_grad
```

打印出 False，因为默认情况下 Tensor 的 `requires_grad` 为 False。为了让 PyTorch 帮我们自动求微分，我们需要将其设为 True：

```python
>>> X.requires_grad = True
>>> X
tensor([1., 1.], requires_grad=True)
```

此时 $\vec{x}$  的 `grad` 和 `grad_fn` 属性为空。接下来我们计算 $\vec{z}$：

```python
>>> z = 4 * X
>>> z
tensor([4., 4.], grad_fn=<MulBackward0>)
```

这里面的 `grad_fn` 是<u>微分函数</u>，在此处是乘法的反向函数。最后我们用 norm() 函数求其长度得到 y：

```python
>>> y = z.norm()
>>> y
tensor(5.6569, grad_fn=<CopyBackwards>)
```

### 1.4 反向传播

接下来，调用输出节点的 `backward()` 函数对整个图进行反向传播，求出微分值：

```python
>>> y.backward()
>>> X.grad
tensor([2.8284, 2.8284])
```

运行后可以发现 $\vec{x}$ 的 grad 属性更新为 $\vec{x}$ 的微分值，这个结果与我们人工计算的结果一致。

再查看一下 $\vec{z}$ 和 $y$ 的 grad 值，发现并没有改变，因为他们都不是叶子节点：

```python
>>> z.grad
>>> y.grad
```

## 2. 线性回归

本节我们将实现一个**线性回归**（LR）模型。

### 2.1 理论分析

#### 1）准备数据

```python
import torch
import matplotlib.pyplot as plt

x = torch.Tensor([1.4, 5, 11, 16, 21])
y = torch.Tensor([14.4, 29.6, 62, 85, 113.4])

plt.scatter(x.numpy(), y.numpy())
plt.show()
```

得到图：![image-20220116200141925](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116200141925.png)

#### 2）目标函数

因为我们假设用一条直线去拟合，所以可以假设该函数为：$y=w_1x+w_0$。我们的目标就是找到一组合适的 $(w_1, w_0)$。

我们把上面 y 改写一下得到：$\hat{y}=w_1x+w_0$，这样 $\hat{y}^{(i)}$ 是由样本中的 $x^{(i)}$ 传入线性模型后计算得到的输出，$y^{(i)}$ 是我们真实样本值。因为测量会产生误差，我们用一个函数来衡量 $\hat{y}^{(i)}$ 和 $y^{(i)}$ 之间的误差，这个函数就是**损失函数**。在这里，我们采用的损失函数是均方误差函数（Mean-Square Error，**MSE**）：

$L(w_1, w_0) = \sum^5_{i=1}(\hat{y}^{(i)} - y^{(i)})^2$

因此，我们的目标就是找一组合适的 $(w_1, w_0)$ 使得损失函数的 L 值最小。

#### 3）优化

为了让损失函数值 L 降到最小，我们就要开始调整参数 $(w_1, w_0)$ 的值了！这个过程被称为**优化**。这里我们采用一种**梯度下降**的方法来寻找这个函数的最小值。

L 的梯度是：$\nabla L = (\frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_0})$

这样优化的过程就是做这样的运算：$\vec{w}^{t+1} = \vec{w}^t - \nabla L(\vec{w}^t) \times \delta$，其中 $\delta$ 是学习率。

#### 4）批量输入

上面的表达式是一次一个样本的形式，在实际的优化中，我们是让多个样本同时在一个公式中出现，所有公式中的 $\vec{x}$ 和 $\hat{y}$ 都要增加一个维度，$\vec{x}$ 升级为矩阵 $\boldsymbol{X}$，$\hat{y}$ 升级为 $\vec{\hat{y}}$ ，最终结果为：$\vec{\hat{y}} = \boldsymbol{X} \cdot \vec{w}$

损失函数 L 可以表示为：$L(w_1, w_0) = |\vec{\hat{y}} - \vec{y}|^2$

#### 5）训练

训练就是不断地通过前向传播和反向传播，对参数 $\vec{w}$ 进行调优，最终让损失函数的损失值 L 达到最小的过程：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116203409197.png" alt="image-20220116203409197" style="zoom:80%;" />

### 2.2 代码实现

#### 1）准备数据

x，y 仍然使用我们之前的数据，我们首先对输入变量和各参数进行初始化：

```python
import torch
import matplotlib.pyplot as plt

# 准备数据

# 生成矩阵X
def Produce_X(x):
	x0 = torch.ones(x.numpy().size) # 用ones产生初始值为1，大小与x相同的向量
	X = torch.stack((x,x0),dim=1)   # stack函数将两个向量拼合
	return X


x = torch.Tensor([1.4,5,11,16,21])
y = torch.Tensor([14.4,29.6,62,85.5,113.4])
X = Produce_X(x)

# 定义权重w的变量
w = torch.rand(2,requires_grad=True)

inputs = X 
target = y
```

+ `Produce_X` 函数将 x 与一个与之相同形状的全 1 向量进行合并得到一个 X，它的实际数据如下：

  ```
  tensor([[ 1.4000,  1.0000],
          [ 5.0000,  1.0000],
          [11.0000,  1.0000],
          [16.0000,  1.0000],
          [21.0000,  1.0000]])
  ```

  这样 X 与 $\vec{w}$ 的乘积便相当于一个 $y = w_1x + w_0$。

+ 用 `rand()` 函数来初始化参数向量 $\vec{w}$，根据 Autograd 中所介绍的，参数 w 属于计算图的叶子节点，需要进行自动微分并利用梯度下降来更新，因此需要专门将 w 的 `requires_grad` 设置为 True。 

#### 2）训练

每一轮的训练分成两部分：前向传播和反向传播

```python
#训练
def train(epochs=1,learning_rate=0.01):
	for epoch in range(epochs):

		#前向传播
		output = inputs.mv(w) #公式：y=Xw
		loss = (output - target).pow(2).sum() # 公式：L = ∑(y-y')^2

		#反向传播
		loss.backward() 
		w.data -= learning_rate * w.grad  # 更新权重w，公式：w_(t+1)= w_(t) - 𝜼*▽J
		
		w.grad.zero_() # 清空grad的值

		if epoch % 80 == 0:
			draw(output,loss)

	#plt.savefig('plot1.png', format='png')

	return w, loss
```

+ 注意，**我们更新完 w 后，必须清空 w 的 grad 的值，否则 grad 的值会持续累加**。所以，这里使用 `zero_()` 函数来清空梯度值。

为了能够观察到训练的变化，我们可以让程序每进行 80 次循环就更新一次图像，于是定义一个 `draw()` 函数：

```python
#绘图
def draw(output,loss):
	plt.cla() # 清空函数图像
	plt.scatter(x.numpy(), y.numpy()) # 绘制散点图
	
	plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5) # 绘制出回归直线
	plt.text(0.5, 0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.text(3, 9,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.axis([10, 160, 0, 0.03])

	plt.pause(0.005)
```

于是便可以训练了：

```python
w, loss = train(10000,learning_rate = 1e-4)  #学习率设置为1x10^(-4)
```

训练完之后打印最终结果：

```python
print("final loss:", loss.item())
print("weights:", w.data)
```

运行结果：

```
final loss: 8.2430419921875
weights: tensor([5.0840, 5.5849])
```

