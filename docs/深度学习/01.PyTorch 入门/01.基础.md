---
title: åŸºç¡€
date: 2022-01-16 17:38:57
permalink: /pages/4264b9/
categories:
  - æ·±åº¦å­¦ä¹ 
  - PyTorch å…¥é—¨
tags:
  - 
---

## 1. Autograd

**Autograd** ä¸­æ–‡æ˜¯**è‡ªåŠ¨å¾®åˆ†**ï¼Œæ˜¯ç¥ç»ç½‘ç»œä¼˜åŒ–çš„æ ¸å¿ƒã€‚

### 1.1 å¾®åˆ†ç¤ºä¾‹

å‡è®¾ä¸€ä¸ªå‘é‡  $\vec{x} = [1, 1]^T$ ä½œä¸ºè¾“å…¥ï¼Œä¹˜ä»¥ 4 å¾—åˆ°å‘é‡ $\vec{z}$ ï¼Œæœ€åæ±‚å…¶é•¿åº¦å¾—åˆ°æ ‡é‡ $y$ï¼Œå€¼ä¸º 5.6569ï¼Œè¿™ä¸ªè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

$y = \sqrt{z_1^2+z_2^2} = \sqrt{(4x_1)^2 + (4x_2)^2} = 4 \sqrt{x_1^2+x_2^2}$

å› æ­¤ $y$ å…³äº $x_1$ çš„å¾®åˆ†æ˜¯ï¼š

$\frac{\partial y}{\partial x_1} = \frac{\partial (4 \sqrt{x_1^2+x_2^2})}{\partial x_1} \approx 2.8284$ 

å½“è¾“å…¥å’Œè®¡ç®—å˜å¾—æ›´ä¸ºå¤æ‚æ—¶ï¼ŒPyTorch çš„ Autograd æŠ€æœ¯å°±å¯ä»¥å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨å»æ±‚è¿™äº›å¾®åˆ†å€¼ã€‚

### 1.2 åŸºæœ¬åŸç†

ä¸Šé¢çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œ$\vec{x}ã€\vec{z}$ å’Œ $y$ éƒ½è¢«å½“åš**èŠ‚ç‚¹**ï¼Œè¿è¡Œè¿‡ç¨‹è¢«æŠ½è±¡ä¸º**ä¿¡æ¯æµ**ï¼Œå¤æ‚çš„è®¡ç®—ä¹Ÿå¯ä»¥è¢«æŠ½è±¡æˆä¸€å¼ **è®¡ç®—å›¾**ï¼š

 ![image-20220116182148630](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116182148630.png)

å¾®åˆ†ç¤ºä¾‹ä¸­ï¼Œ$\vec{x}$ æ˜¯å¶å­èŠ‚ç‚¹ï¼Œ$\vec{z}$ æ˜¯ä¸­é—´èŠ‚ç‚¹ï¼Œ$y$ æ˜¯è¾“å‡ºèŠ‚ç‚¹ï¼Œä»–ä»¬ä¸‰è€…éƒ½æ˜¯   Tensorã€‚

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116183156678.png" alt="image-20220116183156678" style="zoom:80%;"  />

<center>è®¡ç®—å›¾ï¼šç»¿è‰²æ˜¯å¶å­èŠ‚ç‚¹ï¼Œæ©™è‰²æ˜¯ä¸­é—´èŠ‚ç‚¹ï¼Œçº¢è‰²æ˜¯è¾“å‡ºèŠ‚ç‚¹ï¼Œè“è‰²ç®­å¤´è¡¨ç¤ºä¿¡æ¯æµ</center>

Tensor åœ¨è‡ªåŠ¨å¾®åˆ†æ–¹é¢æœ‰ä¸‰ä¸ªé‡è¦å±æ€§ï¼š

+ **requires_grad**ï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œé»˜è®¤ Falseï¼Œå½“å…¶ä¸º True æ—¶è¡¨ç¤ºè¯¥ Tensor éœ€è¦è‡ªåŠ¨å¾®åˆ†
+ **grad**ï¼šç”¨äºå­˜å‚¨ Tensor çš„å¾®åˆ†å€¼
+ **grad_fn**ï¼šç”¨äºå­˜å‚¨ Tensor çš„å¾®åˆ†å‡½æ•°

å½“å¶å­èŠ‚ç‚¹çš„ `requires_grad` ä¸º True æ—¶ï¼Œ**ä¿¡æ¯æµç»è¿‡è¯¥èŠ‚ç‚¹æ—¶ï¼Œæ‰€æœ‰ä¸­é—´èŠ‚ç‚¹çš„ `requires_grad`  å±æ€§éƒ½ä¼šå˜æˆ True**ï¼Œåªè¦åœ¨è¾“å‡ºèŠ‚ç‚¹è°ƒç”¨åå‘ä¼ æ’­å‡½æ•° `backward()`ï¼ŒPyTorch å°±ä¼šè‡ªåŠ¨æ±‚å‡ºå¶å­èŠ‚ç‚¹çš„å¾®åˆ†å€¼å¹¶æ›´æ–°å­˜å‚¨åœ¨å¶å­èŠ‚ç‚¹çš„ grad å±æ€§ã€‚æ³¨æ„ï¼Œ**åªæœ‰å¶å­èŠ‚ç‚¹çš„ `grad` å±æ€§èƒ½è¢«æ›´æ–°**ã€‚

### 1.3 å‰å‘ä¼ æ’­

Autograd æŠ€æœ¯å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä»å¶å­èŠ‚ç‚¹å¼€å§‹è¿½è¸ªä¿¡æ¯æµï¼Œè®°ä¸‹æ•´ä¸ªè¿‡ç¨‹ä½¿ç”¨çš„å‡½æ•°ï¼ŒçŸ¥é“è¾“å‡ºèŠ‚ç‚¹ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º**å‰å‘ä¼ æ’­**ã€‚

é¦–å…ˆåˆå§‹åŒ–å¶å­èŠ‚ç‚¹ $\vec{x}$ï¼š

```python
x = torch.one(2)
x.requires_grad
```

æ‰“å°å‡º Falseï¼Œå› ä¸ºé»˜è®¤æƒ…å†µä¸‹ Tensor çš„ `requires_grad` ä¸º Falseã€‚ä¸ºäº†è®© PyTorch å¸®æˆ‘ä»¬è‡ªåŠ¨æ±‚å¾®åˆ†ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è®¾ä¸º Trueï¼š

```python
>>> X.requires_grad = True
>>> X
tensor([1., 1.], requires_grad=True)
```

æ­¤æ—¶ $\vec{x}$  çš„ `grad` å’Œ `grad_fn` å±æ€§ä¸ºç©ºã€‚æ¥ä¸‹æ¥æˆ‘ä»¬è®¡ç®— $\vec{z}$ï¼š

```python
>>> z = 4 * X
>>> z
tensor([4., 4.], grad_fn=<MulBackward0>)
```

è¿™é‡Œé¢çš„ `grad_fn` æ˜¯<u>å¾®åˆ†å‡½æ•°</u>ï¼Œåœ¨æ­¤å¤„æ˜¯ä¹˜æ³•çš„åå‘å‡½æ•°ã€‚æœ€åæˆ‘ä»¬ç”¨ norm() å‡½æ•°æ±‚å…¶é•¿åº¦å¾—åˆ° yï¼š

```python
>>> y = z.norm()
>>> y
tensor(5.6569, grad_fn=<CopyBackwards>)
```

### 1.4 åå‘ä¼ æ’­

æ¥ä¸‹æ¥ï¼Œè°ƒç”¨è¾“å‡ºèŠ‚ç‚¹çš„ `backward()` å‡½æ•°å¯¹æ•´ä¸ªå›¾è¿›è¡Œåå‘ä¼ æ’­ï¼Œæ±‚å‡ºå¾®åˆ†å€¼ï¼š

```python
>>> y.backward()
>>> X.grad
tensor([2.8284, 2.8284])
```

è¿è¡Œåå¯ä»¥å‘ç° $\vec{x}$ çš„ grad å±æ€§æ›´æ–°ä¸º $\vec{x}$ çš„å¾®åˆ†å€¼ï¼Œè¿™ä¸ªç»“æœä¸æˆ‘ä»¬äººå·¥è®¡ç®—çš„ç»“æœä¸€è‡´ã€‚

å†æŸ¥çœ‹ä¸€ä¸‹ $\vec{z}$ å’Œ $y$ çš„ grad å€¼ï¼Œå‘ç°å¹¶æ²¡æœ‰æ”¹å˜ï¼Œå› ä¸ºä»–ä»¬éƒ½ä¸æ˜¯å¶å­èŠ‚ç‚¹ï¼š

```python
>>> z.grad
>>> y.grad
```

## 2. çº¿æ€§å›å½’

æœ¬èŠ‚æˆ‘ä»¬å°†å®ç°ä¸€ä¸ª**çº¿æ€§å›å½’**ï¼ˆLRï¼‰æ¨¡å‹ã€‚

### 2.1 ç†è®ºåˆ†æ

#### 1ï¼‰å‡†å¤‡æ•°æ®

```python
import torch
import matplotlib.pyplot as plt

x = torch.Tensor([1.4, 5, 11, 16, 21])
y = torch.Tensor([14.4, 29.6, 62, 85, 113.4])

plt.scatter(x.numpy(), y.numpy())
plt.show()
```

å¾—åˆ°å›¾ï¼š![image-20220116200141925](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116200141925.png)

#### 2ï¼‰ç›®æ ‡å‡½æ•°

å› ä¸ºæˆ‘ä»¬å‡è®¾ç”¨ä¸€æ¡ç›´çº¿å»æ‹Ÿåˆï¼Œæ‰€ä»¥å¯ä»¥å‡è®¾è¯¥å‡½æ•°ä¸ºï¼š$y=w_1x+w_0$ã€‚æˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°ä¸€ç»„åˆé€‚çš„ $(w_1, w_0)$ã€‚

æˆ‘ä»¬æŠŠä¸Šé¢ y æ”¹å†™ä¸€ä¸‹å¾—åˆ°ï¼š$\hat{y}=w_1x+w_0$ï¼Œè¿™æ · $\hat{y}^{(i)}$ æ˜¯ç”±æ ·æœ¬ä¸­çš„ $x^{(i)}$ ä¼ å…¥çº¿æ€§æ¨¡å‹åè®¡ç®—å¾—åˆ°çš„è¾“å‡ºï¼Œ$y^{(i)}$ æ˜¯æˆ‘ä»¬çœŸå®æ ·æœ¬å€¼ã€‚å› ä¸ºæµ‹é‡ä¼šäº§ç”Ÿè¯¯å·®ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå‡½æ•°æ¥è¡¡é‡ $\hat{y}^{(i)}$ å’Œ $y^{(i)}$ ä¹‹é—´çš„è¯¯å·®ï¼Œè¿™ä¸ªå‡½æ•°å°±æ˜¯**æŸå¤±å‡½æ•°**ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å‡æ–¹è¯¯å·®å‡½æ•°ï¼ˆMean-Square Errorï¼Œ**MSE**ï¼‰ï¼š

$L(w_1, w_0) = \sum^5_{i=1}(\hat{y}^{(i)} - y^{(i)})^2$

å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ‰¾ä¸€ç»„åˆé€‚çš„ $(w_1, w_0)$ ä½¿å¾—æŸå¤±å‡½æ•°çš„ L å€¼æœ€å°ã€‚

#### 3ï¼‰ä¼˜åŒ–

ä¸ºäº†è®©æŸå¤±å‡½æ•°å€¼ L é™åˆ°æœ€å°ï¼Œæˆ‘ä»¬å°±è¦å¼€å§‹è°ƒæ•´å‚æ•° $(w_1, w_0)$ çš„å€¼äº†ï¼è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º**ä¼˜åŒ–**ã€‚è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç§**æ¢¯åº¦ä¸‹é™**çš„æ–¹æ³•æ¥å¯»æ‰¾è¿™ä¸ªå‡½æ•°çš„æœ€å°å€¼ã€‚

L çš„æ¢¯åº¦æ˜¯ï¼š$\nabla L = (\frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_0})$

è¿™æ ·ä¼˜åŒ–çš„è¿‡ç¨‹å°±æ˜¯åšè¿™æ ·çš„è¿ç®—ï¼š$\vec{w}^{t+1} = \vec{w}^t - \nabla L(\vec{w}^t) \times \delta$ï¼Œå…¶ä¸­ $\delta$ æ˜¯å­¦ä¹ ç‡ã€‚

#### 4ï¼‰æ‰¹é‡è¾“å…¥

ä¸Šé¢çš„è¡¨è¾¾å¼æ˜¯ä¸€æ¬¡ä¸€ä¸ªæ ·æœ¬çš„å½¢å¼ï¼Œåœ¨å®é™…çš„ä¼˜åŒ–ä¸­ï¼Œæˆ‘ä»¬æ˜¯è®©å¤šä¸ªæ ·æœ¬åŒæ—¶åœ¨ä¸€ä¸ªå…¬å¼ä¸­å‡ºç°ï¼Œæ‰€æœ‰å…¬å¼ä¸­çš„ $\vec{x}$ å’Œ $\hat{y}$ éƒ½è¦å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œ$\vec{x}$ å‡çº§ä¸ºçŸ©é˜µ $\boldsymbol{X}$ï¼Œ$\hat{y}$ å‡çº§ä¸º $\vec{\hat{y}}$ ï¼Œæœ€ç»ˆç»“æœä¸ºï¼š$\vec{\hat{y}} = \boldsymbol{X} \cdot \vec{w}$

æŸå¤±å‡½æ•° L å¯ä»¥è¡¨ç¤ºä¸ºï¼š$L(w_1, w_0) = |\vec{\hat{y}} - \vec{y}|^2$

#### 5ï¼‰è®­ç»ƒ

è®­ç»ƒå°±æ˜¯ä¸æ–­åœ°é€šè¿‡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œå¯¹å‚æ•° $\vec{w}$ è¿›è¡Œè°ƒä¼˜ï¼Œæœ€ç»ˆè®©æŸå¤±å‡½æ•°çš„æŸå¤±å€¼ L è¾¾åˆ°æœ€å°çš„è¿‡ç¨‹ï¼š

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116203409197.png" alt="image-20220116203409197" style="zoom:80%;" />

### 2.2 ä»£ç å®ç°

#### 1ï¼‰å‡†å¤‡æ•°æ®

xï¼Œy ä»ç„¶ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰çš„æ•°æ®ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥å˜é‡å’Œå„å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼š

```python
import torch
import matplotlib.pyplot as plt

# å‡†å¤‡æ•°æ®

# ç”ŸæˆçŸ©é˜µX
def Produce_X(x):
	x0 = torch.ones(x.numpy().size) # ç”¨onesäº§ç”Ÿåˆå§‹å€¼ä¸º1ï¼Œå¤§å°ä¸xç›¸åŒçš„å‘é‡
	X = torch.stack((x,x0),dim=1)   # stackå‡½æ•°å°†ä¸¤ä¸ªå‘é‡æ‹¼åˆ
	return X


x = torch.Tensor([1.4,5,11,16,21])
y = torch.Tensor([14.4,29.6,62,85.5,113.4])
X = Produce_X(x)

# å®šä¹‰æƒé‡wçš„å˜é‡
w = torch.rand(2,requires_grad=True)

inputs = X 
target = y
```

+ `Produce_X` å‡½æ•°å°† x ä¸ä¸€ä¸ªä¸ä¹‹ç›¸åŒå½¢çŠ¶çš„å…¨ 1 å‘é‡è¿›è¡Œåˆå¹¶å¾—åˆ°ä¸€ä¸ª Xï¼Œå®ƒçš„å®é™…æ•°æ®å¦‚ä¸‹ï¼š

  ```
  tensor([[ 1.4000,  1.0000],
          [ 5.0000,  1.0000],
          [11.0000,  1.0000],
          [16.0000,  1.0000],
          [21.0000,  1.0000]])
  ```

  è¿™æ · X ä¸ $\vec{w}$ çš„ä¹˜ç§¯ä¾¿ç›¸å½“äºä¸€ä¸ª $y = w_1x + w_0$ã€‚

+ ç”¨ `rand()` å‡½æ•°æ¥åˆå§‹åŒ–å‚æ•°å‘é‡ $\vec{w}$ï¼Œæ ¹æ® Autograd ä¸­æ‰€ä»‹ç»çš„ï¼Œå‚æ•° w å±äºè®¡ç®—å›¾çš„å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦è¿›è¡Œè‡ªåŠ¨å¾®åˆ†å¹¶åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°ï¼Œå› æ­¤éœ€è¦ä¸“é—¨å°† w çš„ `requires_grad` è®¾ç½®ä¸º Trueã€‚ 

#### 2ï¼‰è®­ç»ƒ

æ¯ä¸€è½®çš„è®­ç»ƒåˆ†æˆä¸¤éƒ¨åˆ†ï¼šå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­

```python
#è®­ç»ƒ
def train(epochs=1,learning_rate=0.01):
	for epoch in range(epochs):

		#å‰å‘ä¼ æ’­
		output = inputs.mv(w) #å…¬å¼ï¼šy=Xw
		loss = (output - target).pow(2).sum() # å…¬å¼ï¼šL = âˆ‘(y-y')^2

		#åå‘ä¼ æ’­
		loss.backward() 
		w.data -= learning_rate * w.grad  # æ›´æ–°æƒé‡wï¼Œå…¬å¼ï¼šw_(t+1)= w_(t) - ğœ¼*â–½J
		
		w.grad.zero_() # æ¸…ç©ºgradçš„å€¼

		if epoch % 80 == 0:
			draw(output,loss)

	#plt.savefig('plot1.png', format='png')

	return w, loss
```

+ æ³¨æ„ï¼Œ**æˆ‘ä»¬æ›´æ–°å®Œ w åï¼Œå¿…é¡»æ¸…ç©º w çš„ grad çš„å€¼ï¼Œå¦åˆ™ grad çš„å€¼ä¼šæŒç»­ç´¯åŠ **ã€‚æ‰€ä»¥ï¼Œè¿™é‡Œä½¿ç”¨ `zero_()` å‡½æ•°æ¥æ¸…ç©ºæ¢¯åº¦å€¼ã€‚

ä¸ºäº†èƒ½å¤Ÿè§‚å¯Ÿåˆ°è®­ç»ƒçš„å˜åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥è®©ç¨‹åºæ¯è¿›è¡Œ 80 æ¬¡å¾ªç¯å°±æ›´æ–°ä¸€æ¬¡å›¾åƒï¼Œäºæ˜¯å®šä¹‰ä¸€ä¸ª `draw()` å‡½æ•°ï¼š

```python
#ç»˜å›¾
def draw(output,loss):
	plt.cla() # æ¸…ç©ºå‡½æ•°å›¾åƒ
	plt.scatter(x.numpy(), y.numpy()) # ç»˜åˆ¶æ•£ç‚¹å›¾
	
	plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5) # ç»˜åˆ¶å‡ºå›å½’ç›´çº¿
	plt.text(0.5, 0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.text(3, 9,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.axis([10, 160, 0, 0.03])

	plt.pause(0.005)
```

äºæ˜¯ä¾¿å¯ä»¥è®­ç»ƒäº†ï¼š

```python
w, loss = train(10000,learning_rate = 1e-4)  #å­¦ä¹ ç‡è®¾ç½®ä¸º1x10^(-4)
```

è®­ç»ƒå®Œä¹‹åæ‰“å°æœ€ç»ˆç»“æœï¼š

```python
print("final loss:", loss.item())
print("weights:", w.data)
```

è¿è¡Œç»“æœï¼š

```
final loss: 8.2430419921875
weights: tensor([5.0840, 5.5849])
```

### 2.3 å¤§è§„æ¨¡æ•°æ®é›†å®ä¾‹

ä¹‹å‰è®­ç»ƒæ—¶æˆ‘ä»¬å°† 5 ä¸ªæ•°æ®æ ·æœ¬åŒæ—¶è¾“å…¥ç¨‹åºï¼Œè¿™ç§æ–¹å¼å«åš**æ‰¹è¾“å…¥**ï¼Œè¿™ç§æ–¹å¼æ˜¯å¿«é€Ÿè€Œæœ‰æ•ˆçš„ã€‚

äººä»¬é€šè¿‡å¯¹ç¥ç»å…ƒçš„ç ”ç©¶ï¼Œå¯¹å…¶è¿›è¡Œæ•°å­¦æŠ½è±¡å¾—åˆ°äº†**äººå·¥ç¥ç»å…ƒæ¨¡å‹**ï¼š

![image-20220117175443927](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220117175443927.png)

PyTorch ä¸ºæˆ‘ä»¬é¢„å…ˆç¼–å†™å¥½äº†æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å‡½æ•°ç­‰ï¼Œæˆ‘ä»¬å°†ä»£ç å†é‡æ–°ç¼–å†™ä¸€æ¬¡ï¼š

#### 1ï¼‰å‡†å¤‡æ•°æ®

```python
import torch
import matplotlib.pyplot as plt
from torch import nn, optim
from time import perf_counter

# ç”¨linspaceäº§ç”Ÿï¼ˆ-3ï¼Œ3ï¼‰åŒºé—´å†…çš„100000ä¸ªç‚¹ï¼Œå¹¶ä½¿ç”¨unsqueezeå‡½æ•°å¢åŠ ä¸€ä¸ªç»´åº¦
x = torch.unsqueeze(torch.linspace(-3,3,100000),dim=1)

# å‡è®¾çœŸå®å‡½æ•°æ˜¯y=xï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢å¢åŠ ä¸€äº›è¯¯å·®ï¼Œæ›´åŠ ç¬¦åˆå®é™…æƒ…å†µ
y = x + 1.2 * torch.rand(x.size())
```

#### 2ï¼‰å®šä¹‰æ¨¡å‹

å®šä¹‰ä¸€ä¸ªçº¿æ€§å›å½’çš„æ¨¡å‹ LRï¼Œå®ƒç»§æ‰¿è‡ª `nn.Module`ï¼Œå¹¶åœ¨å…¶ä¸­ä½¿ç”¨ `nn.Linear()` æ„é€ çº¿æ€§æ¨¡å‹ï¼š

```python
class LR(nn.Module):
    def __init__(self):
        super(LR,self).__init__()
        self.linear = nn.Linear(1,1)

    def forward(self,x):
        out = self.linear(x)
        return out
```

+ `nn.Linear()` çš„ç¬¬ä¸€ä¸ªå‚æ•°ä»£è¡¨è¾“å…¥æ•°æ®çš„ç»´åº¦ï¼Œç¬¬äºŒä¸ªå‚æ•°ä»£è¡¨è¾“å‡ºæ•°æ®çš„ç»´åº¦ã€‚è¿™é‡Œ x å’Œ y éƒ½æ˜¯ä¸€ç»´çš„ï¼Œå› æ­¤è®¾ç½®ä¸º `nn.Linear(1, 1)`ã€‚
+ `forward()` å‡½æ•°æ¥æ„é€ ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­çš„è®¡ç®—æ­¥éª¤ã€‚

#### 3ï¼‰å®ä¾‹åŒ–æ¨¡å‹

å¦‚æœå¹³å°æ”¯æŒ CUDAï¼Œå®ä¾‹åŒ– LR ç±»åéœ€è¦è°ƒç”¨ `cuda()` æ–¹æ³•ï¼š

```python
#å¦‚æœæ”¯æŒCUDAï¼Œåˆ™é‡‡ç”¨CUDAåŠ é€Ÿ
CUDA = torch.cuda.is_available()

if CUDA:
	LR_model = LR().cuda()
	inputs = x.cuda()
	target = y.cuda()
else:
	LR_model = LR()
	inputs = x
	target = y
```

#### 4ï¼‰æŸå¤±å‡½æ•°

nn æ¨¡å—ä¸­é¢„è®¾æœ‰å‡æ–¹è¯¯å·®å‡½æ•°ï¼š

```python
criterion = nn.MSELoss()
```

#### 5ï¼‰ä¼˜åŒ–å™¨

ä¸‹é¢é‡‡ç”¨â€œéšæœºæ¢¯åº¦ä¸‹é™â€çš„æ–¹æ³•æ¥æ›´æ–°æƒé‡ã€‚**éšæœºæ¢¯åº¦ä¸‹é™**å®é™…ä¸Šå°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•çš„æ”¹è‰¯ç‰ˆï¼Œä¸é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•ä¸­æŠŠå…¨éƒ¨æ•°æ®æ‹¿æ¥è®¡ç®—æ¢¯åº¦çš„æ–¹æ³•ï¼Œè€Œæ˜¯<u>æ¯æ¬¡éšæœºæŒ‘é€‰ä¸€ä¸ªæ•°æ®æ ·æœ¬è®¡ç®—æ¢¯åº¦å€¼ï¼Œå¹¶è¿›è¡Œæƒå€¼æ›´æ–°</u>ã€‚è¿™æ ·åšçš„**å¥½å¤„**æ˜¯å¯ä»¥é¿å…ä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨æ•°æ®å¯¼è‡´çš„å†…å­˜æº¢å‡ºé—®é¢˜ï¼Œè¿˜å¯ä»¥é˜²æ­¢ä¼˜åŒ–çš„æ—¶å€™é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ PyTorch é¢„è®¾çš„éšæœºæ¢¯åº¦ä¸‹é™å‡½æ•° `SGD()` è¿›è¡Œæ›´æ–°ï¼š

```python
optimizer = optim.SGD(LR_model.parameters(), lr=1e-4)
```

+ `SGD()` å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯**éœ€è¦ä¼˜åŒ–çš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‚æ•°**ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯**å­¦ä¹ ç‡**ã€‚

#### 6ï¼‰è®­ç»ƒ

å¼€å§‹ç¼–å†™ `train()` å‡½æ•°ï¼Œå…¶å‚æ•°ä¾æ¬¡æ˜¯è¢«è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè®­ç»ƒè½®æ•°ï¼š

```python
def draw(output,loss):
    """å¯è§†åŒ–"""
    if CUDA:
        output = output.cpu()
    plt.cla()
    plt.scatter(x.numpy(), y.numpy())
    plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5)
    plt.text(0.5,0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
    plt.pause(0.005)

def train(model, criterion, optimizer, epochs):
    global loss
    for epoch in range(epochs):
        # forward
        output = model(inputs)
        loss = criterion(output,target)

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


        if epoch % 80 == 0:
            draw(output,loss)

    return model, loss
```

+ å¦‚æœé‡‡ç”¨äº† CUDA åŠ é€Ÿï¼Œdraw å‡½æ•°çš„ output éœ€è¦è¿˜åŸæˆ CPU çš„æ•°æ®ç±»å‹æ‰èƒ½è¿›è¡Œç»˜å›¾
+ åœ¨**å‰å‘ä¼ æ’­**é˜¶æ®µï¼Œæˆ‘ä»¬å°† inputs è¾“å…¥ç¥ç»ç½‘ç»œæ¨¡å‹ model å¾—åˆ° outputï¼Œæ¥ä¸‹æ¥ç”¨å®šä¹‰çš„æŸå¤±å‡½æ•° criterion æ¥è®¡ç®—æŸå¤±å€¼ã€‚
+ åœ¨**åå‘ä¼ æ’­**é˜¶æ®µï¼Œå…ˆç”¨ optimizer.zero_grad() æ¸…ç©ºæƒå€¼çš„ grad å€¼ï¼Œéšåç”¨ backward() è®¡ç®—æ¢¯åº¦ï¼Œå¹¶ç”¨ä¼˜åŒ–å™¨ optimizer.stip() å‡½æ•°è¿›è¡Œæƒå€¼æ›´æ–°ã€‚ 

æ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰åˆè¯•æ—¶é—´ startï¼Œå¹¶ä¼ å…¥æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ä»¥åŠè®­ç»ƒè½®æ•°ï¼ˆ10 000 æ¬¡ï¼‰ï¼š

```python {2}
start = perf_counter()
LR_model,loss = train(LR_model,criterion,optimizer,10000)
finish = perf_counter()
time = finish - start

print("è®¡ç®—æ—¶é—´:%s" % time)
print("final loss:",loss.item())
print("weights:",list(LR_model.parameters()))
```

ä»£ç çš„è®­ç»ƒç»“æœå¦‚ä¸‹ï¼š

```
è®¡ç®—æ—¶é—´:164.62969759999942
final loss: 0.12093639373779297
weights: [Parameter containing:
tensor([[0.9995]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.5632], device='cuda:0', requires_grad=True)]
```

