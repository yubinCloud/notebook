---
title: 基础
date: 2022-01-16 17:38:57
permalink: /pages/4264b9/
categories:
  - 深度学习
  - PyTorch 入门
tags:
  - 
---

## 1. Autograd

**Autograd** 中文是**自动微分**，是神经网络优化的核心。

### 1.1 微分示例

假设一个向量  $\vec{x} = [1, 1]^T$ 作为输入，乘以 4 得到向量 $\vec{z}$ ，最后求其长度得到标量 $y$，值为 5.6569，这个计算过程如下：

$y = \sqrt{z_1^2+z_2^2} = \sqrt{(4x_1)^2 + (4x_2)^2} = 4 \sqrt{x_1^2+x_2^2}$

因此 $y$ 关于 $x_1$ 的微分是：

$\frac{\partial y}{\partial x_1} = \frac{\partial (4 \sqrt{x_1^2+x_2^2})}{\partial x_1} \approx 2.8284$ 

当输入和计算变得更为复杂时，PyTorch 的 Autograd 技术就可以帮助我们自动去求这些微分值。

### 1.2 基本原理

上面的计算过程中，$\vec{x}、\vec{z}$ 和 $y$ 都被当做**节点**，运行过程被抽象为**信息流**，复杂的计算也可以被抽象成一张**计算图**：

 ![image-20220116182148630](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116182148630.png)

微分示例中，$\vec{x}$ 是叶子节点，$\vec{z}$ 是中间节点，$y$ 是输出节点，他们三者都是   Tensor。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116183156678.png" alt="image-20220116183156678" style="zoom:80%;"  />

<center>计算图：绿色是叶子节点，橙色是中间节点，红色是输出节点，蓝色箭头表示信息流</center>

Tensor 在自动微分方面有三个重要属性：

+ **requires_grad**：一个布尔值，默认 False，当其为 True 时表示该 Tensor 需要自动微分
+ **grad**：用于存储 Tensor 的微分值
+ **grad_fn**：用于存储 Tensor 的微分函数

当叶子节点的 `requires_grad` 为 True 时，**信息流经过该节点时，所有中间节点的 `requires_grad`  属性都会变成 True**，只要在输出节点调用反向传播函数 `backward()`，PyTorch 就会自动求出叶子节点的微分值并更新存储在叶子节点的 grad 属性。注意，**只有叶子节点的 `grad` 属性能被更新**。

### 1.3 前向传播

Autograd 技术可以帮助我们从叶子节点开始追踪信息流，记下整个过程使用的函数，知道输出节点，这个过程被称为**前向传播**。

首先初始化叶子节点 $\vec{x}$：

```python
x = torch.one(2)
x.requires_grad
```

打印出 False，因为默认情况下 Tensor 的 `requires_grad` 为 False。为了让 PyTorch 帮我们自动求微分，我们需要将其设为 True：

```python
>>> X.requires_grad = True
>>> X
tensor([1., 1.], requires_grad=True)
```

此时 $\vec{x}$  的 `grad` 和 `grad_fn` 属性为空。接下来我们计算 $\vec{z}$：

```python
>>> z = 4 * X
>>> z
tensor([4., 4.], grad_fn=<MulBackward0>)
```

这里面的 `grad_fn` 是<u>微分函数</u>，在此处是乘法的反向函数。最后我们用 norm() 函数求其长度得到 y：

```python
>>> y = z.norm()
>>> y
tensor(5.6569, grad_fn=<CopyBackwards>)
```

### 1.4 反向传播

接下来，调用输出节点的 `backward()` 函数对整个图进行反向传播，求出微分值：

```python
>>> y.backward()
>>> X.grad
tensor([2.8284, 2.8284])
```

运行后可以发现 $\vec{x}$ 的 grad 属性更新为 $\vec{x}$ 的微分值，这个结果与我们人工计算的结果一致。

再查看一下 $\vec{z}$ 和 $y$ 的 grad 值，发现并没有改变，因为他们都不是叶子节点：

```python
>>> z.grad
>>> y.grad
```

## 2. 线性回归

本节我们将实现一个**线性回归**（LR）模型。

### 2.1 理论分析

#### 1）准备数据

```python
import torch
import matplotlib.pyplot as plt

x = torch.Tensor([1.4, 5, 11, 16, 21])
y = torch.Tensor([14.4, 29.6, 62, 85, 113.4])

plt.scatter(x.numpy(), y.numpy())
plt.show()
```

得到图：![image-20220116200141925](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116200141925.png)

#### 2）目标函数

因为我们假设用一条直线去拟合，所以可以假设该函数为：$y=w_1x+w_0$。我们的目标就是找到一组合适的 $(w_1, w_0)$。

我们把上面 y 改写一下得到：$\hat{y}=w_1x+w_0$，这样 $\hat{y}^{(i)}$ 是由样本中的 $x^{(i)}$ 传入线性模型后计算得到的输出，$y^{(i)}$ 是我们真实样本值。因为测量会产生误差，我们用一个函数来衡量 $\hat{y}^{(i)}$ 和 $y^{(i)}$ 之间的误差，这个函数就是**损失函数**。在这里，我们采用的损失函数是均方误差函数（Mean-Square Error，**MSE**）：

$L(w_1, w_0) = \sum^5_{i=1}(\hat{y}^{(i)} - y^{(i)})^2$

因此，我们的目标就是找一组合适的 $(w_1, w_0)$ 使得损失函数的 L 值最小。

#### 3）优化

为了让损失函数值 L 降到最小，我们就要开始调整参数 $(w_1, w_0)$ 的值了！这个过程被称为**优化**。这里我们采用一种**梯度下降**的方法来寻找这个函数的最小值。

L 的梯度是：$\nabla L = (\frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_0})$

这样优化的过程就是做这样的运算：$\vec{w}^{t+1} = \vec{w}^t - \nabla L(\vec{w}^t) \times \delta$，其中 $\delta$ 是学习率。

#### 4）批量输入

上面的表达式是一次一个样本的形式，在实际的优化中，我们是让多个样本同时在一个公式中出现，所有公式中的 $\vec{x}$ 和 $\hat{y}$ 都要增加一个维度，$\vec{x}$ 升级为矩阵 $\boldsymbol{X}$，$\hat{y}$ 升级为 $\vec{\hat{y}}$ ，最终结果为：$\vec{\hat{y}} = \boldsymbol{X} \cdot \vec{w}$

损失函数 L 可以表示为：$L(w_1, w_0) = |\vec{\hat{y}} - \vec{y}|^2$

#### 5）训练

训练就是不断地通过前向传播和反向传播，对参数 $\vec{w}$ 进行调优，最终让损失函数的损失值 L 达到最小的过程：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220116203409197.png" alt="image-20220116203409197" style="zoom:80%;" />

### 2.2 代码实现

#### 1）准备数据

x，y 仍然使用我们之前的数据，我们首先对输入变量和各参数进行初始化：

```python
import torch
import matplotlib.pyplot as plt

# 准备数据

# 生成矩阵X
def Produce_X(x):
	x0 = torch.ones(x.numpy().size) # 用ones产生初始值为1，大小与x相同的向量
	X = torch.stack((x,x0),dim=1)   # stack函数将两个向量拼合
	return X


x = torch.Tensor([1.4,5,11,16,21])
y = torch.Tensor([14.4,29.6,62,85.5,113.4])
X = Produce_X(x)

# 定义权重w的变量
w = torch.rand(2,requires_grad=True)

inputs = X 
target = y
```

+ `Produce_X` 函数将 x 与一个与之相同形状的全 1 向量进行合并得到一个 X，它的实际数据如下：

  ```
  tensor([[ 1.4000,  1.0000],
          [ 5.0000,  1.0000],
          [11.0000,  1.0000],
          [16.0000,  1.0000],
          [21.0000,  1.0000]])
  ```

  这样 X 与 $\vec{w}$ 的乘积便相当于一个 $y = w_1x + w_0$。

+ 用 `rand()` 函数来初始化参数向量 $\vec{w}$，根据 Autograd 中所介绍的，参数 w 属于计算图的叶子节点，需要进行自动微分并利用梯度下降来更新，因此需要专门将 w 的 `requires_grad` 设置为 True。 

#### 2）训练

每一轮的训练分成两部分：前向传播和反向传播

```python
#训练
def train(epochs=1,learning_rate=0.01):
	for epoch in range(epochs):

		#前向传播
		output = inputs.mv(w) #公式：y=Xw
		loss = (output - target).pow(2).sum() # 公式：L = ∑(y-y')^2

		#反向传播
		loss.backward() 
		w.data -= learning_rate * w.grad  # 更新权重w，公式：w_(t+1)= w_(t) - 𝜼*▽J
		
		w.grad.zero_() # 清空grad的值

		if epoch % 80 == 0:
			draw(output,loss)

	#plt.savefig('plot1.png', format='png')

	return w, loss
```

+ 注意，**我们更新完 w 后，必须清空 w 的 grad 的值，否则 grad 的值会持续累加**。所以，这里使用 `zero_()` 函数来清空梯度值。

为了能够观察到训练的变化，我们可以让程序每进行 80 次循环就更新一次图像，于是定义一个 `draw()` 函数：

```python
#绘图
def draw(output,loss):
	plt.cla() # 清空函数图像
	plt.scatter(x.numpy(), y.numpy()) # 绘制散点图
	
	plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5) # 绘制出回归直线
	plt.text(0.5, 0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.text(3, 9,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	#plt.axis([10, 160, 0, 0.03])

	plt.pause(0.005)
```

于是便可以训练了：

```python
w, loss = train(10000,learning_rate = 1e-4)  #学习率设置为1x10^(-4)
```

训练完之后打印最终结果：

```python
print("final loss:", loss.item())
print("weights:", w.data)
```

运行结果：

```
final loss: 8.2430419921875
weights: tensor([5.0840, 5.5849])
```

### 2.3 大规模数据集实例

之前训练时我们将 5 个数据样本同时输入程序，这种方式叫做**批输入**，这种方式是快速而有效的。

人们通过对神经元的研究，对其进行数学抽象得到了**人工神经元模型**：

![image-20220117175443927](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220117175443927.png)

PyTorch 为我们预先编写好了损失函数和优化函数等，我们将代码再重新编写一次：

#### 1）准备数据

```python
import torch
import matplotlib.pyplot as plt
from torch import nn, optim
from time import perf_counter

# 用linspace产生（-3，3）区间内的100000个点，并使用unsqueeze函数增加一个维度
x = torch.unsqueeze(torch.linspace(-3,3,100000),dim=1)

# 假设真实函数是y=x，我们在上面增加一些误差，更加符合实际情况
y = x + 1.2 * torch.rand(x.size())
```

#### 2）定义模型

定义一个线性回归的模型 LR，它继承自 `nn.Module`，并在其中使用 `nn.Linear()` 构造线性模型：

```python
class LR(nn.Module):
    def __init__(self):
        super(LR,self).__init__()
        self.linear = nn.Linear(1,1)

    def forward(self,x):
        out = self.linear(x)
        return out
```

+ `nn.Linear()` 的第一个参数代表输入数据的维度，第二个参数代表输出数据的维度。这里 x 和 y 都是一维的，因此设置为 `nn.Linear(1, 1)`。
+ `forward()` 函数来构造神经网络前向传播的计算步骤。

#### 3）实例化模型

如果平台支持 CUDA，实例化 LR 类后需要调用 `cuda()` 方法：

```python
#如果支持CUDA，则采用CUDA加速
CUDA = torch.cuda.is_available()

if CUDA:
	LR_model = LR().cuda()
	inputs = x.cuda()
	target = y.cuda()
else:
	LR_model = LR()
	inputs = x
	target = y
```

#### 4）损失函数

nn 模块中预设有均方误差函数：

```python
criterion = nn.MSELoss()
```

#### 5）优化器

下面采用“随机梯度下降”的方法来更新权重。**随机梯度下降**实际上就是梯度下降法的改良版，不采用梯度下降法中把全部数据拿来计算梯度的方法，而是<u>每次随机挑选一个数据样本计算梯度值，并进行权值更新</u>。这样做的**好处**是可以避免一次性加载全部数据导致的内存溢出问题，还可以防止优化的时候陷入局部最小值。这里我们使用 PyTorch 预设的随机梯度下降函数 `SGD()` 进行更新：

```python
optimizer = optim.SGD(LR_model.parameters(), lr=1e-4)
```

+ `SGD()` 函数的第一个参数是**需要优化的神经网络模型的参数**，第二个参数是**学习率**。

#### 6）训练

开始编写 `train()` 函数，其参数依次是被训练的神经网络模型、损失函数、优化器和训练轮数：

```python
def draw(output,loss):
    """可视化"""
    if CUDA:
        output = output.cpu()
    plt.cla()
    plt.scatter(x.numpy(), y.numpy())
    plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5)
    plt.text(0.5,0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
    plt.pause(0.005)

def train(model, criterion, optimizer, epochs):
    global loss
    for epoch in range(epochs):
        # forward
        output = model(inputs)
        loss = criterion(output,target)

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


        if epoch % 80 == 0:
            draw(output,loss)

    return model, loss
```

+ 如果采用了 CUDA 加速，draw 函数的 output 需要还原成 CPU 的数据类型才能进行绘图
+ 在**前向传播**阶段，我们将 inputs 输入神经网络模型 model 得到 output，接下来用定义的损失函数 criterion 来计算损失值。
+ 在**反向传播**阶段，先用 optimizer.zero_grad() 清空权值的 grad 值，随后用 backward() 计算梯度，并用优化器 optimizer.stip() 函数进行权值更新。 

接下来我们定义初试时间 start，并传入模型、损失函数、优化器以及训练轮数（10 000 次）：

```python {2}
start = perf_counter()
LR_model,loss = train(LR_model,criterion,optimizer,10000)
finish = perf_counter()
time = finish - start

print("计算时间:%s" % time)
print("final loss:",loss.item())
print("weights:",list(LR_model.parameters()))
```

代码的训练结果如下：

```
计算时间:164.62969759999942
final loss: 0.12093639373779297
weights: [Parameter containing:
tensor([[0.9995]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.5632], device='cuda:0', requires_grad=True)]
```

