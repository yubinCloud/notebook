---
title: 神经网络的复习
date: 2022-03-06 21:07:19
permalink: /pages/dbe9c1/
categories:
  - 深度学习
  - 斋藤康毅-自然语言处理
tags:
  - 
---

## 1. 张量

不再介绍向量和矩阵的基础知识。

在数学和深度学习等领域，向量一般作为列向量处理，不过，考虑到实现层面的一致性，本书将向量作为行向量（每次都会注明是行向量），在代码中往往会用 x 或 W 这样来表示向量和矩阵。行向量也可视为 $1 \times N$ 的矩阵。

## 2. 神经网络的推理

神经网络中进行的处理分为学习和推理两部分，本节先介绍推理部分。

### 2.1 神经网络推理过程的全貌

神经网络将输入变换成输出。我们考虑一个输入二维数据，输出三维数据的模型：

![image-20220308161503018](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220308161503018.png)

+ 往往用圆圈 ○ 表示神经元，用箭头表示它们的连接。此时，**箭头上有权重**，这个权重和对应的**神经元的值**分别相乘，其和（严格地讲，是经过激活函数变换后的值）作为下一个神经元的输入。

上图中网络一共包含三层，但实际有权重的只有两层，所以我们称之为 2 层神经网络，但也有文献称之为 3 层神经网络。

下面用数学式子来表示该网络进行的计算。输入层数据是 $(x_1, x_2)$，用 $w_{11}、w_{12}$ 表示权重，偏置为 $b_1$，这样隐藏层的第 1 个神经元的计算是： $h_1 = x_1w_{11} + x_2w_{21}+b_1$。

这些下标的规则并不重要，实际上，第一层的全连接层的变换通过矩阵乘积表示如下：

$$h = xW + b$$

+ 可以对矩阵的运算进行**形状检查**。

::: tip 形状检查

我们往往会**同时**将多笔样本数据（称为 **mini-batch**，**小批量**）进行推理和学习，因此我们将单独的样本数据保存在矩阵 $x$ 的各行中。假设 batch size 为 N，即 N 个样本作为一个 mini-batch。

![image-20220308170946563](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220308170946563.png)

+ 形状匹配正确

:::

全连接是线性变换，**激活函数**赋予它“非线性”的效果，这里我们使用 sigmoid。

### 2.2 层的类化及正向传播的实现

因为全连接层的变化类似于仿射变换，所以称之为 Affine 层。Sigmoid 的变换称之为 Sigmoid 层。神经网络有各种各样的层，我们将其实现为 Python 的类，通过这种模块化，可以像乐高积木一样搭建网络。在实现它们时，我们制定了如下代码规范：

+ **所有的层都有 `forward` 方法 和 `backward` 方法**，对应正向和反向传播
+ **所有的层都有 `params` 和 `grads` 实例变量**，`params` 使用列表保存权重和偏置等参数，`grads` 以与 `params` 对应的方式保存各参数的梯度

这里先只考虑正向传播，因此先把重点放到 `forward` 和 `params` 上。

#### 2.2.1 sigmoid 层的实现

```python
import numpy as np

class Sigmoid:
    def __init__(self):
        self.params = []

    def forward(self, x):
        return 1 / (1 + np.exp(-x))
```

+ 因为 Sigmoid 没有可以学习的参数，所以使用空列表来初始化 `params`

#### 2.2.2 Affine 层的实现

```python
class Affine:
    def __init__(self, W, b):
        self.params = [W, b]

    def forward(self, x):
        W, b = self.params
        out = np.dot(x, W) + b
        return out
```

+ 在初始化时接收权重和偏置。

#### 2.2.3 搭建一个网络

现在我们使用上面的层来搭建一个如下的网络：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220308164725950.png" alt="image-20220308164725950" style="zoom:80%;" />

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        I, H, O = input_size, hidden_size, output_size

        # 初始化权重和偏置
        W1 = np.random.randn(I, H)
        b1 = np.random.randn(H)
        W2 = np.random.randn(H, O)
        b2 = np.random.randn(O)

        # 生成层
        self.layers = [
            Affine(W1, b1),
            Sigmoid(),
            Affine(W2, b2)
        ]

        # 将所有的权重整理到列表中
        self.params = []
        for layer in self.layers:
            self.params += layer.params

    def predict(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x
```

+ 将推理处理实现为 `predict(x)` 方法。

## 3. 神经网络的学习

### 3.1 损失函数

基于监督数据和神经网络的预测结果，将模型的恶劣程度作为标量计算出来，得到的就是**损失**。

进行多类别分类时，常用**交叉熵误差**（cross entropy error）作为损失函数。这里，我们将 softmax 和 cross entropy error 层加入网络中。 

> **得分**（score）是计算概率之前的值。得分越高，这个神经元对应的类别的概率越高。我们可以把得分输入 softmax 层，得到概率。

我们将 softmax 与交叉熵误差的层实现为 Softmax with Loss 层，这里省略其说明，代码在 `common/layers.py` 中。

### 3.2 导数和梯度

神经网络的学习**目标是找到损失尽可能小的参数**。

求导时，假设有函数 $L = f(x)$，其中 L 是标量，x 是向量，此时，$L$ 关于 $x_i$ 的导数可以写成：$\frac{\partial{L}}{\partial{x_i}}$。即：

$$\frac{\partial{L}}{\partial{x}} = (\frac{\partial{L}}{\partial{x_1}}, \frac{\partial{L}}{\partial{x_2}}, ..., \frac{\partial{L}}{\partial{x_n}})$$

矩阵也是类似。这里的重点是，矩阵 $W$ 和 $\frac{\partial{L}}{\partial{W}}$ 具有相同的形状。利用“**矩阵和其梯度具有相同的形状**”这一性质，可以轻松地进行参数的更新和链式法则的实现。

### 3.3 链式法则

两个函数：$y=f(x),z=g(y)$，那么有 $\frac{\partial{z}}{\partial{x}} = \frac{\partial{z}}{\partial{y}} \frac{\partial{y}}{\partial{x}}$。

也就是说，**只要能够计算各个函数的局部的导数，就能基于他们的积计算最终的整体的导数**。

### 3.4 计算图