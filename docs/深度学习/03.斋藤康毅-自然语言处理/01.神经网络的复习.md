---
title: 神经网络的复习
date: 2022-03-06 21:07:19
permalink: /pages/dbe9c1/
categories:
  - 深度学习
  - 斋藤康毅-自然语言处理
tags:
  - 
---

## 1. 张量

不再介绍向量和矩阵的基础知识。

在数学和深度学习等领域，向量一般作为列向量处理，不过，考虑到实现层面的一致性，本书将向量作为行向量（每次都会注明是行向量），在代码中往往会用 x 或 W 这样来表示向量和矩阵。行向量也可视为 $1 \times N$ 的矩阵。

## 2. 神经网络的推理

神经网络中进行的处理分为学习和推理两部分，本节先介绍推理部分。

### 2.1 神经网络推理过程的全貌

神经网络将输入变换成输出。我们考虑一个输入二维数据，输出三维数据的模型：

![image-20220308161503018](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220308161503018.png)

+ 往往用圆圈 ○ 表示神经元，用箭头表示它们的连接。此时，**箭头上有权重**，这个权重和对应的**神经元的值**分别相乘，其和（严格地讲，是经过激活函数变换后的值）作为下一个神经元的输入。

上图中网络一共包含三层，但实际有权重的只有两层，所以我们称之为 2 层神经网络，但也有文献称之为 3 层神经网络。

下面用数学式子来表示该网络进行的计算。输入层数据是 $(x_1, x_2)$，用 $w_{11}、w_{12}$ 表示权重，偏置为 $b_1$，这样隐藏层的第 1 个神经元的计算是： $h_1 = x_1w_{11} + x_2w_{21}+b_1$。

这些下标的规则并不重要，实际上，第一层的全连接层的变换通过矩阵乘积表示如下：

$$h = xW + b$$

+ 可以对矩阵的运算进行**形状检查**。

::: note 形状检查

我们往往会**同时**将多笔样本数据（称为 **mini-batch**，**小批量**）进行推理和学习，因此我们将单独的样本数据保存在矩阵 $x$ 的各行中。假设 batch size 为 N，即 N 个样本作为一个 **mini-batch**。

![image-20220315221352971](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315221352971.png)

+ 在矩阵乘积等计算中，注意矩阵的形状并观察其变化的形状检查非常重要。据此，神经网络的实现可以更顺利地进行。

:::

全连接是线性变换，**激活函数**赋予它“非线性”的效果，这里我们使用 sigmoid。

### 2.2 层的类化及正向传播的实现

因为全连接层的变化类似于仿射变换，所以称之为 Affine 层。Sigmoid 的变换称之为 Sigmoid 层。神经网络有各种各样的层，我们将其实现为 Python 的类，通过这种模块化，可以像乐高积木一样搭建网络。在实现它们时，我们制定了如下代码规范：

+ **所有的层都有 `forward` 方法 和 `backward` 方法**，对应正向和反向传播
+ **所有的层都有 `params` 和 `grads` 实例变量**，`params` 使用列表保存权重和偏置等参数，`grads` 以与 `params` 对应的方式保存各参数的梯度

这里先只考虑正向传播，因此先把重点放到 `forward` 和 `params` 上。

#### 2.2.1 sigmoid 层的实现

```python
import numpy as np

class Sigmoid:
    def __init__(self):
        self.params = []

    def forward(self, x):
        return 1 / (1 + np.exp(-x))
```

+ 因为 Sigmoid 没有可以学习的参数，所以使用空列表来初始化 `params`

#### 2.2.2 Affine 层的实现

```python
class Affine:
    def __init__(self, W, b):
        self.params = [W, b]

    def forward(self, x):
        W, b = self.params
        out = np.dot(x, W) + b
        return out
```

+ 在初始化时接收权重和偏置。

#### 2.2.3 搭建一个网络

现在我们使用上面的层来搭建一个如下的网络：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220308164725950.png" alt="image-20220308164725950" style="zoom:80%;" />

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        I, H, O = input_size, hidden_size, output_size

        # 初始化权重和偏置
        W1 = np.random.randn(I, H)
        b1 = np.random.randn(H)
        W2 = np.random.randn(H, O)
        b2 = np.random.randn(O)

        # 生成层
        self.layers = [
            Affine(W1, b1),
            Sigmoid(),
            Affine(W2, b2)
        ]

        # 将所有的权重整理到列表中
        self.params = []
        for layer in self.layers:
            self.params += layer.params

    def predict(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x
```

+ 将推理处理实现为 `predict(x)` 方法。

## 3. 神经网络的学习

### 3.1 损失函数

基于监督数据和神经网络的预测结果，将模型的恶劣程度作为标量计算出来，得到的就是**损失**。损失指示学习阶段中某个时间点的神经网络的性能。

进行多类别分类时，常用**交叉熵误差**（cross entropy error）作为损失函数。这里，我们将 softmax 和 cross entropy error 层加入网络中。 

我们将 softmax 与交叉熵误差的层实现为 Softmax with Loss 层，这里省略其说明，代码在 `common/layers.py` 中。

#### 3.1.1 softmax

softmax 函数：$y_k = \frac{exp(s_k)}{\sum^n_{i=1}exp(s_i)}$

+ 该式是当输出总共有 n 个时，计算第 k 个输出 $y_k$ 时的算式，这个 $y_k$ 是对应于第 k 个类别的 softmax 函数的输出。

softmax 函数输出的各个元素是 0.0 ～ 1.0 的实数。另外，如果将这些元素的和为 1。因此，**softmax 的输出可以解释为概率**。之后， 这个概率往往会被输入交叉熵误差。

> **得分**（score）是计算概率之前的值。得分越高，这个神经元对应的类别的概率越高。我们可以把得分输入 softmax 层，得到概率。

#### 3.1.2 交叉熵误差

交叉熵误差：$L = - \sum_k{t_k \log y_k}$

+ $t_k$ 是对应于第 k 个类别的监督标签，它以 one-hot 向量的形式表示，比如 t = (0, 0, 1)。

在考虑了 mini-batch 处理的情况下，交叉熵误差可以表示为：

$$L = - \frac{1}{N}\sum_n\sum_k{t_{nk} \log y_{nk}}$$

+ 这里假设数据有 N 笔，下标 $nk$ 表示第 n 笔数据的第 k 维元素的值，$y_{nk}$ 表示神经网络的输出，$t_{nk}$ 表示监督标签。
+ 该式看上去有些复杂，其实只是将表示单笔数据的损失函数的式扩展到了 N 笔数据的情况。除以 N 可以求单笔数据的平均损失。**通过这样的平均化，无论 mini-batch 的大小如何，都始终可以获得一致的指标**。

引入了 Softmax with Loss 层后的一个神经网络示例：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315223329994.png" alt="image-20220315223329994" style="zoom:80%;" />

### 3.2 导数和梯度

神经网络的学习**目标是找到损失尽可能小的参数**。

求导时，假设有函数 $L = f(x)$，其中 L 是标量，x 是向量，此时，$L$ 关于 $x_i$ 的导数可以写成：$\frac{\partial{L}}{\partial{x_i}}$。即：

$$\frac{\partial{L}}{\partial{x}} = (\frac{\partial{L}}{\partial{x_1}}, \frac{\partial{L}}{\partial{x_2}}, ..., \frac{\partial{L}}{\partial{x_n}})$$

矩阵也是类似。这里的重点是，矩阵 $W$ 和 $\frac{\partial{L}}{\partial{W}}$ 具有相同的形状。利用“**矩阵和其梯度具有相同的形状**”这一性质，可以轻松地进行参数的更新和链式法则的实现。

### 3.3 链式法则

两个函数：$y=f(x),z=g(y)$，那么有 $\frac{\partial{z}}{\partial{x}} = \frac{\partial{z}}{\partial{y}} \frac{\partial{y}}{\partial{x}}$。

链式法则的重要之处在于，无论我们要处理的函数有多复杂（无论复合了多少个函数），都可以根据它们各自的导数来求复合函数的导数。也就是说，**只要能够计算各个函数的局部的导数，就能基于它们的积计算最终的整体的导数**。

### 3.4 计算图

**计算图**是计算过程的图形表示。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315223645029.png" alt="image-20220315223645029" style="zoom:67%;" />

+ 此图便是 $z = x + y$ 的计算图。
+ 计算图通过节点和箭头来表示。**变量写在箭头上，用节点表示计算**，处理结果有序流动。

这里重要的是，梯度沿与正向传播相反的方向传播，这个反方向的传播称为反向传播。

我们看一下反向传播的全貌。虽然我们处理的是 z = x + y 这 一计算，但是在该计算的前后，还存在其他的“某种计算”。另外， 假设最终输出的是标量 L（在神经网络的学习阶段，**计算图的最终输出是损失，它是一个标量**）：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315224012379.png" alt="image-20220315224012379" style="zoom:80%;" />

我们的目标是**求 L 关于各个变量的导数**（梯度）。这样一来，计算图的反向传播就可以绘制成下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315224048275.png" alt="image-20220315224048275" style="zoom: 80%;" />

+ 反向传播用蓝色的粗箭头表示，在箭头的下方标注传播的值。此时，**传播的值是指最终的输出 L 关于各个变量的导数**。

这里我们处理一下 z = x + y 这个节点的运算，$\frac{\partial z}{\partial x}=1$，所以加法节点将上游传来 的值乘以 1，再将该梯度向下游传播：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315224406786.png" alt="image-20220315224406786" style="zoom:80%;" />

下面，我们将介绍几个典型的运算节点。

#### 3.4.1 乘法节点

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315224745808.png" alt="image-20220315224745808" style="zoom:80%;" />

乘法节点的计算是 $z = x \times y$，导数可以分别求出 $\frac{\partial z}{\partial x}=y$。

> 可以这样理解：最终的输出是 L，所要求的是 L 关于各个变量的导数，那么 L 关于 x 的导数就是：
>
> $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial x} = \frac{\partial L}{\partial z} y$$

在目前为止的加法节点和乘法节点的介绍中，流过节点的数据都是“单变量”。但是，这不仅限于单变量，也可以是多变量。当张量流过加法节点（或者乘法节点）时，只需独立计算张量中的各 个元素。

#### 3.4.2 分支节点

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315225423954.png" alt="image-20220315225423954" style="zoom: 80%;" />

相同的值被复制并分叉。因此，分支节点也称为复制节点。它的反向传播是上游传来的梯度之和。

#### 3.4.3 Repeat 节点

分支节点有两个分支，但也可以扩展为 N 个分支（副本），这里称为 Repeat 节点。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315225607849.png" alt="image-20220315225607849" style="zoom:80%;" />

这个例子中将长度为 D 的数组复制了 N 份。因为这个 Repeat 节点可以视为 N 个分支节点，所以它的反向传播可以通过 N 个梯度的总和求出。

示例：

```python
>>> import numpy as np
>>> D, N = 8, 7
>>> x = np.random.randn(1, D) # 输入
>>> y = np.repeat(x, N, axis=0) # 正向传播

>>> dy = np.random.randn(N, D) # 假设的梯度
>>> dx = np.sum(dy, axis=0, keepdims=True) # 反向传播
```

+ 通过指定 `keepdims=True`，可以维持二维数组的维数。在上面的例子中，当 keepdims=True 时，np.sum() 的结果的形状是 (1, D)；当 keepdims=False 时，形状是 (D, )。

#### 3.4.4 Sum 节点

Sum 节点是通用的加法节点。这里考虑对一个 N × D 的数组沿第 0 个轴求和。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220315225907433.png" alt="image-20220315225907433" style="zoom:80%;" />

+ shape 为 (N, D) 的矩阵 X，在经过沿第 0 个轴求和后，即 `np.sum(X, aixs=0)` 后，会使原来 X 的第 0 个轴消失，结果的 shape 变成 (D, )，当设置 keepdims 时则会变成 (1, D)。

Sum 节点的反向传播将上游传来的梯度分配到所有箭头上。这是加法节点的反向传播的自然扩展。示例：

```python
>>> import numpy as np
>>> D, N = 8, 7
>>> x = np.random.randn(N, D) # 输入
>>> y = np.sum(x, axis=0, keepdims=True) # 正向传播

>>> dy = np.random.randn(1, D) # 假设的梯度
>>> dx = np.repeat(dy, N, axis=0) # 反向传播
```

#### 3.4.5 MatMul 节点

// TODO