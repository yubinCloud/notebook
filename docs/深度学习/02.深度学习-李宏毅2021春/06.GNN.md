---
title: GNN
date: 2022-04-10 09:16:06
permalink: /pages/617f77/
categories:
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

> 一篇发表在 distill 上的介绍 GNN 的博客也值得学习：[A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)

## 1. Introduction

### 1.1 Graphs and where to find them

#### 1）Images as graphs

可以将 image 看成一个 graph，每一个 pixel 是一个 node，pixel 之间通过 edge 连接。这样每一个节点用 RGB 表示方式可以表示为一个 3 维 vector。

如果用一个 adjacency matrix 来表示 image 的话，一个 image 可以表示成一个 $n_{nodes} \times n_{nodes}$ 的 matrix。

 <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410092941805.png" alt="image-20220410092941805" style="zoom: 67%;" />

#### 2）Text as graphs

可以将一段 text 视为一个 directed graph：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410093055576.png" alt="image-20220410093055576" style="zoom:80%;" />

> 当然 image 和 text 通常不会这样被编码，因为他们本身就有非常规整的结构，将其视为 graph 的话会产生冗余。

#### 3）Graph-valued data in the wild

分子可以视为一个 graph，社交网络可以视为一个 graph，文章的引用也可以视为一个 graph …

### 1.2 What types of problems have graph structured data?

What tasks do we want to perform on graph data? 

| Level            | Goal                                                         | Example                                                      |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| graph-level task | predict the property of an entire graph                      | 将分子视为一个 graph，预测其味道或者能否治疗某种疾病。也就是给整个 image 加一个 label。 |
| node-level       | predicting the identity or role of each node within a graph  | 将一个社交网络中的人分成两个团体的人。比如一个武道馆，根据人与人之间的联系，将学生划分忠诚于两个老师的团体。 |
| edge-level       | given nodes that represent the objects in the image, we wish to predict which of these nodes share an edge or what the value of that edge is. | <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410094602320.png" alt="image-20220410094602320" style="zoom:80%;" /> |

### 1.3 GNN: How?

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410094749334.png" alt="image-20220410094749334" style="zoom:67%;" /></center>

How to embed node into a feature space using convolution?

+ Solution 1: Generalize the concept of convolution (corelation) to graph >> **Spatial-based convolution**
+ Solution 2: Back to the definition of convolution in signal processing >> **Spectral-based convolution**

### 1.4 Roadmap

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410094944372.png" alt="image-20220410094944372" style="zoom:80%;" />

+ 常用的还是 GAT 和 GCN 的 model
+ GNN 或许和 NLP 听上去像八竿子打不着的东西，但就是有办法把他们扯在一起

## 2. Spatial-based GNN

Terminology：

+ <mark>Aggregate</mark>：用 neighbor feature 来 update 下一个 hidden state
+ <mark>Readout</mark>：把所有 nodes 的 feature 集合起来代表整个 graph

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410100744568.png" alt="image-20220410100744568" style="zoom:80%;" /></center>

+ 比如用 $h^0_0$、$h^0_1$、$h^0_2$ 和 $h^0_4$ 做一个 aggregate 便得到 $h^1_3$。
+ 得到 $h_G$ 之后就可以用来做整个 graph label 的 classification 或 prediction。

### 2.1 NN4G

NN4G（Neural Networks for Graph）

假设我们将一个分子视为一个图，原子是它的节点，将 graph 作为输入：

input layer：<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410101652634.png" alt="image-20220410101652634" style="zoom:67%;" />

此处每个 node 有一个 node feature，首先要经过一个 Embedding Layer 做一个 embedding，这里直接用一个 Embedding Matrix 来得到它的 feature：$h^0_3 = \bar{W_0} \cdot x_3$

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410102221519.png" alt="image-20220410102221519" style="zoom:67%;" />

重点是接下来如何做 aggregation：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410102740576.png" alt="image-20220410102740576" style="zoom:50%;" /></center>

+ 要计算 $h^1_3$，需要将上一层的 $h^0_3$ 的邻居节点找出来做一个 sum，之后再加上原本的 input feature。具体计算过程可见上图。

将上述过程重复叠了多层之后（假如叠了 3 层），就要最后做一个 Readout：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410103027587.png" alt="image-20220410103027587" style="zoom: 80%;" /></center>

+ 我们将每一个层的 node feature 各自全部加起来，然后经过一个 transform，再加起来变成一个 feature $y$，代表整个 graph 的 feature。

> 为什么用相加？其实也可以用其他方式，但多数用相加，可能是因为如果不相加的话会很难处理每个节点的邻居数量不同的这件事情。

### 2.2 DCNN

DCNN（Diffusion-Convolution Neural Network）

有了 input 之后，它做 update 的方式是这样子的：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410103802957.png" alt="image-20220410103802957" style="zoom:50%;" /></center>

+ $MEAN(d(3,\cdot)=1)$ 表示将与 3 这个节点距离为 1 的所有节点加起来再取个平均。
+ 取了平均之后再做一个 weight transform。其余的节点都做一样的事情。

再到第二层时与之前也有点区别：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410104356790.png" alt="image-20220410104356790" style="zoom:50%;" /></center>

+ 计算第 2 层的 hidden state $h^1_3$ 时，将与其距离为 2 的全部节点的 feature 都加起来再取平均，这里加的 feature 是 input layer 中的 input feature。
+ 这种情况下，假设我叠 k 层，那我就可以看到一个节点的 k neighborhood 里面得东西

最后计算一个节点的 feature 的方法是：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410104747709.png" alt="image-20220410104747709" style="zoom:80%;" /></center>

+ 这样就可以得到 1 号这个节点的 output feature $y_1$，其余节点也类似。

### 2.3 DGC

DGC（Diffusion Graph Convolution），[原论文](https://arxiv.org/pdf/1707.01926.pdf)。它与上面的 DCNN 很像，就是在最后把所有 $H^i$ 加在了一起：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410105429461.png" alt="image-20220410105429461" style="zoom:67%;" />

### 2.4 MoNET

MoNET（Mixture Model Networks），[原文](https://arxiv.org/pdf/1611.08402.pdf)。

之前做 aggregate 是把 neighbor feature 直接相加，但可能每个邻居的重要性不一样，所以这个网络就是用 weighted sum 替代了 simply summing up：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410110043556.png" alt="image-20220410110043556" style="zoom:67%;" />

+ 这里的 $u(x,y)$ 就代表了“距离”这一个概念，这里的距离是用 degree 定义的，但你也可以用其他方式来定义。

> 这里距离的定义是一开始就给出的，之后有的模型会让 model 自己学习这个距离的定义。

### 2.5 GraphSAGE

GraphSAGE， **SA**mple and aggre**G**at**E**。

它的 aggregation 方式：mean, max-pooling, or LSTM

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410110622938.png" alt="image-20220410110622938" style="zoom:70%;" /></center>

> 有一种 aggregation 方式是 LSTM，它是把邻居的 feature 喂到 LSTM 中，然后把最后的 hidden state 当做他最后的一个 output，然后拿这个东西来做 update。但 LSTM 是 sequential 的，而邻居不应该有顺序啊，所以这里的做法是每次输入时都乱 sample 出一个顺序，每次 update 的时候都是 sample 出不同的顺序。所以最后可以学到说去忽略这个顺序的影响。

一个实验结果的对比：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410111026491.png" alt="image-20220410111026491" style="zoom:67%;" />

### 2.6 GAT

GAT，Graph Attention Networks，[原论文](https://arxiv.org/pdf/1710.10903.pdf)。

GAT 的重点是我不只做 weighted sum，这里的 weight 要让它自己去学，因此这样它做的方法就是我对邻居做 attention，方法是这样：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410111430197.png" alt="image-20220410111430197" style="zoom:80%;" />

+ 这里 $f(h^0_3,h^0_0) = e_{3,0}$ 得到的这个 energy $e$ 表示了对于 $h^0_3$ 而言，$h^0_0$ 有多重要。其余的 energy $e$ 也是如此。

> 大家最喜欢用的 GNN 中，其中一个就是它。

### 2.7 GIN

GIN，Graph Isomorphism Network，[原论文](https://openreview.net/forum%3Fid=ryGs6iA5Km)。之前的 Network 我们是直接用，并没有关心它为什么会 work，但这个 GIN 直接告诉了我们有些方法会 work，但有些方法是不会 work 的，因此它还提供了一些理论证明，这里我们直接说结论。

结论告诉我们，在 update 的时候，我们最好使用下面这种方式来 update：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410122439413.png" alt="image-20220410122439413" style="zoom:80%;" /></center>

+ $h_v^{(k)}$ 表示节点 v 在第 k 层的 hidden representation
+ 先把全部邻居的上一层的 $h_u^{(k-1)}$ 先全部加起来，再加上 $(1+\epsilon^k)$乘以自身上一层的 hidden representation。这里 $\epsilon$ 可以自己学，但取 0 也是没问题的
+ MLP 是多层感知机，**用 MLP 而不是 1 layer**
+ 重点是用 $\sum$ 部分**应该用 sum 而不是 mean 或 max pooling**

我们看一下 Sum instead of mean or max 的原因，先看一下的图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220410123150332.png" alt="image-20220410123150332" style="zoom:80%;" />

+ 在 (a) 图中，如果采用 mean pooling，那么左右两个图得到的结果是一样的，此时无法区分这两个图是不同的图。另外的两个图类似。