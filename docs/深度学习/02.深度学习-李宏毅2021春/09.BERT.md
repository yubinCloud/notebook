---
title: BERT
date: 2022-04-23 11:12:46
permalink: /pages/b6b6a5/
categories:
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

## 1. BERT 简介

### 1.1 芝麻街与进击的巨人

### 1.2 Self-supervised Learning

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423140311925.png" alt="image-20220423140311925" style="zoom:67%;" /></center>

+ 监督学习中我们有一个模型，输入 x 得到输出 y，此外还有一个 label $\hat y$，用他们训练 model。
+ **Self-supervised（自监督学习）用另一种方式来监督，没有标签**。假设我们只有一堆没有 label 的文章，但我们试图找到一种方法把它**分成两部分**，让其中一部分 $x’$ 作为模型的输入数据，另一部分 $x''$作为标签。Self-supervised 学习也**是一种无监督的学习方法**。

我们以 BERT 为例来说明 Self-supervised Learning 是什么意思。

#### 1）Masking Input

首先，**BERT 是一个 Transformer 的 Encoder**，它和 Transformer 的 Encoder 一样，里面有很多 Self-Attention、Residual connection 和 Normalization 等等，这就是 BERT。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423144250257.png" alt="image-20220423144250257" style="zoom:50%;" />

关键点是：**BERT 可以输入一行向量，然后输出另一行向量，输出的长度与输入的长度相同**。BERT 一般用于自然语言处理，用于文本场景，所以一般来说，它的输入是一串文本，也是一串数据。

接下来，我们随机**盖住**一些输入的文字，**被 mask 的部分是随机决定的**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423153522106.png" alt="image-20220423153522106" style="zoom:50%;" />

mask 的具体实现有两种方法，使用哪种方法都可以：

1. **用一个特殊的符号替换句子中的一个词**。我们用 `MASK` 来作为特殊符号，这个字完全是一个新词，意味着 mask 了原文。
2. **随机把某一个字换成另一个字**。比如“湾”字可以随机换成“大”、“一”等字。

mask 后把序列输入 BERT，把输出看作另一个序列，从中寻找 mask 部分的相应输出，然后让这个向量通过一个 Linear transform，即矩阵相乘，再做 softmax，输出一个分布：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423154635696.png" alt="image-20220423154635696" style="zoom:67%;" />

在训练过程中。我们知道被 mask 的字符是什么，而 BERT 不知道，我们可以用一个 one-hot vector 来表示这个字符，并使输出和 one-hot vector 之间的交叉熵损失最小。这其实就是在做一个分类问题，**BERT 要做的就是预测什么被盖住**，具体来说就是被掩盖的字符属于"湾"类。

在训练中，我们**在BERT之后添加一个线性模型**，并将它们**一起训练**。所以，BERT 里面是一个 transformer 的 Encoder，它有一堆参数。这两个需要共同训练，并试图预测被覆盖的字符是什么，这叫做 <mark>mask</mark>。

#### 2）Next Sentence Prediction

当我们训练 BERT 时，除了 mask 之外，我们还会使用另一种方法：<mark>Next Sentence Prediction</mark>。它的意思是我们拿两个句子，句子之间加一个特殊标记 `SEQ` 作为分隔符，再在开头加一个 `CLS` 的特殊标记。这样我们就有了一个 `<CLS> <seq1> <SEQ> <seq2>` 拼成的 seq，将它输入 BERT 得到一个输出 seq，然后我们**只看 output seq 中 CLS 对应的输出**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423155821427.png" alt="image-20220423155821427" style="zoom:67%;" />

将 CLS 对应的输出乘一个 Linear transform，输出 yes or no，即**预测第二句是否是第一句的后续句**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423160315515.png" alt="image-20220423160315515" style="zoom: 80%;" />

然后而来的研究发现，对于 BERT 要做的任务来说，Next Sentence Prediction 并没有真正的帮助。可能的原因是 Next Sentence Prediction 任务太简单，导致 BERT 没有学到太有用的东西。还有一种类似的方法叫作 <mark>Sentence order prediction</mark>（**SOP**），这个方法的思想是，选两个连续的句子，要么句子1在句子2后面相连，要么句子2在句子1后面相连，有两种可能性，我们问 BERT 是哪一种。这种方法被用在 ALBERT 模型中。

所以当我们训练时，我们要求BERT学习两个任务：

+ 一个是掩盖一些字符，具体来说是汉字，然后要求它填补缺失的字符。
+ 另一个任务表明它能够预测两个句子是否有顺序关系。

总的来说，BERT 学会了如何填空，但 BERT 的神奇之处在于，在你训练了一个填空的模型之后，它还可以**用于其他任务**，而这些任务不一定与填空有关。BERT 实际使用的任务称为 <mark>Downstream Tasks</mark>（**下游任务**），它是我们真正关心的任务，但**当我们想让 BERT 学习做这些 Downstream Tasks 时，我们仍然需要一些标记的信息**。

BERT 就像胚胎干细胞，具有各种无限潜力，虽然它还没有使用它的力量，但以后它有能力解决各种任务。我们只需要给它一点数据来激发它，它就能做到。BERT 分化成各种任务的功能细胞，被称为 <mark>Fine-tune</mark>。我们对 BERT 进行微调可以使他能够完成某种任务，在微调之前产生这个 BERT 的过程称为<mark>预训练</mark>。所以，**生成 BERT 的过程就是 Self-supervised 学习**。

在我们谈论如何微调 BERT 之前，我们应该先看看它的能力。为了测试 Self-supervised 学习的能力，通常，你会在多个任务上测试它，让 BERT 分化成各种任务的功能细胞，看看它在每个任务上的准确性，然后我们取其平均值，得到一个总分。这个不同任务的集合，称之为**任务集**。任务集中最著名的基准是 **GLUE**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423172922089.png" alt="image-20220423172922089" style="zoom:80%;" />

+ 黑线表示人类的工作
+ 蓝色曲线表示机器 GLUE 得分的平均值

### 1.3 How to use BERT

#### Case 1：Sentiment analysis

第一个案例是我们的 Downstream Tasks 要输入一个序列，然后输出一个 class。比如说 *Sentiment analysis 就是给机器一个句子，让它判断这个句子是正面的还是负面的*：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423174337526.png" alt="image-20220423174337526" style="zoom:80%;" /></center>

在实践中，你必须为你的 Downstream Tasks 提供标记数据，**因为 BERT 没有办法从头开始解决情感分析问题，你仍然需要向 BERT 提供大量的句子，以及它们的正负标签，来训练这个 BERT 模型**。

在训练的时候，Linear transform 和 BERT 模型都是利用 gradient descent 来更新参数的：

+ <u>Linear transform的参数是随机初始化的</u>
+ <u>而 BERT 的参数是由学会填空的BERT初始化的</u>

我们为什么要这样做呢？最直观和最简单的原因是，当你把学会填空的BERT放在这里时，它将获得比随机初始化BERT更好的性能。

+ <u>当你进行 Self-supervised 学习时，你使用了大量的无标记数据</u>
+ <u>另外在 Downstream Tasks 需要少量的标记数据</u>

所谓的<mark>半监督</mark>就是指用大量的无标签数据和少量的有标签数据。使用 BERT 的整个过程是连续应用 Pre-Train 和 Fine-Tune，它可以被视为一种半监督方法。

#### Case 2：POS tagging

第二个案例是输入输入一个序列，然后输出另一个序列，而输入和输出的长度是一样的。*POS tagging（词性标记）是指你给机器一个句子，它必须告诉你这个句子中每个词的词性*。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423175633606.png" alt="image-20220423175633606" style="zoom: 80%;" /></center>

你只需向 BERT 输入一个句子。之后，对于这个句子中的每一个 token 是一个中文单词，有一个代表这个单词的相应向量。然后，这些向量会依次通过 Linear transform 和 softmax 层。最后，网络会预测给定单词所属的类别，例如，它的词性。

这是一个典型的分类问题，唯一不同的是 BERT 的参数不是随机初始化的，在预训练过程中，它已经找到了不错的参数。

#### Case 3：Natural Language Inference

第三个案例以两个句子为输入，输出一个类别。在 Natural Language Inference（**NLI**）中，机器要做的是*判断是否有可能从前提中推断出假设*，即这个前提与这个假设是否相矛盾：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423180209764.png" alt="image-20220423180209764" style="zoom:80%;" /></center>

+ 在上面这个例子中，我们的前提是“一个人骑着马，然后他跳过一架破飞机”，假设是“这个人在一个餐馆”，所以这个推论说这是一个矛盾。

机器要做的就是把两个句子作为输入，并输出这两个句子之间的关系。比如舆情分析中给定一篇文章和一个评论，判断这个消息是同意还是反对这篇文章。

BERT 的做法就是在这两个句子之间放一个特殊的标记，并在最开始放 CLS 标记，把这个序列作为 BERT 的输入，并只把 CLS 标记对应的 output 作为 Linear transform 的 input，从而决定这两个输入句子的类别：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423180959729.png" alt="image-20220423180959729" style="zoom:67%;" /></center>

#### Case 4：Extraction-based Question Answering (QA)

Extraction-based 的 AQ 是在机器读完一篇文章后，你问它一个问题，它将给你一个答案，并假设答案必须出现在文章。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423181553545.png" alt="image-20220423181553545" style="zoom:67%;" />

在这个任务中，一个输入序列包含一篇文章和一个问题，文章和问题都是一个序列。对于中文来说，每个d代表一个汉字，每个 q 代表一个汉字。你把 d 和 q 放入 QA 模型中，我们希望它输出两个正整数 s 和 e。根据这两个正整数，我们可以直接从文章中截取一段，它就是答案。这听起来很疯狂，但是无论如何，这是今天一个非常普遍的方法。

举一个例子，这里有一个问题和一篇文章，正确答案是 "gravity"。机器如何输出正确答案：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423181837446.png" alt="image-20220423181837446" style="zoom:67%;" />

为了训练这个QA模型，我们使用BERT预训练的模型：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423182005963.png" alt="image-20220423182005963"  />

在这个任务中，你唯一需要**从头训练**的只有**两个向量**。"从头训练 "是指**随机初始化**。这里我们用橙色向量和蓝色向量来表示，这两个向量的长度与BERT的输出相同。首先,计算这个橙色向量和那些与文件相对应的输出向量的内积，然后将它们通过 softmax 函数，找到分数最大的位置，得到 s，代表输出的起始位置。

蓝色部分也做一样的事情，计算结果 e 代表答案的终点：

![image-20220423182334764](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423182334764.png)

如果答案不在文章中，你就不能使用这个技巧。

### 1.4 Training BERT is challenging!

虽然 BERT 在预训练中只是做填空题，但你自己真的不能把它训练起来。谷歌最早的 BERT 中使用的数据规模已经很大了，它包括了 30 亿个词汇。更痛苦的是训练过程，这将需要大量的时间。

谷歌已经训练了 BERT，这些 Pre-Train 模型是公开的，那我们再去训练有什么意义呢？也许可能是想建立一个 **BERT 胚胎学**，这样可以观察到 BERT 什么时候学会填什么词汇，它是如何提高填空能力的。[论文链接](https://arxiv.org/abs/2010.02480)供大家参考，不过可以提前爆冷一下就是：事实和你直观想象的不一样。

### 1.5 Pre-training a seq2seq model

BERT 只是一个预训练的 Encoder，有没有办法预训练 Seq2Seq 模型的 Decoder？有！现在我有一个 seq2seq 模型，带着一个 Encoder 和 Decoder，input 一个 seq，output 一个 seq，中间用 Cross Attention 连接。然后你故意 Encoder 的输入上做一些干扰来破坏它。这样，**Encoder 看到的是被破坏的结果，那么 Decoder 应该输出句子被破坏前的结果，训练这个模型实际上是预训练一个 Seq2Seq 模型**。如下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220423200036067.png" alt="image-20220423200036067" style="zoom:67%;" />

有各种破坏 Encoder 输入的方法：删除一些词，打乱词的顺序，旋转词的顺序，或者插入一个MASK，再去掉一些词。在破坏了输入的句子之后，它可以通过 Seq2Seq 模型来恢复它。

那么多 mask 方法，哪种最好呢？谷歌的一篇论文 T5（Transfer Text-To-Text Transformer）做了各种尝试，可以读一下看看结论。T5 是在一个叫做 C4 的数据集上训练的，大小有 7TB。可以看到，在 deep learning 中，数据量和模型都很惊人。