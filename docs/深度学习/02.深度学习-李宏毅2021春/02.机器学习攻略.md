---
title: 机器学习攻略
date: 2022-04-03 19:39:56
permalink: /pages/22f046/
categories:
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

## 1. General Guidance

训练模型的过程中，以下就是如何让你做得更好的攻略：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403195635625.png" alt="image-20220403195635625" style="zoom:67%;" />

当对训练结果不满意时（testing data 的 loss 太大），首先应检查你的 training data，看看你的 model 有没有在 training data 上学起来，再去看 testing 的结果。**如果你发现你的 training data 的 loss 很大，显然它在训练集上面也没有训练好**，接下来你要分析一下在训练集上面没有学好是什么原因。一种原因是 model bias，一种是 optimization 的问题。

### 1.1 model bias

所谓 **model bias 的意思是说，你的 model 太过简单**。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210311205956634.png" alt="img" style="zoom:80%;" />

举例来说，我们现在写了一个有未知 parameter 的 function，这个未知的 parameter，我们可以代各种不同的数字，你代 $\theta ^1$ 就可以得到一个 function $f_{\theta ^1}(x)$，代 $\theta ^2$ 就可以得到一个 function $f_{\theta ^2}(x)$，把所有的 function 集合起来，可以得到一个 function set。

**如果 model 太简单，那么这个 function set 太小了，使得它没有包含任何一个 function 可以让我们的 loss 变得够低**。这时即便找到这里面最好的那个 function $f_{\theta ^*}(x)$，依然无济于事，这个 loss 还是不够低。

这个状况就像你想要在大海里面捞针，这个针指的是一个 loss 低的 function，结果针根本就不在海里面。

**Solution：重新设计一个 model，给你的 model 更大的弹性**。比如增加输入的 features、设计一个更大的 model …

### 1.2 optimization issue

但是并不是 training 的时候，loss 大就代表一定是 model bias，你可能会遇到另外一个问题：**optimization 做得不好**。

我们可能卡在一个 <mark>local minima</mark> 的地方，这时你没有办法找到一个真的可以让 loss 很低的参数，如图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210311213108040.png" alt="img" style="zoom: 67%;" />

这就好像是说我们想大海捞针，针确实在海里，但是我们却没有办法把针捞起来。

这就产生了一个问题：**training data 的 loss 不够低的时候，到底是 model bias，还是 optimization 的问题呢**？一个建议判断的方法，就是你可以透过比较不同的模型，来得知你的 model 现在到底够不够大。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210311214054168.png" alt="image-20210311214054168" style="zoom:67%;" />

举一个例子，如上图，有两个 network，一个有 20 层，一个有 56 层，现在我们把它们测试在测试集上，这个横轴指的是 training 的过程。随着参数的 update，当然你的 loss 会越来越低,但是结果 20 层的 loss 比较 56 层的 loss还高，这说明 56 层的 network 的 optimization 没有做好，因为 20 层 network 能做到的事，56 层可以轻而易举地做到。

所以如果 56 层的 optimization 成功的话，它的 loss 应当是比 20 层的 network 低的。

那么，**我们怎样知道我们的 optimization 有没有做好**？这边给的建议是：看到一个你从来没有做过的问题，也许你可以先跑一些比较小的、比较浅的network，这些 model 会竭尽全力地找出一组最好的参数，不太会有失败的问题。所以我们可以先 train 一些比较简单的 model，先可以知道它们可以得道什么样的 loss。

<font color="blue">If deeper networks do not obtain smaller loss on training data, then there is optimization issue.</font>

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210313203557412.png" alt="img" style="zoom:67%;" />

+ 这个 5 layer 的 model 就是 optimization 没有做好

如果 optimization 没有做好该怎么办？我们会在之后讲。

### 1.3 overfitting

假设你现在经过一番的努力，你已经可以让 training data 的 loss 变小了，那接下来你就可以来看 testing data loss，如果它仍很大，那可能真的遇到 overfitting 的问题了。

> 注意，**training 的 loss 小，testing 的 loss 大，才有可能是 overfitting**，而不是一看到 testing 上结果不好就说是 overfitting 了。

什么是 overfitting 不再介绍了。

## 2. local minima 与 saddle point

### 2.1 Critical Point

我们只讨论 Optimization 的时候，怎么把 gradient descent 做得更好，为什么 Optimization 会失败呢？

常常在做 Optimization 时，你会发现，**随着你的参数不断的 update，你的 training 的 loss 不会再下降，但是你对这个 loss 仍然不满意**。比如你把 deep 的 network 与 shallow network 比较，发现 deep 的并没有做得更好，所以你会觉得 deep network 没有发挥它完整的力量，所以 Optimization 显然是有问题的。**有时候甚至会发现，一开始你的 model 就 train 不起来，不管你怎样 update 你的参数，你的 loss 通通掉不下去**，这时候到底发生了什么事呢？

过去常见的一个猜想是我们走到了一个地方，**这个地方参数对 loss 的微分为零**，这样 gradient descent 就没有办法再 update 参数了，loss 当然就不会再下降了。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210314153200619.png" alt="img" style="zoom: 67%;" />

local minima 和 saddle point 的 gradient 都是 0，所以当你的 loss 没有办法再下降时，也许就是因为卡在了这样的地方，它们统称为 <mark>critical point</mark>。

但是今天如果你发现你的 gradient 真的很靠近 0，卡在了某个 critical point，我们有没有办法知道，到底是 local minima 还是 saddle point？其实是有办法的。

**为什么我们想知道到底是卡在 local minima 还是卡在 saddle point 呢**？

+ 如果卡在 local minima，那可能就没有路可以走了。因为四周都比较高，你所在的位置就是最低点了
+ 如果卡在 saddle point 的话，它的旁边是还有路可以让 loss 变低的，**只要你逃离 saddle point，你就有可能让你的 loss 更低**

**如何鉴别今天的一个 critical point 是属于 local minima 还是 saddle point 呢**？

### 2.2 判断是 local minima 还是 saddle point

虽然我们没有办法完整知道整个 loss function 的样子，但如果给定某一组参数，比如说蓝色的这个 $\theta'$，在 $\theta'$ 附近的 loss function 是有办法写出来的。所以 $L(\theta)$ 完整的样子写不出来，但它在 $\theta'$ 附近，既可以用一个 Taylor 级数展开来表示它：

$$L(\theta) \approx L(\color{blue}{\theta'}) + (\theta-\color{blue}{\theta'})^T \color{green}{g}+ \frac{1}{2}(\theta-\color{blue}{\theta'})\color{red}{H}(\theta-\color{blue}{\theta'})$$

<div align=center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404073642773.png" alt="image-20220404073642773" style="zoom: 67%;" /></div>

+ 第一项是 $L(\theta')$，当 $\theta$ 与 $\theta'$ 很近时，$L(\theta)$ 与 $L(\theta')$ 也是很靠近的
+ 第二项是 $(\theta-\theta')^T\color{green}{g}$，这个绿色的 $g$ 是一个向量，它就是我们的 gradient，它会弥补 $\theta'$ 与 $\theta$ 之间的差距，这个向量的第 i 个元素就是 $\theta$ 的第 i 个元素对 L 的微分，如下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404074527896.png" alt="image-20220404074527896" style="zoom: 50%;" />

+ 第三项会再补足加上 gradient 之后与真正的 $L(\theta)$ 之间的差距。其中有一个 Hessian 矩阵 $H$，计算方式为：$H_{ij}=\frac{\partial^2}{\partial \theta_i \partial \theta_j}L(\theta')$。

> 比如参数有 $w_1$ 和 $w_2$，那分别求出 $H_{11}=\frac{\partial^2L}{\partial w_1^2}$、$H_{12}=\frac{\partial^2L}{\partial w_1 \partial w_2}$、$H_{21}=\frac{\partial^2L}{\partial w_2 \partial w_1}$、$H_{22}=\frac{\partial^2L}{\partial w_2^2}$ 即可得到 $H$。

当我们走到一个 critical point 时，意味着 gradient 为 0，也就是绿色的这一项完全不见了，只剩下红色的这一项。所以在 critical point 处，它的 loss function 可以被近似为 $L(\theta')$ 加上红色这一项。根据红色这一项，我们就可以判断 $\theta'$ 附近的 error surface 长什么样，从而判断是 local minima 还是 saddle point。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404075617386.png" alt="image-20220404075617386" style="zoom:67%;" />

**怎样根据 Hessian，即红色这一项，来判断附近的地貌呢**？这可以通过将附近的 $\theta'$ 代入计算：

<div align=center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210314161411744.png" alt="img" style="zoom: 50%;" /></div>

但是我们怎么可能把所有的 $v$ 都拿来试一下呢，所以需要一个更加简便的方法来确认。这就需要线性代数的知识了，如果对于所有的 $v$ 而言，$v^THv$ 都大于 0，那这个矩阵 $H$ 叫做正定矩阵（positive definite matrix），它的所有 eigen value（特征值）都是正的。于是我们可以得出如下的结论：

::: theorem 判断是 local minima 还是 saddle point

求出 $H$ 的 eigen value，如果：

+ 所有 eigen value 都是正的，那代表 $\forall v,v^THv \gt 0$，这是一个 local minima；
+ 所有 eigen value 都是负的，那代表 $\forall v,v^THv \lt 0$，这是一个 local maxima；
+ 如果 eigen valule 有正有负，那代表这是一个 saddle point。

:::

以上我们借助 Hessian 矩阵来判断出了一个 critical point 是属于 local minima 还是 saddle point，但在实际的 implementation 里面，你几乎不会真的把 Hessian 算出来，这个要是二次微分，需要的运算量非常大，更遑论还要求它的eigen value，**所以你几乎看不到有人用这一个方法来逃离 saddle point**。之后我们会讲其他的方法来逃离 saddle point，我们这里讲这个方法，是想说，如果是卡在 saddle point，也许没有那么可怕，最糟的状况下你还有这一招可以告诉你要往哪一个方向走。

### 2.3  Saddle Point v.s. Local Minima

一个问题是，到底 **saddle point 跟 local minima 谁比较常见呢**？

> 我们先讲一个可能不太相关的故事。1543 年东罗马帝国的国王不知道要怎么对抗土耳其人，这时有人找来一个魔法师，叫做**狄奥伦娜**。他有一个能力跟张飞一样，可以“*万军从中取上将首级如探囊取物*”，这个狄奥伦娜也一样，他可以直接取得那个苏丹的头。大家想让狄奥伦娜展示一下他的能力，于是他一下拿出了一个圣杯，这个圣杯本来是放在圣索菲亚大教堂的地下室，而且它是被放在一个石棺里面，这个石棺是密封的，没有人可以打开它，但狄奥伦娜却取了出来。为什么他可以做到呢？因为这个石棺你觉得它是封闭的，那是因为你是从三维的空间来看，但狄奥伦娜可以进入四维的空间，从高维的空间中这个石棺是有路可以进去的，它并不是封闭的。
>
> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404084246805.png" alt="image-20220404084246805" style="zoom:67%;" />
>
> 总之这个**从三维的空间来看，是没有路可以走的东西，在高维的空间中是有路可以走的，那 error surface 会不会也一样呢**？

当你在一维的空间中，一维的一个参数的 error surface，你会觉得好像到处都是 local minima，但是会不会在二维空间来看，它就只是一个 saddle point 呢？如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210314205016598.png" alt="img" style="zoom:50%;" />

略过实验过程，从经验上看起来，**其实 local minima 并没有那么常见**，多数的时候，你觉得你 train 到一个地方，你的 gradient 真的很小，然后你的参数不再 update 了，**往往是因为你卡在了一个 saddle point**。

## 3. Batch

实际上在算微分的时候，并不是真的对所有 data 算出来的 L 作微分，而是把所有的 data 分成一个一个的 mini-batch，每次拿一个 batch 来算 loss、算 gradient，从而 update 参数。所有的 batch 看过一遍，叫做一个 epoch。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315142626597.png" alt="image-20210315142626597" style="zoom: 67%;" />

Small Batch v.s. Large Batch

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404085703048.png" alt="image-20220404085703048" style="zoom:67%;" />

考虑如上的两个极端情况，假设我们有 20 笔训练资料：

+ 左边的 case 就是没有用 batch，batch size 设的跟训练资料一样多，这种情况叫做 **Full Batch**，就是没有 batch 的意思
+ 右边的 case 就是 batch size = 1

比较两者，会发现左边没有用 Batch 的方式，它蓄力的时间比较长，还有它技能冷却的时间比较长，你要把所有的资料都看过一遍才能够 update 一次参数。而右边的方法 batch size = 1 的时候，蓄力的时间比较短，每次看到一笔参数，你就会更新一次你的参数。

### 3.1 larger batch 可能花费时间更少

但**实际上考虑并行运算的话，左边这个并不一定时间比较长**。从真正的实验结果来看，比较大的 batch size，你要算 loss，再进而算 gradient，所需要的时间不一定比小的 batch size 要花的时间长。

<font color=blue> Larger batch size does not require longer time to compute gradient.</font>

一个实验结果如下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404090543327.png" alt="image-20220404090543327" style="zoom:80%;" />

+ 纵轴是花费的时间
+ 因为 GPU 有平行运算的能力，因此实际上当你的 batch size 小的时候，你要跑完一个 epoch 花的时间是比大的 batch size 还要多的
+ 但它平行运算能力终究是有个极限，所以你 batch size 真的很大的时候，时间还是会增加的

### 3.2 small batch 训练得到的精确度可能更好

可以看到 Large Batch 在时间上是有优势的。那在训练结果上呢？小的 batch 在训练过程中每一步会受到 noisy 的影响，而 **noisy 的 gradient 反而可以帮助 training**。我们来看一个实验结果，来比较不同 batch size 在精确度方面的不同：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315153147903.png" alt="image-20210315153147903" style="zoom: 80%;" />

+ 横轴代表的是 Batch Size,从左到右越来越大
+ 纵轴代表的是正确率,越上面正确率越高,当然正确率越高越好

可以看到 batch size 越大，它在 training 和 validation 中的 acc 都在降低，这是一个 optimization issue。**当你用大的 Batch Size 的时候，你的 optimization 可能会有问题**。

**为什么 small batch 的 noisy update 会在 training 中更好呢**？一个可能的解释是：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315155345489.png" alt="image-20210315155345489" style="zoom:67%;" />

+ 假如你是 Full Batch，那你今天在 update 你的参数的时候,你就是沿著一个 loss function 来 update 参数，如果走到一个 critical point，就会停下来了从而不再更新参数
+ 假如你是 Small Batch，因为我们是每次挑出一个 batch 来算 loss，这样 update 参数的时候 loss function 是有差异的，比如第一个 batch 中用 L1 来算 gradient，到第二个 batch 时用 L2 来算 gradient，这样假设 L1 算 gradient 是 0，卡住了，但 L2 的 function 与 L1 不同，这样 L2 不会卡住从而 update。

### 3.3 small batch 更容易泛化

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315160405510.png" alt="image-20210315160405510" style="zoom:67%;" />

+ 会发现，小的 batch 居然在 testing 的时候会比较好

一个解释是，**大的 Batch Size，会让我们倾向于走到峡谷里面，而小的 Batch Size，倾向于让我们走到盆地里面**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315161935349.png" alt="img" style="zoom:50%;" />

+ local minima 也有好坏之分，好的 minima 更容易有好的 generalization。

::: warning Large Batch 比较 Small Batch

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315164405953.png" alt="image-20210315164405953" style="zoom: 67%;" />

:::

它们各有擅长的地方，所以 batch size 变成另外一个你需要去调整的 hyperparameter。

> 那我们能不能够鱼与熊掌兼得呢,我们能不能够截取大的 Batch 的优点,跟小的 Batch 的优点,我们用大的 Batch Size 来做训练,用平行运算的能力来增加训练的效率,但是训练出来的结果同时又得到好的结果呢,又得到好的训练结果呢？这时有可能的，有多篇论文给出了一些思路，这里不再介绍。
>
> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404094100122.png" alt="image-20220404094100122" style="zoom:67%;" />

## 4. Momentum

 Momentum 是另外一个可以对抗 critical point 的技术。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404094558500.png" alt="image-20220404094558500" style="zoom:67%;" />

+ 考虑物理世界，当一个球滚到 local minima 时，由于惯性，他可能继续向前走，从而翻过小坡逃离 local minima。

,一般的 Gradient Descent 长什么样子呢：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315170131552.png" alt="image-20210315170131552" style="zoom: 67%;" />

+ 有一个初始参数 $\theta^0$，然后计算一下 gradient，再往 gradient 的反方向去 update 参数：$\theta^1=\theta^0-\eta g^0$。

而  **Gradient Descent + Momentum** 是不只往 Gradient 的反方向来移动参数，而**是 Gradient 的反方向，加上前一步移动的方向，用两者加起来的结果去调整去到我们的参数**。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315171104120.png" alt="image-20210315171104120" style="zoom:67%;" />

+ 先初始化一个参数 $m^0 = 0$，在 $\theta^0$ 的地方计算 gradient 的方向 $g^0$，之后决定下一步怎么走：

$$m^1 = \lambda m^0 - \eta g^0$$

$$\theta^1=\theta^0+m^1$$

之后一直进行这个过程。

来看一个简单的例子：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404095604056.png" alt="image-20220404095604056" style="zoom:67%;" />

当走到一个 local minima 的点时，已经没有 gradient 的方向了 ，但如果有 momentum 的话，就有办法继续走下去，因为下一步的方向不只看 gradient，这样翻过这个小丘的话，也许就走到了一个更好的 local minima。这就是 Momentum 有可能带来的好处。

