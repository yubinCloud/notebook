---
title: Self-Attention
date: 2022-04-05 15:40:26
permalink: /pages/c0e876/
categories:
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

Self-Attention 是一个很常见的 Network 架构。

## 1. 引言

之前我们的 model 的输入是一个向量，输出可能是一个数值，这是 Regression，还可能是一个类别，这是 Classification。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405192828786.png" alt="image-20220405192828786" style="zoom: 80%;" />

假设我们遇到更复杂的问题，比如输入是多个向量，且输入的向量数目会改变。现在如果我们 model 输入的 sequence 的数目、长度都不一样，这时该如何处理？

### 1.1 Vector Set as Input

#### 1）文字处理

假设我们今天要Network的输入是一个句子,每一个句子的长度都不一样,每个句子裡面词汇的数目都不一样, 如果我们把一个**句子裡面的每一个词汇都描述成一个向量**,那我们的Model的输入,就会是一个Vector Set,而且每次句子的长度不一样,那 Vector Set 的大小就不一样：

![image-20220405193330755](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405193330755.png)

那怎么把词汇表示成一个向量呢？最简单的是 one-hot encoding，还有一种方法是 Word Embedding，形成单词的分布式表示。

#### 2）声音信号

一段声音讯号其实是一排向量，把一段声音讯号取一个范围，这个范围叫做一个 **Window**，每个 Window 里面得资讯描述成一个向量，这个向量就是一个 <mark>Frame</mark>：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405193742951.png" alt="image-20220405193742951" style="zoom:80%;" />

把一小段声音讯号变成一个 Frame，有很多种做法，这里不再细讲了。

通常 Window 的长度是 25 个 Millisecond，为了描述一整段声音讯号，我们会把这个 Window 右移一点，通常移动的大小是 10 个 Millisecond：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405194126813.png" alt="image-20220405194126813" style="zoom: 80%;" />

一秒钟声音讯号就有 100 个向量，一分钟声音讯号有 6000 个向量，所以语音其实还是挺复杂的。

#### 3）图

一个 Graph 也是一堆向量。在 Social Network 上面每个节点是一个向量，关系可以视为向量。

#### 4）分子信息

一个分子也可以看作是一个 Graph：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405194428967.png" alt="image-20220405194428967" style="zoom: 67%;" />

一个原子可以用 One-Hot Vector 来表示，比如氢就是 `1000`，碳是 `0010` 等。

### 1.2 What is the output?

刚才看到输入是一堆向量，那我们有可能有什么样的输出呢？

#### 1）每一个向量都有一个对应的 Label

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405194956848.png" alt="image-20220405194956848" style="zoom:80%;" /></center>

当你的模型的输入是四个向量的时候，它就要输出四个 Label，而

+ 若每个 Label 是一个数值，那就是 Regression 的问题
+ 若每个 Label 是一个类别，那就是 Classification 的问题

应用举例：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405195054435.png" alt="image-20220405195054435" style="zoom:67%;" />

+ **POS Tagging**，即词性标注，让机器自动决定每一个词汇的词性，是名词、动词还是形容词等
+ **语音辨识**，对每一个 vector，来辨识它是哪一个 Phonetic
+ 在 Social Network 中，你的 model 来决定每一个节点的特性，比如他会不会买某个商品

#### 2）一整个 Sequence，只需要输出一个 Label

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405195419262.png" alt="image-20220405195419262" style="zoom:67%;" /></center>

比如 Sentiment Analysis（情感分析），给机器看一段话，判断他是正面还是负面的；比如语音辨认，给机器听一段语音，然后判断是谁讲的；比如在 graph 领域，给一个分子，然后预测它有没有毒性等

#### 3）机器要自己决定应该要输出多少个 Label

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405195645751.png" alt="image-20220405195645751" style="zoom:67%;" /></center>

这种任务又叫做 **seq2seq**。

### 1.3 Sequence Labeling

这种输入跟输出数目一样多的状况又叫做 <mark>Sequence Labeling</mark>，我们着重研究这个问题。

解决这个问题的一个简单想法是用 Fully-Connected（简称 **FC**）的 Network：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405200003629.png" alt="image-20220405200003629" style="zoom:80%;" />

但这样有一个巨大的瑕疵，它无法识别出不同语境下“saw”的不同，它有“看见”、“锯子”的意思。怎么办才有可能让 Fully-Connected 的 Network 考虑更多的上下文 context 的资讯呢？

这时有可能的，只需要把前后几个向量都串起来，一起丢到 FC 的 network 中就行了：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405200309936.png" alt="image-20220405200309936" style="zoom:80%;" />

但这样总是有极限的，比如一个任务不是考虑一个 Window 就可以解决的，而是要考虑一整个 Sequence 才能够解决的话，就只能再把 Window 开大一点直到能覆盖整个 sequence。但是这么大的 Window，意味着 FC 的 network 需要非常多的参数，运算量大且容易 overfitting。

所以<u>有没有更好的方法来考虑整个 Input Sequence 的资讯呢？这就要用到我们接下来要跟大家介绍的 Self-Attention 这个技术了</u>。

## 2. Self Attention

### 2.1 self attention 概述

Self-Attention 的运作方式就是**它会吃一整个 Sequence 的资讯**。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405213532561.png" alt="image-20220405213532561" style="zoom:80%;" />

+ Self Attention 中 input 几个 vector 就输出几个 vector，这里输出的 4 个 vector 有个特别的地方：**它们都是考虑一整个 Sequence 以后才得到的**。等一会我们会讲 Self-Attention 怎么考虑到一整个 Sequence 的资讯的。

Self-Attention 输出的向量是 with context 的，这样一来 FC 的 Network 就不只是考虑一个非常小的 Window 了，而是一整个 Sequence 的资讯，再决定应该输出什么样的结果，这个就是 <mark>Self-Attention</mark>。

**Self-Attention 不是只能用一次，也可以叠加很多次**，可以把 Fully-Connected 的 network 跟 self attention 交替使用： 

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405220625274.png" alt="image-20220405220625274" style="zoom:67%;" />

+ Self-Attention 处理整个 Sequence 的资讯
+ FC 的 Network 专注于处理某一个位置的资讯
+ 再用 Self-Attention 把整个 Sequence 资讯再处理一次
+ 然后交替使用 Self-Attention 跟 FC

> 有关 self attention，最知名的文章就是《Attention is all you need》，它提出了 Transformer 的 Network。

### 2.2 self attention 过程

Self-Attention 的 input 是一串的 vector，这个 vector 可能是整个 Network 的 input，也可能是某个 hidden layer 的 output，所以我们这边不是用 $x$ 来表示它，而是用 $a$ 来表示。self attention 的 output 是另一排的 vector $b$。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220406090213518.png" alt="image-20220406090213518" style="zoom: 50%;" /></center>

+ 每一个 $b$ 都是考虑了所有的 $a$ 才生成出来的，所以刻意画了非常非常多的箭头

接下来我们说明一下怎样产生 $b^1$ 这个向量，由此就可以知道怎样产生另外的几个了。

这里有个**特别的机制**：<u>这个机制根据 $a^1$ 这个向量，找出整个 sequence 里到底哪些部分是重要的，哪些部分是我们要决定 $a^1$ 的 class 或 regression 数值的时候所需要用到的资讯</u>。**每一个向量跟 $a^1$ 的关联程度用一个数值 $\alpha$ 表示**：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220406091044650.png" alt="image-20220406091044650" style="zoom: 80%;" /></center>

那如何自动计算两个向量之间的关联性呢，即如何计算两个向量之间的数值 $\alpha$ 呢？这需要一个计算 attention 的模组：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220406091811854.png" alt="image-20220406091811854" style="zoom:80%;" />

这个计算 attention 的模组就是拿两个向量作为输入，输出 $\alpha$。我们常用的是左边 Dot-product 的做法：

+ 输入的这两个向量分别乘上两个不同的矩阵，得到 $q$ 和 $k$
+ $q$ 和 $k$ 做 dot product 得到一个 scalar，这个 scalar 就是 $\alpha$

还有其他许多计算 attention 的模组，之后我们只用上图左边的那种，它也是最常用的。

上图计算过程中的 vector $q$ 叫做 <mark>Query</mark>，vector $k$ 叫做 <mark>key</mark>，它们的上标与输入向量的上标相同，比如 self-attention 的输入向量 $a^1$ 计算得到的是 $q^1$ 和 $k^1$。

用来表示两个向量的关联性的 $\alpha$ 叫做 <mark>attention score</mark>，用 $a_{1,3}=q^1 \cdot k^3$ 作为其下标的含义，这样计算各 attention score 的过程如下图：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220406092825332.png" alt="image-20220406092825332" style="zoom: 67%;" /></center>

实际情况下，一般也会让 $q^1$ 与自己算关联性。计算出 $a^1$ 跟每一个向量的关联性以后，接下来进入一个 softmax：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220406094030458.png" alt="image-20220406094030458" style="zoom:80%;" /></center>

+ 本来有一排的 $\alpha$，经过 softmax 后就得到 $\alpha'$
+ **不一定非要用 softmax，用别的替代也没问题**，换其他的 activation function 都可以，这需要手工调试一试

得到 $\alpha'$ 后就可以根据它去抽取出 sequence 里面重要的资讯。**由 $\alpha'$ 可以知道哪些向量跟 $a^1$ 最有关系**，那怎样抽取重要的资讯呢？

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220406094934071.png" alt="image-20220406094934071" style="zoom:90%;" /></center>

+ 将 $a^i$ 与 $W^v$ 相乘得到 $v^i$
+ 各 $v^i$ 乘上 attention score $\alpha'$
+ 再累加起来得到 $b^1$：$b^1=\sum_i a_{1,i}' v^i$

如果某一个向量它得到的分数越高，比如说如果 $a^1$ 与 $a^2$ 的关联性很强，那这个得到的 $\alpha'$ 的值就很大，那我们做 weight sum 以后，得到的 $b^1$ 的值就可能会比较接近 $v^2$。所以**谁的那个 attention score 最大，谁的那个 $v$ 就会 dominant 你抽出来的结果**。

以上就讲完了如何从一整个 Sequence 里得到 $b^1$。