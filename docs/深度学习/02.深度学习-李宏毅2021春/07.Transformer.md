---
title: Transformer
date: 2022-04-16 10:29:15
permalink: /pages/7b85f5/
categories:
  - 深度学习
  - 深度学习-李宏毅2021春
tags:
  - 
---

## 1. Seq2Seq 的应用

Transformer 就是一个 Sequence-to-sequence（<mark>Seq2Seq</mark>）的 model，它由机器来决定 output 的长度。

#### 1）语音辨识

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416103449161.png" alt="image-20220416103449161" style="zoom: 80%;" />

+ 输入是一串 vector 表示的声音讯号，输出是语音辨识的结果，即对应的文字。这里的输出的长度由机器来决定。

#### 2）机器翻译

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416103640679.png" alt="image-20220416103640679" style="zoom:80%;" />

#### 3）语音翻译

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416103715621.png" alt="image-20220416103715621" style="zoom:80%;" />

+ 比如把听到的英文声音讯号翻译成中文文字

#### 4）语音合成

比如输入是台语声音，输出中文的文字

#### 5）chat bot（聊天机器人）

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416103951311.png" alt="image-20220416103951311" style="zoom:80%;" />

#### 6）QA

即 Question Answering，给机器读一段文字，然后你问机器一个问题，希望他可以给你一个正确的答案。

#### 7）Syntactic Parsing（文法剖析）：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416105557696.png" alt="image-20220416105557696" style="zoom:80%;" />

#### 8）multi-label classification

把它与 multi-class 的 classification 区分一下。

+ multi-class classification 是说对一个东西从多个 class 中选一个出来
+ multi-label classification 是说**同一个东西，他可以属于多个 class**，比如做文章分类：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416110049946.png" alt="image-20220416110049946" style="zoom:67%;" />

#### 9）Object Detection

Object Detection 与 seq2seq model 看起来八竿子打不着，但却可以用它来硬解：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416110246454.png" alt="image-20220416110246454" style="zoom:67%;" />

> 对多数 NLP 任务而言，尽管 seq2seq 可以处理，但**为这些任务定制化模型，往往你会得到更好的结果**。

## 2. Encoder

一般的 seq2seq's model 会分成 Encoder 和 Decoder：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416110732673.png" alt="image-20220416110732673" style="zoom:80%;" />

今天讲到 seq2seq，可能首先想到的就是 Transformer，它也是有一个 Encoder 和一个 Decoder，里面有很多花花绿绿的 block，之后会逐渐讲解。

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210429205517760.png" alt="image-20210429205517760" style="zoom: 67%;" /></center>

### 2.1 简化的 Transformer Encoder

seq2seq model 的 <mark>Encoder</mark>要做的事情，就是**给一排向量，输出另外一排向量**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416111316640.png" alt="image-20220416111316640" style="zoom:80%;" />

我们先简化一下，现在 Encoder 里面会分成很多很多的 block：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416111537287.png" alt="image-20220416111537287" style="zoom:67%;" />

每一个 block 里面做的事情，就是好几个 layer 在做的事情，大概是这样：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416111646159.png" alt="image-20220416111646159" style="zoom:67%;" />

+ 输入一排 vector，先做一个 self-attention 考虑整个 sequence 的资讯，输出另一排 vector，之后再丢进 fully-connected 的 network 里面，其输出作为 block 的输出。

事实上原来的 Transformer 做的事情是更复杂的。

### 2.2 加入 residual 和 layer norm

在之前说 self-attention 的时候，输出的 vector 是考虑了所有 input 后得到的结果。在 Transformer 里面，它加入了一个设计，我们不只是输出这个 vector，还要把这个 vector 加上它的 input，得到新的 output：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416112619930.png" alt="image-20220416112619930" style="zoom: 67%;" />

这样子的 network 架构，就叫做 <mark>residual connection</mark>（**残差连接**）。这种方式在 Deep Learning 领域用的还是很广泛的。

得到 residual 的结果以后，再把它做一个 <mark>layer normalization</mark>：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416112931590.png" alt="image-20220416112931590" style="zoom:80%;" />

::: danger

注意区分 layer normalization 和 batch normalization。

+ **layer normalization** 是输入一个 vector，输出另一个 vector，不需要考虑 batch。它把输入的这个 vector 计算 mean 和 standard deviation。
+ **bacth normalization** 是对不同 example、不同 feature 的同一个 dimension 去计算 mean 和 standard deviation。

:::

计算出 mean $m$ 和 standard deviation $\sigma$ 后，对 input vector 的每一个 dimension 的 $x$ 计算 normalization $x'$：

$$x_i' = \frac{x_i-m}{\sigma}$$

计算得到 layer norm 的输出后，才把它作为 FC 的输入：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416114538035.png" alt="image-20220416114538035" style="zoom: 67%;" />

+ **FC Network 这边也有用到 residual 的架构**：把 FC 的 input 与 output 加起来，做一下 residual 得到新的输出。
+ FC 做完 residual 后，还要再把 residual 的结果再做一次 layer norm，得到的输出才是 block 的输出。

### 2.3 小结 Encoder

刚刚所讲的，总结起来就是下面这张图了：

<center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416115049275.png" alt="image-20220416115049275" style="zoom:67%;" /></center>

+ 光用 self-attention 是没有位置的资讯的，因此需要加上一个 positional encoding
+ self-attention 用了 multi-head 的 attention
+ `Add & Norm` 就是 residual + layer norm
+ 接下来经过 fully-connected 的 Feed Forward，之后再做一次 Add & Norm，才是一个 block 的输出
+ 这个 block 会重复 N 次。这个复杂的 block 会用到之后的 BERT 里面。

### 2.4 To Learn More

也许你会问，Transformer 的 Encode 为什么要这样设计，其实也可以不这样设计。原始论文的设计不代表它是最 optimal 的设计。比如以下论文提出的：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220416122847194.png" alt="image-20220416122847194" style="zoom: 80%;" />

+ 第一篇是说能不能把 layer norm 放到每一个 block 的 input。比如上图中左边是原始的 Transformer，右边是把 block 内部稍微更换顺序后的 Transformer。这样更换之后结果是比较好的。
+ 还有一篇 paper 是说在 Transformer 里面为什么 batch norm 不如 layer  norm，接下来又提了一个 power norm，它比 layer norm 的 performance 的效果稍微好一点。