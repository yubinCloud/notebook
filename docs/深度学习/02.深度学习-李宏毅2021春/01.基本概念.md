## 1. Machine Learning

先简单介绍一下 machine learning 还有 deep learning 的基本概念。**机器学习就是让机器具备找一个函式的能力**。

> **函式**即 function，也就是常说的函数。

### Different types of Functions

+ **Regression**：要找的函式的输出是一个 scalar，即数值
+ **Classification**：就是要机器做选择题
+ **Structured Learning**：让机器画一张图，写一篇文章，这种叫机器產生有结构的东西的问题就叫作 Structured Learning

### Case Study

机器怎么找一个函式呢？举一个例子，在 YouTube 后台，你可以看到很多相关的资讯，比如说每一天按讚的人数有多少，每一天订阅的人数有多少，每一天观看的次数有多少。我们能不能够根据一个频道过往所有的资讯去预测它明天有可能的观看的次数是多少呢，我们能不能够<u>找一个函式，这个函式的输入是我 YouTube 后台的资讯，输出就是某一天，隔天这个频道会有的总观看的次数</u>。

**机器学习找这个函式的过程，分成三个步骤**，那我们就用 YouTube 频道点阅人数预测这件事情，来跟大家说明这三个步骤，是怎么运作的：

#### 1）Function with Unknown Parameters

第一个步骤是我们要**写出一个带有未知参数的函式**。简单来说就是 我们先猜测一下,我们打算找的这个函式,它的数学式到底长什麼样子。举例来说，我们这边先做一个最初步的猜测，我们写成这个样子：$y = b + w \times x$，其中：

+ <u>y 是我们準备要预测的东西</u>，我们準备要预测的是今天 2 月 26 号这个频道总共观看的人；
+ <u>$x_1$ 是这个频道前一天总共观看的人数</u>，$y$ 跟 $x_1$ 都是数值；
+ <u>b 跟 w 是未知的参数</u>，它是准备要透过资料去找出来的，我们还不知道w跟b应该是多少,我们只是隐约的猜测

**这个猜测往往就来自于你对这个问题本质上的了解，也就是 <u>Domain knowledge</u>**，所以才会听到有人说,这个做机器学习啊，就需要一些 Domain knowledge。

我们就随便猜说 $y=b+w*xₗ$，而 <u>b 跟 w 是未知的</u>，**这个带有 Unknown 的 Parameter 的 Function 我们就叫做 <mark>Model</mark>**。这个 $x_1$ 是这个 function 里面我们已知的，它是来自于 YouTube 后台的资讯，我们已经知道2月25号点阅的总人数是多少，称之为 <mark>Feature</mark>。

#### 2）Define Loss from Training Data

第二个步骤我们要**定义一个 Loss**。<u><mark>Loss</mark> 也是一个 function，输入是 Model 里面的参数，输出的值代表说，现在如果我们把这一组未知的参数，设定某一个数值的时候，这笔数值好还是不好</u>。比如例子中 $b、w$ 就是 Loss 的输入。Loss 要从训练样本中进行计算，计算结果为 L，<u>大 L 越大，代表我们现在这一组参数越不好</u>，这个大 L 越小，代表现在这一组参数越好。

估测的值跟实际的值之间的差距，其实有不同的计算方法。比如 MAE、MSE 以及 Cross-Entropy 等。

为不同的 w 跟 b 的组合，都去计算它的 Loss，然后就可以画出以下这一个等高线图，称为 **Error Surface**：

![image-20220120144323003](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220120144323003.png)

+ 越偏红色系，代表计算出来的Loss越大，就代表这一组 w 跟 b 越差，如果越偏蓝色系，就代表 Loss 越小，就代表这一组 w 跟 b 越好。

#### 3）Optimization

第三步要做其实是**解一个最佳化的问题**。在这个例子中就是找一个 w 跟 b，从未知的参数中找一个数值出来，代入后可以让 Loss 值最小。在这一门课里面，我们唯一会用到的 Optimization 的方法叫做 <mark>Gradient Descent</mark>。

为了要简化，我们先假设未知的参数只有一个 $w$，得到其 Error Surface：

![image-20220120144912668](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220120144912668.png)

**那怎样找一个 w 让这个 loss 的值最小呢**？随机选取一个初始的点，这个初始的点,我们叫做 $w_0$ -> 求参数对 Loss 的微分 -> 更新参数值。更新时这一步要跨多大呢？这一步的步伐的大小取决于这个地方的斜率和学习率 ($\eta$)。

**什么时候停下来呢**？往往有两种状况：

+ 第一种状况是你失去耐心了，你一开始会设定说，我今天在调整我的参数的时候，我**最多计算几次**；
+ 那还有另外一种理想上停下来的可能是，今天当我们不断调整参数时调整到一个地方，它的微分的值算出来正好是 0 的时候，如果这一项正好算出来是0.0乘上 learning rate 还是 0，所以你的参数就不会再移动位置，那参数的位置就不会再更新。

![image-20220120152717142](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220120152717142.png)

你可能会发现 Gradient Descent 这个方法有一个巨大的问题，我们没有找到真正最好的解，我们只是找到的 local minima 而不是 global minima。其实，**local minima是一个假问题**，我们在做 Gradient Descent 的时候真正面对的难题不是 local minima，之后会讲到它的真正痛点在哪。

刚刚只有一个参数 w，将其扩展至二维乃至多维是同理。