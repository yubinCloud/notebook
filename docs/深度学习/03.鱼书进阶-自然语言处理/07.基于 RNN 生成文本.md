---
title: 基于 RNN 生成文本
date: 2022-04-01 20:04:07
permalink: /pages/54bf96/
categories:
  - 深度学习
  - 鱼书进阶-自然语言处理
tags:
  - 
---

本章我们将利用 LSTM 实现几个有趣的应用。首先将使用语言模型进行文本生成，然后再介绍 seq2seq 的神经网络，将一个时序数据转换为另一个时序数据。

## 1. 使用语言模型生成文本

语言模型可用于各种各样的应用，其中具有代表性的例子有机器翻译、语音识别和文本生成。这里，我们将使用语言模型来生成文本。

### 1.1 使用 RNN 生成文本的步骤

上一章我们用 LSTM 层实现了语言模型，网络结构如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401205251936.png" alt="image-20220401205251936" style="zoom:80%;" />

现在我们来说明一下语言模型生成文本的顺序。这里仍以“you say goobye and i say hello.”这一在语料库上学习好的语言模型为例，考虑将单词 i 赋给这个语言模型的情况：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401210407764.png" alt="image-20220401210407764" style="zoom:80%;" />

+ 语言模型根据已经出现的单词输出下一个出现的单词的概率分布。

**语言模型计算出下一个出现的单词的概率分布后，它如何生成下一个新单词呢**？<u>一种方法是选择概率最高的单词</u>，这时结果能唯一确定；<u>另一种方法是根据概率分布进行选择</u>，这样概率高的单词容易被选到，这时被采样到的单词每次都不一样。

这里我们想让每次生成的文本有所变化，因此我们使用后一种方法来选择单词。下面是生成的过程：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401212135069.png" alt="image-20220401212135069" style="zoom:80%;" />

之后根据需要重复此过程即可，直到出现 `<eos>` 这一结尾记号，这样一来，我们就可以生成新的文本。

> 这里需要注意的是，像上面这样生成的新文本是训练数据中没有的新生成的文本。因为**语言模型并不是背诵了训练数据，而是学习了训练数据中单词的排列模式**。如果语言模型通过语料库正确学习了单词的出现模式，我们就可以期待该语言模型生成的文本对人类而言是自然的、有意义的。

### 1.2 文本生成的实现

下面我们进行文本生成的实现，我们基于上一章的 Rnnlm 类来创建继承自它的 RnnlmGen 类，然后向这个类添加生成文本的方法：

```python
class RnnlmGen(Rnnlm):
    def generate(self, start_id, skip_ids=None, sample_size=100):
        word_ids = [start_id]

        x = start_id
        while len(word_ids) < sample_size:
            x = np.array(x).reshape(1, 1)
            score = self.predict(x)
            p = softmax(score.flatten())

            sampled = np.random.choice(len(p), size=1, p=p)
            if (skip_ids is None) or (sampled not in skip_ids):
                x = sampled
                word_ids.append(int(x))

        return word_ids
```

这个类用 `generate(start_id, skip_ids, sample_size)` 生成本文，此处 `start_id` 是第一个单词 ID，`sample_size` 表示要采样的单词数量，`skip_ids` 是单词 ID 列表，它指定的单词将不被采样，这个参数用于排除 PTB 数据集中的 `<unk>` 、N 等被预处理过的单词。generate 方法首先通过 `model.predict(x)` 输出各个单词的得分，然后基于 `p=softmax(score)` 对得分进行正则化，这样就获得了我们想要的概率分布。

> PTB 数据集对原始文本进行了预处理，稀有单词被 `<unk>` 替换，数字被 N 替换。另外，我们用 `<eos>` 作为文本的分隔符。

现在，使用这个 RnnlmGen 类进行文本生成，这里先在完全没有学习的状态（即权重参数是随机初始值的状态）下生成文本。我们将第一个单词设为 you，可以得到如下的句子：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401224157250.png" alt="image-20220401224157250" style="zoom:80%;" />

可见，输出的文本是一堆乱七八糟的单词，不过这可以理解，因此这里的模型权重使用的是随机初始值，所以输出了没有意义的文本。我们再利用上一章学习好的权重来进行文本生生成，为此使用 `model.load_params(...)` 读入学习好的参数，并生成文本：

![image-20220401224448455](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401224448455.png)

虽然上面的结果中可以看到多处语法错误和意思不通的地方，不过也有几处读起来已经比较像句子了。这个实验生成的文本在某种程度上可以说是正确的，不过结果中仍有许多不自然的地方，改进空间很大。

### 1.3 更好的文本生成

之前我们改进了语音模型，实现了 BetterRnnlm 类们现在我们继承它并用于文本生成，结果如下：

![image-20220401224753529](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220401224753529.png)

可以看出，这个模型生成了比之前更自然的文本。

## 2. seq2seq 模型

文本数据、音频数据和视频数据都是时序数据，并存在许多需要将一种时序数据转换为另一种时序数据的任务，比如机器翻译、语音识别、聊天机器人、将源代码转为机器语言的编译器等。像这样，世界上存在许多输入输出均为时序数据的任务。现在我们会考察将时序数据转换为其他时序数据的模型，作为实现，我们将介绍使用两个 RNN 的 seq2seq 模型。



