---
title: Gated RNN
date: 2022-03-31 10:43:50
permalink: /pages/b4c94a/
categories:
  - 深度学习
  - 鱼书进阶-自然语言处理
tags:
  - 
---

上一章的 RNN 存在环路，可以记忆过去的信息，但这个 RNN 的效果并不好。原因在于，许多情况下它都无法很好地学习到时序数据的长期依赖关系。目前简单的 RNN 经常被名为 LSTM 或 GRU 的层所代替。实际上，当我们说 RNN 时，更多的是指 LSTM 层。

LSTM 和 GRU 中增加了一种名为“门”的结构。基于这个门，可以学习到时序数据的长期依赖关系。我们将介绍代替它的 LSTM 和 GRU 等“Gated RNN”，研究 LSTM 的结构并揭示它实现“长期记忆”的机制。

## 1. RNN 的问题

我们举一个例子来说明为什么 RNN 层不擅长长期记忆。

复习 RNN：当输入时序数据 $x_t$ 时，RNN 层输出 $h_t$。这个 $h_t$ 也称为 RNN 层的隐藏状态，它记录过去的信息。RNN 利用了上一时刻的隐藏状态来实现继承过去的信息，其计算图为：

![image-20220331220412796](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331220412796.png)

RNN 层的正向传播进行的计算由矩阵乘积、矩阵加法和基于激活函数 tanh 的变换构成，我们来看一下这个 RNN 层存在的问题。

### 1.1 梯度消失和梯度爆炸

语言模型的任务是根据已经出现的单词预测下一个将要出现的单词。在 RNNLM 模型中如果要完成下面这个任务：

![image-20220331221027963](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331221027963.png)

此处应填的是 Tom，而要正确回答这个问题，RNNLM 需要记住前面的两个句子的信息，这些信息必须被编码保存在 RNN 层的隐藏状态中。当我们使用 BPTT 进行学习时，梯度将从正确解标签 Tom 出现的地方向过去的方向传播：

![image-20220331221232874](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331221232874.png)

**RNN 层通过向过去传递“有意义的梯度”，能够学习时间方向上的依赖关系**。但如果这个梯度在中途变弱（甚至没有包含任何信息），则权重参数将不会被更新。不幸的是，**随着时间的回溯，这个简单 RNN 未能避免梯度变小（梯度消失）或者梯度变大（梯度爆炸）的命运**，因此其无法学习长期的依赖关系。

### 1.2 梯度消失和梯度爆炸的原因

我们深挖一下 RNN 层中梯度消失（或者梯度爆炸）的起因。如下图，这里仅关注 RNN 层在时间方向上的梯度传播：

![image-20220331221609240](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331221609240.png)

考虑第 T 个正确解标签是 Tom 的情形，此时关注时间方向上的梯度，可知反向传播的梯度流经 tanh、“+”和 MatMul（矩阵乘积）运算。

“+”的反向传播将上游传来的梯度原样传给下游，因此梯度的值不变。而 $y=\tanh (x)$ 的导数是 $\frac{dy}{dx}=1-y^2$，其值的变化如图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331222018922.png" alt="image-20220331222018922" style="zoom:80%;" />

可以看出，当它的值小于 1.0，并且随着 x 远离 0，它的值在变小，这意味着如果经过 tanh 函数 T 次，则梯度也会减小 T 次。

> RNN 的激活函数一般使用 tanh 函数，但是如果改为 ReLU 函数，则有希望抑制梯度消失的问题，这是因为在 ReLU 的情况下，当 x 大于 0 时，反向传播将上游的梯度原样传递到下游，梯度不会“退化”。

下面我们关注 MatMul（矩阵乘积）节点，忽略其他节点，这样 RNN 层的反向传播的梯度就仅取决于 MatMul 运算：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331222352326.png" alt="image-20220331222352326" style="zoom:80%;" />

+ 假定从上游传来梯度 $dh$，此时 MatMul 节点的反向传播通过矩阵乘积 $dh *W_h^T$ 计算梯度，之后，根据时序数据的时间步长，将这个计算重复相应次数。这里需要注意的是，每一次矩阵乘积计算都使用相同的权重 $W_h$。

那么，反向传播时梯度的值通过 MatMul 节点时会如何变化呢？实验后可以看出结果：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331222848415.png" alt="image-20220331222848415" style="zoom:80%;" />

可知梯度的大小随着时间步长呈指数级增加，这就是**梯度爆炸**。<u>如果发生梯度爆炸，最终就会导致溢出，出现 NaN 之类的值。如此一来，神经网络的学习将无法正确运行</u>。

修改一下初始值，可以看到如下结果：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331223026721.png" alt="image-20220331223026721" style="zoom:80%;" />

可以看到这次梯度呈指数级减小，这就是**梯度消失**。<u>如果发生梯度消失，梯度将迅速变小。一旦梯度变小，权重梯度不能被更新，模型就会无法学习长期的依赖关系</u>。

为什么会出现梯度的大小或者呈指数级增加，或者呈指数级减小？**因为矩阵 $W_h$ 被反复乘了 T 次**。如果 $W_h$ 是标量，这很容易理解。而如果 $W_h$ 是矩阵的话，矩阵的奇异值将成为指标。简单而言，矩阵的奇异值表示数据的离散程度。根据这个奇异值（更准确地说是多个奇异值中的最大值）是否大于 1，可以预测梯度大小的变化。

> 如果奇异值的最大值大于 1，则可以预测梯度很有可能会呈指数级增加；而如果奇异值的最大值小于 1，则可以判断梯度会呈指数级减小。但并不是说奇异值比 1 大就一定会出现梯度爆炸。这是必要条件，而不是充分条件。

### 1.3 梯度爆炸的对策

解决梯度爆炸有既定的方法，称为**梯度裁剪**（gradients clipping），其伪代码为：<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220331223522862.png" alt="image-20220331223522862" style="zoom:73%;" />

+ 假设可以将神经网络用到的所有参数的**梯度**整合成一个，并用符号 $\hat{g}$ 表示。
+ <u>将阈值设置为 threshold。此时，如果梯度的 L2 范数 $||\hat{g}||$ 大于或等于阈值，就按上述方法修正梯度，这就是梯度裁剪</u>。

虽然这个方法很简单，但是在许多情况下效果都不错。

我们用 Python 来实现一下梯度裁剪：

```python
import numpy as np

dW1 = np.random.rand(3, 3) * 10
dW2 = np.random.rand(3, 3) * 10
grads = [dW1, dW2]  # 梯度的列表
max_norm = 5.0      # 阈值

def clip_grads(grads, max_norm):
	total_norm = 0
	for grad in grads:
		total_norm += np.sum(grad ** 2)
	total_norm = np.sqrt(total_norm)
    
	rate = max_norm / (total_norm + 1e-6)
	if rate < 1:
		for grad in grads:
            grad *= rate

clip_grads(grads, max_norm)
```

> 我们在用于 RNNLM 学习的 RnnlmTrainer 类的内部利用了上述梯度裁剪以防止梯度爆炸。

以上是对梯度裁剪的说明，用于解决梯度爆炸的问题。下面，我们看一下防止梯度消失的对策。

## 2. 梯度消失和 LSTM

梯度消失也是一个大问题，为了解决它，需要从根本上改变 RNN 层的结构，这里本章的主题 Gated RNN 就要登场了。人们提出了很多 Gated RNN 结构，其中 LSTM 和 GRU 比较具有代表性，我们将关注 LSTM 并仔细研究它的结构，并阐明为何它不会（难以）引起梯度消失。

### 2.1 LSTM 的接口



