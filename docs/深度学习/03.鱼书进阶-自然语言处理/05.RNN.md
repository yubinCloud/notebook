---
title: RNN
date: 2022-03-27 22:37:43
permalink: /pages/529862/
categories:
  - 深度学习
  - 鱼书进阶-自然语言处理
tags:
  - 
---

之前我们看到的神经网络都是**前馈型**神经网络，网络的传播方向是单向的。这种网络不能很好地处理时间序列数据（简称**时序数据**），于是，循环神经网络（RNN）出现了。

## 1. 概率和语言模型

作为介绍 RNN 的准备，我们用概率描述自然语言相关的现象，最后介绍从概率视角研究语言的“语言模型”。

### 1.1 概率视角下的 word2vec

$P(w_t|w_{t-1},w_{t+1})$ 表示当给定 $w_{t−1}$ 和 $w_{t+1}$ 时目标词是 $w_t$ 的概率，窗口大小为 1 的 CBOW 模型就是对这一后验概率进行建模。

如果将上下文限定为左侧窗口，仅将左侧 2 个单词作为上下文的情况下，CBOW 模型输出的概率为 $P(w_t|w_{t-2},w_{t-1})$，这时 CBOW 模型的损失函数可以写出 $L=-\log P(w_t|w_{t-2},w_{t-1})$。CBOW 模型的学习就是找到使这个损失函数值最小的权重参数。为了达成这一目标，随着学习的推进，（作为副产品）获得了编码了单词含义信息的单词的分布式表示。

### 1.2 语言模型

**语言模型**给出了单词序列发生的概率，即在多大程度上是自然的单词序列。比如对“you say goodbye”这一单词序列给出高概率（比如 0.092），而对“you say good die”这一序列给出低概率（比如 0.00000032）。

语言模型可以应用于多种应用，典型的例子有机器翻译和语音识别，使用语言模型，可以按照“作为句子是否自然”这一基准对候选句子进行排序。语言模型也可以用于生成新的句子。因为语言模型可以使用概率来评价单词序列的自然程度，所以它可以根据这一概率分布造出（采样）单词。

m 个单词 $w_1,...,w_m$ 按顺序构成的句子出现的概率记为一个**联合概率** $P(w_1,...,w_m)$。

使用后验概率可以拆解联合概率：

$$P(A,B)=P(A|B)P(B)$$

因此有：

$$P(w_1,\dots,w_m)=P(w_m|w_1,\dots,w_{m-1})P(w_{m-1}|w_1,\dots,w_{m-2}) \dots P(w_2|w_1)P(w_1)$$

注意这里的后验概率是以目标词左侧的全部单词为上下文（条件）时的概率：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220328104602650.png" alt="image-20220328104602650" style="zoom:67%;" />

这里我们总结一下，我们的目标是求 $P(w_t|w_1,\dots,w_{t-1})$ 这个概率，如果能计算这个概率，就能求得语言模型的联合概率 $P(w_1,\dots,w_m)$。

### 1.3 将 CBOW 模型用作语言模型？

如果要把 word2vec 的 CBOW 模型（强行）用作语言模型，该怎么办呢？可以通过将上下文的大小限制在某个值来近似实现，比如限定为左侧 2 个单词：

$$P(w_1,\dots,w_m)=\prod_{t=1}^m P(w_t|w_1,\dots,w_{t-1}) \approx P(w_t|w_{t-2},w_{t-1})$$

> **马尔可夫性**是指未来的状态仅依存于当前状态。此外，当某个事件的概率仅取决于其前面的 N 个事件时，称为“**N 阶马尔可夫链**”。这里展示的是下一个单词仅取决于前面 2 个单词的模型，因此可以称为“2 阶马尔可夫链”。

上下文大小虽说可以设定为任意长度，但必须是某个“固定”长度，**这总会导致上下文更左侧的单词的信息会被忽略**，比如：

![image-20220328105514649](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220328105514649.png)

+ 这个问题要获得答案，比如把前面 18 个单词处的 Tom 记住，当 CBOW 上下文大小为 10 时，这个问题将无法被正确回答。

**CBOW 模型还存在忽视了上下文中单词顺序的问题**。当上下文大小为 2 时，CBOW 模型的中间层是那 2 个单词向量的和，，因此上下文的单词顺序会被忽视。比如 (you, say) 和 (say, you) 会被作为相同的内容进行处理。

> CBOW 是 Continuous Bag-Of-Words 的简称。Bag-Of-Words 是“一袋子单词”的意思，这意味着袋子中单词的顺序被忽视了。

如果想要考虑上下文中单词顺序的模型，可以像下图那样拼接中间层，但这样会导致权重参数的数量将与上下文大小成比例地增加：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220328105926163.png" alt="image-20220328105926163" style="zoom:80%;" />

如何解决这些问题呢？这就轮到 RNN 出场了。<u>RNN 具有一个机制，那就是无论上下文有多长，都能将上下文信息记住</u>。因此，使用 RNN 可以处理任意长度的时序数据。

## 2. RNN

### 2.1 循环的神经网络

循环需要一个“环路”，随着数据的循环，信息不断被更新。RNN 的特征就在于拥有这样一个环路（或回路）。这个环路可以使数据不断循环。通过数据的循环，RNN 一边记住过去的数据，一边更新到最新的数据。

> 血液在我们体内循环。今天流动的血液是接着昨天的血液继续流动的。血液通过在体内循环，从过去一直被“更新” 到现在。

我们来看一下 RNN 层：<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220328111603288.png" alt="image-20220328111603288" style="zoom:67%;" />

通过该环路，数据可以在层内循环。在上图中，时刻 t 的输入是 $x_t$，这暗示着时序数据 $(x_0, x_1, \dots , x_t, \dots)$ 会被输入到层中。然后，以与输入对应的形式，输出 $(h_0, h_1, \dots, h_t, \dots)$。这里假定输入的 $x_t$ 是向量，比如处理句子时，将各单词的分布式表示作为 $x_t$ 输入 RNN 层。

看一下 RNN 层中，可以发现输出有两个分叉，这意味着同一个东西被复制了。**输出中的一个分叉将成为其自身的输入**。

下面我们详细介绍一下这个循环结构。

### 2.2 展开循环

通过展开循环，可以将其转化为我们熟悉的神经网络：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220328111715658.png" alt="image-20220328111715658" style="zoom:80%;" />

我们将其转化为了从左向右延伸的长神经网络，这样看上去数据就只向一个方向传播了。不过**这里的多个 RNN 层都是同一个层**。

> 时序数据按时间顺序排列。因此，我们用“时刻”这个词指代时序数据的索引（比如，时刻 t 的输入数据为 $x_t$ ）。在 NLP 的情况下，既使用“第 t 个单词”“第 t 个 RNN 层”这样的表述，也使用“时刻 t 的单词”或者“时刻 t 的 RNN 层”这样的表述。

各个时刻的 RNN 层接收传给该层的输入和前一个 RNN 层的输出，然后据此计算当前时刻的输出：

$$h_t= \tanh (h_{t-1}W_h + x_t W_x + b)$$

::: note 符号说明

RNN 有两个权重：

+ 将输入 $x$ 转化成输出 $h$ 的权重 $W_x$
+ 将前一个 RNN 层的输出 $h_{t-1}$ 转化为当前时刻输出的权重 $W_h$

:::