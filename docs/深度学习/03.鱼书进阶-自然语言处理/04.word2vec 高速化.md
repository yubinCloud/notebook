---
title: word2vec 高速化
date: 2022-03-20 14:01:11
permalink: /pages/fc3337/
categories:
  - 深度学习
  - 鱼书进阶-自然语言处理
tags:
  - 
---

上一章实现的简单的 CBOW 模型存在几个问题，其中最大的问题是，随着语料库中处理的词汇量的增加，计算量也随之显著增加。本章将重点放在 word2vec 的加速上，来改善 word2vec：

+ 引入名为 Embedding 层的新层
+ 引入名为 Negative Sampling 的新损失函数

复习一下上一章的 CBOW 模型：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321092058887.png" alt="image-20220321092058887" style="zoom:80%;" />

在处理大规模语料库时，这个模型就存在多个问题了。假设词汇量有 100 万个，输入层和输出层存在 100 万个神经元。此时以下两个地方的计 算会出现瓶颈：

+ 输入层的 one-hot 表示和权重矩阵 $W_{in}$ 的乘积。这个问题通过引入 Embedding 层来解决。
+ 中间层和权重矩阵 $W_{out}$ 的乘积以及 Softmax 层的计算。这个问题通过引入 Negative Sampling 这一新的损失函数来解决。

## 1. word2vec 改进 —— Embedding

### 1.1 Embedding 层

我们来考虑词汇量是 100 万个的情况，one-hot 表示的上下文和 MatMul 层的权重的乘积如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321092638881.png" alt="image-20220321092638881" style="zoom:80%;" />

此时我们需要计算这个巨大向量和权重矩阵的乘积，但如图所示，所做的无非是将矩阵的某个特定的行取出来，因此做这个矩阵乘法是没有必要的。

我们创建一个从权重参数中抽取“**单词 ID 对应行（向量）**”的 层，这里我们称之为 **Embedding 层**。

> Embedding，“词嵌入”，该层存放词嵌入（分布式表示）。在 NLP 中，单词的密集向量表示称为**词嵌入**（word  embedding）或者单词的**分布式表示**（distributed representation）。

### 1.2 Embedding 层的实现

假设权重 W 是二维数组，如果要从这个权重中取出某个特定的行，只需写 `W[2]` 或者 `W[5]` 即可。另外，从权重 W 中一次性提取多行的处理也很简单。只需通过数组指定行号即可，比如 `W[[2, 5]]` 便提取出 2、5 行，这种提取用于 mini-batch 处理。

下面，我们来实现 Embedding 层的 `forward()` 方法：

```python
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None

    def forward(self, idx):
        W, = self.params
        self.idx = idx
        out = W[idx]
        return out
```

+ 在成员变量 `idx` 中以数组的形式保存需要提取的行的索引（单词 ID）

接下来，我们考虑反向传播。Embedding 层的正向传播只是从权重矩阵 W 中提取特定的行，并将该特定行的神经元原样传给下一层。因此，在反向传播时，从上一层（输出侧的层）传过来的梯度将原样传给下一层（输入侧的层）：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321093826276.png" alt="image-20220321093826276" style="zoom:80%;" />

实现 `backward()` 如下：

```python
def backward(self, dout):
    dW, = self.grads
    dW[...] = 0
    if GPU:
        np.scatter_add(dW, self.idx, dout)
    else:
        np.add.at(dW, self.idx, dout)
    return None
```

+ 这里，取出权重梯度 dW，通过 `dW[...] = 0` 将 dW 的元素设为 0（并不是将 dW 设为 0，而是保持 dW 的形状不变，将它的元素设为 0）。然后，将上一层传来的梯度 dout 写入 idx 指定的行。
+ 使用 `add` 来修改 dW 是因为当 idx 中的元素出现重复时，比如 `idx=[0,2,0,4]` 时会发生下图的情况，这时应该把 dh 各行的值累加到 dW 的对应行中：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321094247929.png" alt="image-20220321094247929" style="zoom:80%;" />

> 这里创建了和权重 W 相同大小的矩阵 dW，并将梯度写入了 dW 对应的行。但是，我们最终想做的事情是更新权重 W，所以没有必要特意创建 dW（大小与 W相同）。相反，只需把需要更新的行号（idx）及其对应的梯度（dout）保存下来，就可以更新权重（W）的特定行。但是，这里为了兼容已经实现的优化器类，所以写成了现在的样子。

关于 Embedding 层的实现就介绍到这里。现在，**我们可以将 word2vec（CBOW 模型）的实现中的输入侧的 MatMul 层换成 Embedding 层**。这样 一来，既能减少内存使用量，又能避免不必要的计算。

## 2. word2vec 改进 —— 负采样

在上一节中，通过引入 Embedding 层，节省了输入层中不必要的计算。剩下的问题就是中间层之后的处理。此时，在以下两个地方需要很多计算时间：

+ 中间层的神经元和权重矩阵（$W_{out}$）的乘积
+ Softmax 层的计算

第 1 个问题在于巨大的矩阵乘积计算，所以很有必要将矩阵乘积计算轻量化。

其次，Softmax 也会发生同样的问题。观察 softmax 的公式：$y_k=\frac{exp(s_k)}{\sum_{i=1}^{1000000}exp(s_i)}$ 可知这个计算也与词汇量成正比，因此需要一个可以替代 Softmax 的轻量的计算。

### 2.1 从多分类到二分类

我们来解释一下负采样。这个方法的关键思想是**用二分类拟合多分类**。

> 二分类处理的是答案为“Yes/No”的问题，比如“目标词是 say 吗”

之前我们处理的都是多分类问题，把它看作了从 100 万个单词中选择 1 个正确单词的任务，比如对于“当上下文是 you 和 goodbye 时，目标词是什么？”这个问题，神经网络可以给出正确答案。现在我<u>们来考虑如何将多分类问题转化为二分类问题</u>。比如，让神经网络来回答“**当上下文是 you 和 goodbye 时，目标词是 say 吗**？”这个问题，这时输出层只需要一个神经元即可，可以认为输出层的神经元输出的是 say 的得分。

此时 CBOW 的模型可以表示为下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321100433199.png" alt="image-20220321100433199" style="zoom:80%;" />

如上图，输出层的神经元仅有一个。因此，要计算中间层和输出侧的权重矩阵的乘积，只需要提取 say 对应的列（单词向量），并用它与中间层的神经元计算内积即可，这个过程如下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321100524319.png" alt="image-20220321100524319" style="zoom:67%;" />

如上图，输出侧的权重 $W_{out}$ 中保存了各个单词 ID 对应的单词向量。此处，<u>我们提取 say 这个单词向量，再求这个向量和中间层神经元的内积，这就是最终的得分</u>。这个得分会经过 sigmoid 函数将其转换为概率。

### 2.2 sigmoid 函数和交叉熵误差

要使用神经网络解决二分类问题，需要**使用 sigmoid 函数将得分转化为概率**。为了求损失，我们**用交叉熵误差作为损失函数**。这些都是二分类神经网络的老套路。

> 多分类时，输出层使用 softmax 将得分转换成概率，损失函数使用交叉熵误差；在二分类中，输出层使用 sigmoid，损失函数也使用交叉熵误差。

sigmoid 函数：$y=\frac{1}{1+exp(-x)}$，输入 x 被转化为 0-1 之间的实数，这个输出 y 可以被解释为概率。Softmax 层如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321101402782.png" alt="image-20220321101402782" style="zoom:80%;" />

通过 sigmoid 函数得到概率 y 后，可以由概率 y 用交叉熵误差来计算损失，其数学表达式为：

$$L=-(t \log y + (1-t) \log (1-y))$$

+ y 是 sigmoid 的输出
+ t 是正确解标签，取值 0/1：0 表示正确解是 “Yes”，0 表示正确解是 “No”

> 二分类和多分类的损失函数均为交叉熵误差，两者的数学式只是写法不同而已。

我们用图来表示 Sigmoid 层和 Cross Entropy Error 层：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220321101811958.png" alt="image-20220321101811958" style="zoom:80%;" />

+ 值得注意的是反向传播的 y − t 这个值。y 是神经网络输出的概率，t 是正确解标签，y − t 正好是这两个值的差。这意味着，当正确解标签是 1 时，如果 y 尽可能地接近 1（100%），误差将很小。反过来，如果 y 远离 1，误差将增大。随后，这个误差向前面的层传播，当误差大时，模型学习得多；当误差小时，模型学习得少。

### 2.3 多分类到二分类的实现

