---
title: word2vec
date: 2022-03-18 21:25:18
permalink: /pages/538482/
categories:
  - 深度学习
  - 鱼书进阶-自然语言处理
tags:
  - 
---

之前我们使用基于计数的方法得到了单词的分布式表示。本章我们将讨论基于推理的方法。

基于推理的方法使用了推理机制，用的是神经网络。本章，著名的 word2vec 将会登场。我们将花很多时间考察 word2vec 的结构，并通过代码实现来加深对它的理解。

## 1. 基于推理的方法和神经网络

用向量表示单词的方法大致可以分为两种：一种是基于计数的方法；另一种是基于推理的方法。两者的背景都是分布式假设。

我们先指出基于计数的方法的问题，并从宏观角度说明它的替代方 法——基于推理的方法的优点。

### 1.1 基于计数的方法的问题

上一章所说的基于计数的方法根据一个单词周围的单词的出现频数来表示该单词。具体来说，先生成所有单词的共现矩阵，再对这个矩阵进行 SVD，以获得密集向量（单词的分布式表示）。但是，基于计数的方法在处理大规模语料库时会出现问题。如果词汇量超过 100 万个，那么使用基于计数的方法就需要生成一个 100 万 × 100 万的庞大矩阵，但对如此庞大的矩阵执行 SVD 显然是不现实的。

而基于推理的方法使用神经网络，通常在 mini-batch 数据上进行学习。这意味着神经网络一次只需要看一部分学习数据（mini-batch），并反复更新权重。这种学习机制的差异如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319223441482.png" alt="image-20220319223441482" style="zoom: 80%;" />

+ 基于计数的方法一次性处理全部学习数据；反之，基于推理的方法使用部分学习数据逐步学习。

基于推理的方法和基于计数的方法相比，还有一些其他的优点。我们之后说明。

### 1.2 基于推理的方法的概要

基于推理的方法的主要操作是“**推理**”，即当给出周围的单词（上下文）时，预测“？”处会出现什么单词：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319223701483.png" alt="image-20220319223701483" style="zoom:67%;" />

**解开上图中的推理问题并学习规律，就是基于推理的方法的主要任务。通过反复求解这些推理问题，可以学习到单词的出现模式**。

从模型的视角出发，这个推理问题如下图所示：

![image-20220319224128136](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319224128136.png)

+ 基于推理的方法：输入上下文，模型输出各个单词的出现概率

基于推理的方法引入了某种模型，我们将神经网络用于此模型。<u>这个模型接收上下文信息作为输入，并输出（可能出现的）各个单词的出现概率</u>。在这样的框架中，使用语料库来学习模型，使之能做出正确的预测。另外，**作为模型学习的产物，我们得到了单词的分布式表示**。这就是基于推理的方法的全貌。

### 1.3 神经网络中单词的处理方法

从现在开始，我们将使用神经网络来处理单词。但是，神经网络无法直接处理 you 或 say 这样的单词，<u>要用神经网络处理单词，需要先将单词转化为固定长度的向量</u>。

一种方式是是将单词转换为 **one-hot 表示**（只有一个元素是 1，其他元素都是 0）。

只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来，输入的神经元如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319224810111.png" alt="image-20220319224810111" style="zoom:67%;" />

+ 输入层由 7 个神经元表示，分别对应于 7 个单词。

现在事情变得很简单了。<u>因为只要将单词表示为向量，这些向量就可以由构成神经网络的各种“层”来处理</u>。比如，对于one-hot表示的某个单词，使用全连接层对其进行变换的情况如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319225029051.png" alt="image-20220319225029051" style="zoom:67%;" />

+ 全连接层通过箭头连接所有节点。这些箭头拥有权重 （参数），它们和输入层神经元的加权和成为中间层的神经元。

> 本章使用的全连接层将省略偏置（这是为了配合后文对 word2vec 的说明）。没有偏置的全连接层相当于在计算矩阵乘积，即 MatMul 层。

神经元之间的连接是用箭头表示的。之后，为了明确地显示权重，我们用如下图所示的表示方法：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319225245940.png" alt="image-20220319225245940" style="zoom:67%;" />

+ 将全连接层的权重表示为一个 7 × 3 形状的 $W$ 矩阵

现在这里的全连接层变换可以写成如下的 Python 代码：

```python
import numpy as np
c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 输入
W = np.random.randn(7, 3) # 权重
h = np.dot(c, W) # 中间节点
print(h)
# [[-0.70012195 0.25204755 -0.79774592]]
```

+ 这里的输入数据（变量c）的维数（ndim）是 2，这是考虑了 mini-batch 的处理。

但这里注意一下 c 与 W 进行矩阵乘积计算的地方（下图），其计算效果相当于“提取”权重的对应行向量：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319225548600.png" alt="image-20220319225548600" style="zoom:80%;" />

+ 这里仅为了提取权重的行向量而进行矩阵乘积计算好像不是很有效率。之后会对这一部分进行改进。

## 2. 简单的 word2vec

是时候实现 word2vec 了，我们要做的事情就是用神经网络完成“输入上下文，模型输出各个单词的出现概率”的任务。这里我们使用由原版 word2vec 提出的名为 continuous bag-of-words（**CBOW**）的模型作为神经网络。

> word2vec 一词最初用来指程序或者工具，但现在也指神经网络的模型。CBOW 模型和 skip-gram 模型是 word2vec 中使用的两个神经网络。

### 2.1 CBOW 模型的推理

// TODO