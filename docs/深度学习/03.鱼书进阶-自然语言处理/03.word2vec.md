---
title: word2vec
date: 2022-03-18 21:25:18
permalink: /pages/538482/
categories:
  - 深度学习
  - 鱼书进阶-自然语言处理
tags:
  - 
---

之前我们使用基于计数的方法得到了单词的分布式表示。本章我们将讨论基于推理的方法。

基于推理的方法使用了推理机制，用的是神经网络。本章我们将花很多时间考察 word2vec 的结构，并通过代码实现来加深对它的理解。

## 1. 基于推理的方法和神经网络

用向量表示单词的方法大致可以分为两种：一种是基于计数的方法；另一种是基于推理的方法。两者的背景都是分布式假设。

我们先指出基于计数的方法的问题，并从宏观角度说明它的替代方 法——基于推理的方法的优点。

### 1.1 基于计数的方法的问题

上一章所说的基于计数的方法根据一个单词周围的单词的出现频数来表示该单词。具体来说，先生成所有单词的共现矩阵，再对这个矩阵进行 SVD，以获得密集向量（单词的分布式表示）。但是，基于计数的方法在处理大规模语料库时会出现问题。如果词汇量超过 100 万个，那么使用基于计数的方法就需要生成一个 100 万 × 100 万的庞大矩阵，但对如此庞大的矩阵执行 SVD 显然是不现实的。

而基于推理的方法使用神经网络，通常在 mini-batch 数据上进行学习。这意味着神经网络一次只需要看一部分学习数据（mini-batch），并反复更新权重。这种学习机制的差异如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319223441482.png" alt="image-20220319223441482" style="zoom: 80%;" />

+ 基于计数的方法一次性处理全部学习数据；反之，基于推理的方法使用部分学习数据逐步学习。

基于推理的方法和基于计数的方法相比，还有一些其他的优点。我们之后说明。

### 1.2 基于推理的方法的概要

基于推理的方法的主要操作是“**推理**”，即当给出周围的单词（上下文）时，预测“？”处会出现什么单词：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319223701483.png" alt="image-20220319223701483" style="zoom:67%;" />

**解开上图中的推理问题并学习规律，就是基于推理的方法的主要任务。通过反复求解这些推理问题，可以学习到单词的出现模式**。

从模型的视角出发，这个推理问题如下图所示：

![image-20220319224128136](https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319224128136.png)

+ 基于推理的方法：输入上下文，模型输出各个单词的出现概率

基于推理的方法引入了某种模型，我们将神经网络用于此模型。<u>这个模型接收上下文信息作为输入，并输出（可能出现的）各个单词的出现概率</u>。在这样的框架中，使用语料库来学习模型，使之能做出正确的预测。另外，**作为模型学习的产物，我们得到了单词的分布式表示**。这就是基于推理的方法的全貌。

### 1.3 神经网络中单词的处理方法

从现在开始，我们将使用神经网络来处理单词。但是，神经网络无法直接处理 you 或 say 这样的单词，<u>要用神经网络处理单词，需要先将单词转化为固定长度的向量</u>。

一种方式是是将单词转换为 **one-hot 表示**（只有一个元素是 1，其他元素都是 0）。

只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来，输入的神经元如下：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319224810111.png" alt="image-20220319224810111" style="zoom:67%;" />

+ 输入层由 7 个神经元表示，分别对应于 7 个单词。

现在事情变得很简单了。<u>因为只要将单词表示为向量，这些向量就可以由构成神经网络的各种“层”来处理</u>。比如，对于one-hot表示的某个单词，使用全连接层对其进行变换的情况如下图所示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319225029051.png" alt="image-20220319225029051" style="zoom:67%;" />

+ 全连接层通过箭头连接所有节点。这些箭头拥有权重 （参数），它们和输入层神经元的加权和成为中间层的神经元。

> 本章使用的全连接层将省略偏置（这是为了配合后文对 word2vec 的说明）。没有偏置的全连接层相当于在计算矩阵乘积，即 MatMul 层。

神经元之间的连接是用箭头表示的。之后，为了明确地显示权重，我们用如下图所示的表示方法：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319225245940.png" alt="image-20220319225245940" style="zoom:67%;" />

+ 将全连接层的权重表示为一个 7 × 3 形状的 $W$ 矩阵

现在这里的全连接层变换可以写成如下的 Python 代码：

```python
import numpy as np
c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 输入
W = np.random.randn(7, 3) # 权重
h = np.dot(c, W) # 中间节点
print(h)
# [[-0.70012195 0.25204755 -0.79774592]]
```

+ 这里的输入数据（变量c）的维数（ndim）是 2，这是考虑了 mini-batch 的处理。

但这里注意一下 c 与 W 进行矩阵乘积计算的地方（下图），其计算效果相当于“提取”权重的对应行向量：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220319225548600.png" alt="image-20220319225548600" style="zoom:80%;" />

+ 这里仅为了提取权重的行向量而进行矩阵乘积计算好像不是很有效率。之后会对这一部分进行改进。

## 2. 简单的 word2vec

我们要做的事情就是用神经网络完成“输入上下文，模型输出各个单词的出现概率”的任务。这里我们使用由原版 word2vec 提出的名为 continuous bag-of-words（**CBOW**）的模型作为神经网络。

> word2vec 一词最初用来指程序或者工具，但现在也指神经网络的模型。CBOW 模型和 skip-gram 模型是 word2vec 中使用的两个神经网络。

### 2.1 CBOW 模型的推理

**CBOW 模型是根据上下文预测目标词的神经网络**（“目标词”是指中间的单词，它周围的单词是“上下文”）。通过训练这个 CBOW 模型，使其能尽可能地进行正确的预测，我们可以获得单词的分布式表示。

CBOW 模型的输入是上下文。这个上下文用 ['you', 'goodbye'] 这样的单词列表表示。我们将其转换为 one-hot 表示，以便 CBOW 模型可以进行处理。其模型的网络可画成下图：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320095759796.png" alt="image-20220320095759796" style="zoom:70%;" />

+ 这里，因为我们对上下文仅考虑两个单词，所以输入层有两个。如果对上下文考虑 N 个单词，则输入层会有 N 个
+ 有两个输入层，经过中间层到达输出层。两层之间的变化由全连接层完成。
+ 图中画的两个 $W_{in}$ 是同一个矩阵。

我们注意一下上图的中间层。此时，**中间层的神经元是各个输入层经全连接层变换后得到的值的“平均”**。就上面的例子而言，经全连接层变换后，第 1 个输入层转化为 $h_1$，第 2 个输入层转化为 $h_2$，那么中间层 的神经元是 $\frac{1}{2}(h_1 + h_2)$。

再看一下上图的输出层，这个输出层有 7 个神经元，**这些神经元对应于各个单词**。**输出层的神经元是各个单词的得分**，它的值越大，说明对应单词的出现概率就越高。得分是指在被解释为概率之前的值，对这些得分应用 Softmax 函数，就可以得到概率。

上图中从输入层到中间层的变换由全连接层（权重是 $W_{in}$）完 成。此时，全连接层的权重 $W_{in}$ 是一个 7 × 3 的矩阵。提前剧透一下，这个权重就是我们要的单词的分布式表示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320100409109.png" alt="image-20220320100409109" style="zoom:67%;" />

**权重 $W_{in}$ 的各行对应各个单词的分布式表示**，通过反复学习，不断更新各个单词的分布式表示，以正确地从上下文预测出应当 出现的单词。令人惊讶的是，如此获得的向量很好地对单词含义进行了编码。**这就是 word2vec 的全貌**。

> **中间层的神经元数量比输入层少这一点很重要**。中间层需要将预测单词所需的信息压缩保存，从而产生密集的向量表示。这时，中间层被写入了我们人类无法解读的代码，这相当于“编码”工作。而从中间层的信息获得期望结果的过程则称为“解码”。这一过程将被编码的信息复原为我们可以理解的形式。

我们再从层视角来展示一下 CBOW 模型：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320100704091.png" alt="image-20220320100704091" style="zoom:80%;" />

::: warning CBOW 模型总结

CBOW 模型一开始有两个 MatMul 层，这两个层的输出被加在一起。然后，对这个相加后得到的值乘以 0.5 求平均，可以得到中间层的神经元。最后，将另一个 MatMul 层应用于中间层的神经元，输出得分。

:::

下面我们来实现 CBOW 模型的推理（即求得分的过程）：

```python
import sys
sys.path.append('..')
import numpy as np
from common.layers import MatMul

# 样本的上下文数据
c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])
c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])

# 权重的初始值
W_in = np.random.randn(7, 3)
W_out = np.random.randn(3, 7)

# 生成层
in_layer0 = MatMul(W_in)
in_layer1 = MatMul(W_in)
out_layer = MatMul(W_out)

# 正向传播
h0 = in_layer0.forward(c0)
h1 = in_layer1.forward(c1)
h = 0.5 * (h0 + h1)       # 计算中间数据
s = out_layer.forward(h)  # 计算各个单词的得分
print(s)
# [[ 0.30916255 0.45060817 -0.77308656 0.22054131 0.15037278
# -0.93659277 -0.59612048]]
```

+ 注意 `in_layer0` 和 `in_layer1` 是共享的权重矩阵 $W_{in}$

### 2.2 CBOW 模型的学习

CBOW 模型在输出层输出了各个单词的得分。通过对这些得分应用 Softmax 函数，可以获得概率。**这个概率表示哪个单词会出现在给定的上下文（周围单词）中间**。

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320101404367.png" alt="image-20220320101404367" style="zoom:67%;" />

+ 在这个例子中，上下文是 you 和 goodbye，正确解标签（神经网络应该预测出的单词）是 say。**如果网络具有“良好的权重”，那么在表示概率的神经元中，对应正确解的神经元的得分应该更高**。

CBOW 模型的学习就是调整权重，以使预测准确。其结果是，权重 $W_{in}$（确切地说是 $W_{in}$ 和 $W_{out}$ 两者）学习到蕴含单词出现模式的向量。

> **CBOW模型只是学习语料库中单词的出现模式**。如果语料库不一样，学习到的单词的分布式表示也不一样。

现在，我们来考虑一下上述神经网络的学习。这里我们处理的模型是一个进行多类别分类的神经网络。因此，对其进行学习只是使用一下 Softmax 函数和交叉熵误差。**首先，使用 Softmax 函数将得分转化为概率，再求这些概率和监督标签之间的交叉熵误差，并将其作为损失进行学习**：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320101926896.png" alt="image-20220320101926896" style="zoom:80%;" />

向上一节介绍的进行推理的 CBOW 模型加上 Softmax 层和 Cross Entropy Error 层，就可以得到损失。这就是 CBOW 模型计算损失的流程，对应于神经网络的正向传播。

### 2.3 word2vec 的权重和分布式表示

word2vec 中使用的网络有两个权重，分别是输入侧的全连接层的权重（$W_{in}$）和输出侧的全连接层的权重（$W_{out}$）。

一般而言，**输入侧的权重 $W_{in}$ 的每一行对应于各个单词的分布式表示**。另外，输出侧的权重 $W_{out}$ 也同样保存了对单词含义进行了编码的向量，其权重在列方向上保存了各个单词的分布式表示：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320102335057.png" alt="image-20220320102335057" style="zoom:80%;" />

那么，我们最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项：A. 只使用输入侧的权重；B. 只使用输出侧的权重； C. 同时使用两个权重。方案 A 和方案 B 只使用其中一个权重。而在采用方案 C 的情况下，根据如何组合这两个权重，存在多种方式，其中一个方式就是简单地将这两个权重相加。

就 word2vec（特别是 skip-gram 模型）而言，最受欢迎的是只使用输入侧的权重。 **许多研究中也都仅使用输入侧的权重 $W_{in}$ 作为最终的单词的分布式表示**。 遵循这一思路，我们也使用 $W_{in}$ 作为单词的分布式表示。

> 有文献通过实验证明了 word2vec 的 skip-gram 模型中 $W_{in}$ 的有效性。另外，在与 word2vec 相似的 GloVe 方法中，通过将两个权重相加，也获得了良好的结果。

### 2.4 学习数据的准备

我们先来准备学习用的数据，仍以“You say goodbye and I say hello.”这个只有一句话的语料库为例进行说明。

#### 2.4.1 上下文和目标词

我们要做的事情是，当向神经网络输入上下文时，使目标词出现的概率高。先从语料库生成上下文和目标词：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320103928088.png" alt="image-20220320103928088" style="zoom:80%;" />

首先，将语料库的文本转化成单词 ID：

```python
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

print(corpus)
# [0 1 2 3 4 1 5 6]
print(id_to_word)
# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
```

然后，从单词 ID 列表 corpus 生成 contexts 和 target：

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320104114248.png" alt="image-20220320104114248" style="zoom:80%;" />

我们来实现这个生成上下文和目标词的函数：

```python
def create_contexts_target(corpus, window_size=1):
    '''生成上下文和目标词

    :param corpus: 语料库（单词ID列表）
    :param window_size: 窗口大小（当窗口大小为1时，左右各1个单词为上下文）
    :return: 上下文和目标词
    '''
    target = corpus[window_size:-window_size]
    contexts = []

    for idx in range(window_size, len(corpus)-window_size):
        cs = []
        for t in range(-window_size, window_size + 1):
            if t == 0:
                continue
            cs.append(corpus[idx + t])
        contexts.append(cs)

    return np.array(contexts), np.array(target)
```

使用以下这个函数：

```python
contexts, target = create_contexts_target(corpus, window_size=1)

print(contexts)
# [[0 2]
# [1 3]
# [2 4]
# [3 1]
# [4 5]
# [1 6]]
print(target)
# [1 2 3 4 1 5]
```

这样就从语料库生成了上下文和目标词。不过，因为这些上下文和目标词的元素还是单词 ID，所以<u>还需要将它们转化为 one-hot 表示</u>。

#### 2.4.2 转化为 one-hot 表示

<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220320104504164.png" alt="image-20220320104504164" style="zoom:80%;" />

+ 这里需要注意各个多维数组的形状。在上面的例子中，使用单词 ID 时的 contexts 的形状是 (6,2)，将其转化为 one-hot 表示后，形状变为 (6,2,7)。

我们使用 `convert_one_hot()` 函数以将单词 ID 转化为 one-hot 表示，其实现不再说明：

```python
vocab_size = len(word_to_id)
target = convert_one_hot(target, vocab_size)
contexts = convert_one_hot(contexts, vocab_size)
```

至此，学习数据的准备就完成了，

### 2.5 CBOW 模型的实现

// TODO