<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>神经网络训练不起来怎么办 | notebook</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/notebook/img/favicon.ico">
    <script data-ad-client="ca-pub-7828333725993554" async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="学习笔记">
    <meta name="keywords" content="全栈学习笔记">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/notebook/assets/css/0.styles.1b58b254.css" as="style"><link rel="preload" href="/notebook/assets/js/app.2bf3b6c1.js" as="script"><link rel="preload" href="/notebook/assets/js/2.0ad58009.js" as="script"><link rel="preload" href="/notebook/assets/js/77.40db9cc6.js" as="script"><link rel="prefetch" href="/notebook/assets/js/10.99522837.js"><link rel="prefetch" href="/notebook/assets/js/100.e3b56889.js"><link rel="prefetch" href="/notebook/assets/js/101.6ea1d00b.js"><link rel="prefetch" href="/notebook/assets/js/102.eca9dfbd.js"><link rel="prefetch" href="/notebook/assets/js/103.6ea477d4.js"><link rel="prefetch" href="/notebook/assets/js/104.ac820d2b.js"><link rel="prefetch" href="/notebook/assets/js/105.58b259a8.js"><link rel="prefetch" href="/notebook/assets/js/106.a86005d0.js"><link rel="prefetch" href="/notebook/assets/js/107.7a79d36f.js"><link rel="prefetch" href="/notebook/assets/js/108.64404e25.js"><link rel="prefetch" href="/notebook/assets/js/109.75f12c0a.js"><link rel="prefetch" href="/notebook/assets/js/11.d26d59e4.js"><link rel="prefetch" href="/notebook/assets/js/110.1155fe36.js"><link rel="prefetch" href="/notebook/assets/js/111.bf8b5871.js"><link rel="prefetch" href="/notebook/assets/js/112.22833ceb.js"><link rel="prefetch" href="/notebook/assets/js/113.6a080233.js"><link rel="prefetch" href="/notebook/assets/js/114.35de9701.js"><link rel="prefetch" href="/notebook/assets/js/115.f598d8c2.js"><link rel="prefetch" href="/notebook/assets/js/116.e3bd29ce.js"><link rel="prefetch" href="/notebook/assets/js/117.c3c02abc.js"><link rel="prefetch" href="/notebook/assets/js/118.136a552a.js"><link rel="prefetch" href="/notebook/assets/js/119.c124f3f8.js"><link rel="prefetch" href="/notebook/assets/js/12.dc66c4f2.js"><link rel="prefetch" href="/notebook/assets/js/120.f835d124.js"><link rel="prefetch" href="/notebook/assets/js/121.367716ae.js"><link rel="prefetch" href="/notebook/assets/js/122.752b0493.js"><link rel="prefetch" href="/notebook/assets/js/123.9f8d6026.js"><link rel="prefetch" href="/notebook/assets/js/124.e8eb61b6.js"><link rel="prefetch" href="/notebook/assets/js/125.cb081200.js"><link rel="prefetch" href="/notebook/assets/js/126.ab87d911.js"><link rel="prefetch" href="/notebook/assets/js/127.ffdbe74d.js"><link rel="prefetch" href="/notebook/assets/js/128.ec526e42.js"><link rel="prefetch" href="/notebook/assets/js/129.71839012.js"><link rel="prefetch" href="/notebook/assets/js/13.32e95b42.js"><link rel="prefetch" href="/notebook/assets/js/130.2bc0bb4d.js"><link rel="prefetch" href="/notebook/assets/js/131.5595b49b.js"><link rel="prefetch" href="/notebook/assets/js/132.4963c5c4.js"><link rel="prefetch" href="/notebook/assets/js/133.44f48cfd.js"><link rel="prefetch" href="/notebook/assets/js/134.cf25626c.js"><link rel="prefetch" href="/notebook/assets/js/135.5ee30fa9.js"><link rel="prefetch" href="/notebook/assets/js/136.bc43f8e6.js"><link rel="prefetch" href="/notebook/assets/js/137.9ab5beac.js"><link rel="prefetch" href="/notebook/assets/js/138.692a33e6.js"><link rel="prefetch" href="/notebook/assets/js/139.08e7c98d.js"><link rel="prefetch" href="/notebook/assets/js/14.c418d170.js"><link rel="prefetch" href="/notebook/assets/js/140.39a861db.js"><link rel="prefetch" href="/notebook/assets/js/141.46678413.js"><link rel="prefetch" href="/notebook/assets/js/142.f7ef5eac.js"><link rel="prefetch" href="/notebook/assets/js/143.c92cbac1.js"><link rel="prefetch" href="/notebook/assets/js/144.d9c61437.js"><link rel="prefetch" href="/notebook/assets/js/145.9f603b31.js"><link rel="prefetch" href="/notebook/assets/js/146.b875f045.js"><link rel="prefetch" href="/notebook/assets/js/147.55e7c4f8.js"><link rel="prefetch" href="/notebook/assets/js/148.4410c365.js"><link rel="prefetch" href="/notebook/assets/js/149.6096ed98.js"><link rel="prefetch" href="/notebook/assets/js/15.e0e7392a.js"><link rel="prefetch" href="/notebook/assets/js/150.24451f07.js"><link rel="prefetch" href="/notebook/assets/js/151.7cff301c.js"><link rel="prefetch" href="/notebook/assets/js/152.035fee1f.js"><link rel="prefetch" href="/notebook/assets/js/153.c61f8ec3.js"><link rel="prefetch" href="/notebook/assets/js/154.7bb549d0.js"><link rel="prefetch" href="/notebook/assets/js/155.1dc494db.js"><link rel="prefetch" href="/notebook/assets/js/156.b87eaf39.js"><link rel="prefetch" href="/notebook/assets/js/157.e3f5a5c0.js"><link rel="prefetch" href="/notebook/assets/js/158.c565c699.js"><link rel="prefetch" href="/notebook/assets/js/159.a22609ef.js"><link rel="prefetch" href="/notebook/assets/js/16.d1aef4ee.js"><link rel="prefetch" href="/notebook/assets/js/160.b29e761c.js"><link rel="prefetch" href="/notebook/assets/js/161.bee1e522.js"><link rel="prefetch" href="/notebook/assets/js/162.c49fca62.js"><link rel="prefetch" href="/notebook/assets/js/163.2cb4d37d.js"><link rel="prefetch" href="/notebook/assets/js/164.4a0dbc64.js"><link rel="prefetch" href="/notebook/assets/js/165.490d05b3.js"><link rel="prefetch" href="/notebook/assets/js/166.df5d2527.js"><link rel="prefetch" href="/notebook/assets/js/167.89a81814.js"><link rel="prefetch" href="/notebook/assets/js/168.9991702e.js"><link rel="prefetch" href="/notebook/assets/js/169.2f9a5dce.js"><link rel="prefetch" href="/notebook/assets/js/17.88ae5445.js"><link rel="prefetch" href="/notebook/assets/js/170.5f23eb3c.js"><link rel="prefetch" href="/notebook/assets/js/171.c521aaa8.js"><link rel="prefetch" href="/notebook/assets/js/172.42110b0a.js"><link rel="prefetch" href="/notebook/assets/js/173.5e36f1bf.js"><link rel="prefetch" href="/notebook/assets/js/174.f48e078a.js"><link rel="prefetch" href="/notebook/assets/js/175.775da6a5.js"><link rel="prefetch" href="/notebook/assets/js/176.9c3c55ea.js"><link rel="prefetch" href="/notebook/assets/js/177.b54d1cff.js"><link rel="prefetch" href="/notebook/assets/js/178.ff08b7f5.js"><link rel="prefetch" href="/notebook/assets/js/179.c6a1af32.js"><link rel="prefetch" href="/notebook/assets/js/18.dcb78196.js"><link rel="prefetch" href="/notebook/assets/js/180.25dd9eba.js"><link rel="prefetch" href="/notebook/assets/js/181.13e6ec84.js"><link rel="prefetch" href="/notebook/assets/js/182.f6849f0d.js"><link rel="prefetch" href="/notebook/assets/js/183.7e664874.js"><link rel="prefetch" href="/notebook/assets/js/184.e6aba86f.js"><link rel="prefetch" href="/notebook/assets/js/185.df07b919.js"><link rel="prefetch" href="/notebook/assets/js/186.02c77e75.js"><link rel="prefetch" href="/notebook/assets/js/187.e8380ed4.js"><link rel="prefetch" href="/notebook/assets/js/188.eddc8bee.js"><link rel="prefetch" href="/notebook/assets/js/189.fbc1840f.js"><link rel="prefetch" href="/notebook/assets/js/19.5997a514.js"><link rel="prefetch" href="/notebook/assets/js/190.a37bfe4c.js"><link rel="prefetch" href="/notebook/assets/js/191.e53a3d4b.js"><link rel="prefetch" href="/notebook/assets/js/192.2f5be408.js"><link rel="prefetch" href="/notebook/assets/js/193.4ca6de49.js"><link rel="prefetch" href="/notebook/assets/js/194.b8e51d9d.js"><link rel="prefetch" href="/notebook/assets/js/195.70e6b23a.js"><link rel="prefetch" href="/notebook/assets/js/196.5d5fbf2d.js"><link rel="prefetch" href="/notebook/assets/js/197.78456dab.js"><link rel="prefetch" href="/notebook/assets/js/198.4308331c.js"><link rel="prefetch" href="/notebook/assets/js/199.2e537849.js"><link rel="prefetch" href="/notebook/assets/js/20.fc057fd7.js"><link rel="prefetch" href="/notebook/assets/js/200.b3309bbf.js"><link rel="prefetch" href="/notebook/assets/js/201.4723461c.js"><link rel="prefetch" href="/notebook/assets/js/202.b15b5177.js"><link rel="prefetch" href="/notebook/assets/js/203.22c50e61.js"><link rel="prefetch" href="/notebook/assets/js/204.5b8b3b00.js"><link rel="prefetch" href="/notebook/assets/js/205.54ee7630.js"><link rel="prefetch" href="/notebook/assets/js/206.f3f20f94.js"><link rel="prefetch" href="/notebook/assets/js/207.a9608973.js"><link rel="prefetch" href="/notebook/assets/js/208.1a80a593.js"><link rel="prefetch" href="/notebook/assets/js/209.586fd293.js"><link rel="prefetch" href="/notebook/assets/js/21.cb4205ee.js"><link rel="prefetch" href="/notebook/assets/js/210.7829dd53.js"><link rel="prefetch" href="/notebook/assets/js/211.3ce139ab.js"><link rel="prefetch" href="/notebook/assets/js/212.84738a64.js"><link rel="prefetch" href="/notebook/assets/js/213.a631830d.js"><link rel="prefetch" href="/notebook/assets/js/214.9d64cf85.js"><link rel="prefetch" href="/notebook/assets/js/215.87030b6b.js"><link rel="prefetch" href="/notebook/assets/js/216.ddbe1944.js"><link rel="prefetch" href="/notebook/assets/js/217.16ae7e40.js"><link rel="prefetch" href="/notebook/assets/js/218.e7780d65.js"><link rel="prefetch" href="/notebook/assets/js/219.abae5e09.js"><link rel="prefetch" href="/notebook/assets/js/22.256014b4.js"><link rel="prefetch" href="/notebook/assets/js/220.8e3a8702.js"><link rel="prefetch" href="/notebook/assets/js/221.4c279d74.js"><link rel="prefetch" href="/notebook/assets/js/222.6c9b2595.js"><link rel="prefetch" href="/notebook/assets/js/223.cc072424.js"><link rel="prefetch" href="/notebook/assets/js/224.c663b40f.js"><link rel="prefetch" href="/notebook/assets/js/225.f3e52654.js"><link rel="prefetch" href="/notebook/assets/js/226.5e00402c.js"><link rel="prefetch" href="/notebook/assets/js/227.1c28ce97.js"><link rel="prefetch" href="/notebook/assets/js/228.42b8c305.js"><link rel="prefetch" href="/notebook/assets/js/229.df9760ec.js"><link rel="prefetch" href="/notebook/assets/js/23.a3d7d66a.js"><link rel="prefetch" href="/notebook/assets/js/230.cfe18f05.js"><link rel="prefetch" href="/notebook/assets/js/231.3a664a46.js"><link rel="prefetch" href="/notebook/assets/js/232.966ce9dc.js"><link rel="prefetch" href="/notebook/assets/js/233.fc06cb57.js"><link rel="prefetch" href="/notebook/assets/js/234.7bb9b7d4.js"><link rel="prefetch" href="/notebook/assets/js/235.b336116e.js"><link rel="prefetch" href="/notebook/assets/js/236.03a38f77.js"><link rel="prefetch" href="/notebook/assets/js/237.0dbda856.js"><link rel="prefetch" href="/notebook/assets/js/238.c1c19749.js"><link rel="prefetch" href="/notebook/assets/js/239.046875c1.js"><link rel="prefetch" href="/notebook/assets/js/24.8e5e267e.js"><link rel="prefetch" href="/notebook/assets/js/240.4bd9cdc0.js"><link rel="prefetch" href="/notebook/assets/js/241.c3dc5804.js"><link rel="prefetch" href="/notebook/assets/js/242.db0b1a91.js"><link rel="prefetch" href="/notebook/assets/js/243.4d9bd61d.js"><link rel="prefetch" href="/notebook/assets/js/244.ee57770b.js"><link rel="prefetch" href="/notebook/assets/js/245.02aab1c1.js"><link rel="prefetch" href="/notebook/assets/js/246.b76a18bb.js"><link rel="prefetch" href="/notebook/assets/js/247.75a673db.js"><link rel="prefetch" href="/notebook/assets/js/248.ad93f81d.js"><link rel="prefetch" href="/notebook/assets/js/249.fb75a938.js"><link rel="prefetch" href="/notebook/assets/js/25.b12f24fe.js"><link rel="prefetch" href="/notebook/assets/js/250.8395c0b6.js"><link rel="prefetch" href="/notebook/assets/js/251.16a6d2a4.js"><link rel="prefetch" href="/notebook/assets/js/252.ef3ee05e.js"><link rel="prefetch" href="/notebook/assets/js/253.78e3471e.js"><link rel="prefetch" href="/notebook/assets/js/254.a5783e07.js"><link rel="prefetch" href="/notebook/assets/js/255.2ab853f6.js"><link rel="prefetch" href="/notebook/assets/js/256.5430831b.js"><link rel="prefetch" href="/notebook/assets/js/257.99c8a0a4.js"><link rel="prefetch" href="/notebook/assets/js/258.4496955b.js"><link rel="prefetch" href="/notebook/assets/js/259.9152b1d2.js"><link rel="prefetch" href="/notebook/assets/js/26.0fce5172.js"><link rel="prefetch" href="/notebook/assets/js/260.072f65e6.js"><link rel="prefetch" href="/notebook/assets/js/261.0bca81af.js"><link rel="prefetch" href="/notebook/assets/js/262.9c9c5337.js"><link rel="prefetch" href="/notebook/assets/js/263.42470957.js"><link rel="prefetch" href="/notebook/assets/js/264.64b5f4fb.js"><link rel="prefetch" href="/notebook/assets/js/265.836a69c5.js"><link rel="prefetch" href="/notebook/assets/js/266.a00cdeb1.js"><link rel="prefetch" href="/notebook/assets/js/267.09dc5ae4.js"><link rel="prefetch" href="/notebook/assets/js/268.6fa6603e.js"><link rel="prefetch" href="/notebook/assets/js/269.3963ce5e.js"><link rel="prefetch" href="/notebook/assets/js/27.47ba3886.js"><link rel="prefetch" href="/notebook/assets/js/270.2826382d.js"><link rel="prefetch" href="/notebook/assets/js/271.3c746c23.js"><link rel="prefetch" href="/notebook/assets/js/272.30698dda.js"><link rel="prefetch" href="/notebook/assets/js/273.b06e3fd2.js"><link rel="prefetch" href="/notebook/assets/js/274.2016c7fa.js"><link rel="prefetch" href="/notebook/assets/js/275.f4aff624.js"><link rel="prefetch" href="/notebook/assets/js/276.e682aa74.js"><link rel="prefetch" href="/notebook/assets/js/277.0c3f41db.js"><link rel="prefetch" href="/notebook/assets/js/278.3c2d5251.js"><link rel="prefetch" href="/notebook/assets/js/279.a9af5703.js"><link rel="prefetch" href="/notebook/assets/js/28.6bac56c6.js"><link rel="prefetch" href="/notebook/assets/js/280.a5da28a3.js"><link rel="prefetch" href="/notebook/assets/js/281.8cc5a3ba.js"><link rel="prefetch" href="/notebook/assets/js/282.55227ff2.js"><link rel="prefetch" href="/notebook/assets/js/283.13f54ae9.js"><link rel="prefetch" href="/notebook/assets/js/284.88644dec.js"><link rel="prefetch" href="/notebook/assets/js/285.0670211f.js"><link rel="prefetch" href="/notebook/assets/js/286.afa43d34.js"><link rel="prefetch" href="/notebook/assets/js/287.9e98e933.js"><link rel="prefetch" href="/notebook/assets/js/288.175a8a9b.js"><link rel="prefetch" href="/notebook/assets/js/289.0d712953.js"><link rel="prefetch" href="/notebook/assets/js/29.3476ca1f.js"><link rel="prefetch" href="/notebook/assets/js/290.4b258761.js"><link rel="prefetch" href="/notebook/assets/js/291.e7ded33e.js"><link rel="prefetch" href="/notebook/assets/js/292.fcfca63e.js"><link rel="prefetch" href="/notebook/assets/js/293.4d6c0f7d.js"><link rel="prefetch" href="/notebook/assets/js/294.59b7e2de.js"><link rel="prefetch" href="/notebook/assets/js/295.0b8dc8f3.js"><link rel="prefetch" href="/notebook/assets/js/296.65434eb0.js"><link rel="prefetch" href="/notebook/assets/js/297.957ba4a7.js"><link rel="prefetch" href="/notebook/assets/js/298.dd81e487.js"><link rel="prefetch" href="/notebook/assets/js/299.eba0d36a.js"><link rel="prefetch" href="/notebook/assets/js/3.a80649d1.js"><link rel="prefetch" href="/notebook/assets/js/30.51a26022.js"><link rel="prefetch" href="/notebook/assets/js/300.23a6a024.js"><link rel="prefetch" href="/notebook/assets/js/301.eb4276c9.js"><link rel="prefetch" href="/notebook/assets/js/302.2c696c44.js"><link rel="prefetch" href="/notebook/assets/js/303.a748a576.js"><link rel="prefetch" href="/notebook/assets/js/304.95020a99.js"><link rel="prefetch" href="/notebook/assets/js/305.c4bc6072.js"><link rel="prefetch" href="/notebook/assets/js/306.74133b05.js"><link rel="prefetch" href="/notebook/assets/js/307.6ea724f3.js"><link rel="prefetch" href="/notebook/assets/js/308.fc7b065c.js"><link rel="prefetch" href="/notebook/assets/js/309.56497801.js"><link rel="prefetch" href="/notebook/assets/js/31.c351e10d.js"><link rel="prefetch" href="/notebook/assets/js/310.692379f9.js"><link rel="prefetch" href="/notebook/assets/js/311.b7393f95.js"><link rel="prefetch" href="/notebook/assets/js/312.f3eec1e1.js"><link rel="prefetch" href="/notebook/assets/js/313.9227351c.js"><link rel="prefetch" href="/notebook/assets/js/314.6960877d.js"><link rel="prefetch" href="/notebook/assets/js/315.f55a1979.js"><link rel="prefetch" href="/notebook/assets/js/316.6121039c.js"><link rel="prefetch" href="/notebook/assets/js/317.7ba118c8.js"><link rel="prefetch" href="/notebook/assets/js/318.2b71444c.js"><link rel="prefetch" href="/notebook/assets/js/319.bc0d5ccf.js"><link rel="prefetch" href="/notebook/assets/js/32.8a802a22.js"><link rel="prefetch" href="/notebook/assets/js/320.79f13ae1.js"><link rel="prefetch" href="/notebook/assets/js/321.21d3b0cf.js"><link rel="prefetch" href="/notebook/assets/js/322.87e4c143.js"><link rel="prefetch" href="/notebook/assets/js/323.4dee2eb7.js"><link rel="prefetch" href="/notebook/assets/js/324.f8c64322.js"><link rel="prefetch" href="/notebook/assets/js/325.c82057d6.js"><link rel="prefetch" href="/notebook/assets/js/326.3ea0d22b.js"><link rel="prefetch" href="/notebook/assets/js/327.90b878d9.js"><link rel="prefetch" href="/notebook/assets/js/328.59e55f0a.js"><link rel="prefetch" href="/notebook/assets/js/329.95fb2ef0.js"><link rel="prefetch" href="/notebook/assets/js/33.18cd7b09.js"><link rel="prefetch" href="/notebook/assets/js/330.ed1fb0e9.js"><link rel="prefetch" href="/notebook/assets/js/331.b84d88a9.js"><link rel="prefetch" href="/notebook/assets/js/332.20dffd14.js"><link rel="prefetch" href="/notebook/assets/js/333.d625fbd2.js"><link rel="prefetch" href="/notebook/assets/js/334.4fedc08a.js"><link rel="prefetch" href="/notebook/assets/js/335.c3b6c886.js"><link rel="prefetch" href="/notebook/assets/js/336.cf000555.js"><link rel="prefetch" href="/notebook/assets/js/337.891a7e6c.js"><link rel="prefetch" href="/notebook/assets/js/338.23da071e.js"><link rel="prefetch" href="/notebook/assets/js/339.92d07729.js"><link rel="prefetch" href="/notebook/assets/js/34.f39f39b2.js"><link rel="prefetch" href="/notebook/assets/js/340.09cb4417.js"><link rel="prefetch" href="/notebook/assets/js/341.3591e649.js"><link rel="prefetch" href="/notebook/assets/js/342.568a9320.js"><link rel="prefetch" href="/notebook/assets/js/343.e61b523f.js"><link rel="prefetch" href="/notebook/assets/js/344.61bb135b.js"><link rel="prefetch" href="/notebook/assets/js/345.f861e5aa.js"><link rel="prefetch" href="/notebook/assets/js/346.c5c70e0f.js"><link rel="prefetch" href="/notebook/assets/js/347.9b389847.js"><link rel="prefetch" href="/notebook/assets/js/348.eb62b86e.js"><link rel="prefetch" href="/notebook/assets/js/349.d4852195.js"><link rel="prefetch" href="/notebook/assets/js/35.c31fd7ed.js"><link rel="prefetch" href="/notebook/assets/js/350.f1db6bfd.js"><link rel="prefetch" href="/notebook/assets/js/351.4d86adaf.js"><link rel="prefetch" href="/notebook/assets/js/36.624192b1.js"><link rel="prefetch" href="/notebook/assets/js/37.680f8e12.js"><link rel="prefetch" href="/notebook/assets/js/38.f9ecec66.js"><link rel="prefetch" href="/notebook/assets/js/39.afab4ce6.js"><link rel="prefetch" href="/notebook/assets/js/4.03ba6111.js"><link rel="prefetch" href="/notebook/assets/js/40.f66ecac0.js"><link rel="prefetch" href="/notebook/assets/js/41.87cdca0e.js"><link rel="prefetch" href="/notebook/assets/js/42.08461558.js"><link rel="prefetch" href="/notebook/assets/js/43.ad5cf182.js"><link rel="prefetch" href="/notebook/assets/js/44.0bb6ad3f.js"><link rel="prefetch" href="/notebook/assets/js/45.5d2af6d4.js"><link rel="prefetch" href="/notebook/assets/js/46.8a06257e.js"><link rel="prefetch" href="/notebook/assets/js/47.3e37541c.js"><link rel="prefetch" href="/notebook/assets/js/48.024eda4c.js"><link rel="prefetch" href="/notebook/assets/js/49.a0685cf7.js"><link rel="prefetch" href="/notebook/assets/js/5.1071c8dd.js"><link rel="prefetch" href="/notebook/assets/js/50.130eaac4.js"><link rel="prefetch" href="/notebook/assets/js/51.0fe4dbd0.js"><link rel="prefetch" href="/notebook/assets/js/52.9d0ae64a.js"><link rel="prefetch" href="/notebook/assets/js/53.1ca09933.js"><link rel="prefetch" href="/notebook/assets/js/54.679cd78c.js"><link rel="prefetch" href="/notebook/assets/js/55.95cbe3a2.js"><link rel="prefetch" href="/notebook/assets/js/56.a58ec2af.js"><link rel="prefetch" href="/notebook/assets/js/57.0e59339a.js"><link rel="prefetch" href="/notebook/assets/js/58.487f643f.js"><link rel="prefetch" href="/notebook/assets/js/59.a8e9a1e3.js"><link rel="prefetch" href="/notebook/assets/js/6.707a1f11.js"><link rel="prefetch" href="/notebook/assets/js/60.c3080f7a.js"><link rel="prefetch" href="/notebook/assets/js/61.7f77e449.js"><link rel="prefetch" href="/notebook/assets/js/62.a5528e33.js"><link rel="prefetch" href="/notebook/assets/js/63.a787a8ee.js"><link rel="prefetch" href="/notebook/assets/js/64.7d3edfda.js"><link rel="prefetch" href="/notebook/assets/js/65.80e083e6.js"><link rel="prefetch" href="/notebook/assets/js/66.4076f29c.js"><link rel="prefetch" href="/notebook/assets/js/67.cf46f254.js"><link rel="prefetch" href="/notebook/assets/js/68.6fc8b1fd.js"><link rel="prefetch" href="/notebook/assets/js/69.4a344d72.js"><link rel="prefetch" href="/notebook/assets/js/7.c507c0e3.js"><link rel="prefetch" href="/notebook/assets/js/70.b13eef1a.js"><link rel="prefetch" href="/notebook/assets/js/71.20ad9776.js"><link rel="prefetch" href="/notebook/assets/js/72.30f44ef6.js"><link rel="prefetch" href="/notebook/assets/js/73.857a629d.js"><link rel="prefetch" href="/notebook/assets/js/74.a2b5a703.js"><link rel="prefetch" href="/notebook/assets/js/75.252e6fc0.js"><link rel="prefetch" href="/notebook/assets/js/76.d64e4a53.js"><link rel="prefetch" href="/notebook/assets/js/78.7a635d12.js"><link rel="prefetch" href="/notebook/assets/js/79.b2249421.js"><link rel="prefetch" href="/notebook/assets/js/8.a5f34392.js"><link rel="prefetch" href="/notebook/assets/js/80.d325f684.js"><link rel="prefetch" href="/notebook/assets/js/81.2e8d667e.js"><link rel="prefetch" href="/notebook/assets/js/82.885af8d1.js"><link rel="prefetch" href="/notebook/assets/js/83.b601bf2e.js"><link rel="prefetch" href="/notebook/assets/js/84.758d5dba.js"><link rel="prefetch" href="/notebook/assets/js/85.2e75fb85.js"><link rel="prefetch" href="/notebook/assets/js/86.6c68d815.js"><link rel="prefetch" href="/notebook/assets/js/87.8fba1553.js"><link rel="prefetch" href="/notebook/assets/js/88.e30608d9.js"><link rel="prefetch" href="/notebook/assets/js/89.be2f87c8.js"><link rel="prefetch" href="/notebook/assets/js/9.1c775f56.js"><link rel="prefetch" href="/notebook/assets/js/90.88dd69c4.js"><link rel="prefetch" href="/notebook/assets/js/91.59a69041.js"><link rel="prefetch" href="/notebook/assets/js/92.b46ca339.js"><link rel="prefetch" href="/notebook/assets/js/93.aeaec51d.js"><link rel="prefetch" href="/notebook/assets/js/94.5a852633.js"><link rel="prefetch" href="/notebook/assets/js/95.4f445663.js"><link rel="prefetch" href="/notebook/assets/js/96.6299f802.js"><link rel="prefetch" href="/notebook/assets/js/97.6cf3ba23.js"><link rel="prefetch" href="/notebook/assets/js/98.b48d73e6.js"><link rel="prefetch" href="/notebook/assets/js/99.f61a2e23.js">
    <link rel="stylesheet" href="/notebook/assets/css/0.styles.1b58b254.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/notebook/" class="home-link router-link-active"><img src="/notebook/img/logo.png" alt="notebook" class="logo"> <span class="site-name can-hide">notebook</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/head.jpg"> <div class="blogger-info"><h3>学习笔记</h3> <span>啦啦啦，向太阳~</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Posts</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>PyTorch 入门</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>鱼书进阶-自然语言处理</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>深度学习-李宏毅</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/notebook/pages/lhy/regression/" class="sidebar-link">Regression</a></li><li><a href="/notebook/pages/lhy/training-tricks/" aria-current="page" class="active sidebar-link">神经网络训练不起来怎么办</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/training-tricks/#_1-general-guidance" class="sidebar-link">1. General Guidance</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_1-1-model-bias" class="sidebar-link">1.1 model bias</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_1-2-optimization-issue" class="sidebar-link">1.2 optimization issue</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_1-3-overfitting" class="sidebar-link">1.3 overfitting</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/training-tricks/#_2-local-minima-与-saddle-point" class="sidebar-link">2. local minima 与 saddle point</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_2-1-critical-point" class="sidebar-link">2.1 Critical Point</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_2-2-判断是-local-minima-还是-saddle-point" class="sidebar-link">2.2 判断是 local minima 还是 saddle point</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_2-3-saddle-point-v-s-local-minima" class="sidebar-link">2.3  Saddle Point v.s. Local Minima</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/training-tricks/#_3-batch" class="sidebar-link">3. Batch</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_3-1-larger-batch-可能花费时间更少" class="sidebar-link">3.1 larger batch 可能花费时间更少</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_3-2-small-batch-训练得到的精确度可能更好" class="sidebar-link">3.2 small batch 训练得到的精确度可能更好</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_3-3-small-batch-更容易泛化" class="sidebar-link">3.3 small batch 更容易泛化</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/training-tricks/#_4-momentum" class="sidebar-link">4. Momentum</a></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/training-tricks/#_5-adaptive-learning-rate" class="sidebar-link">5. Adaptive Learning Rate</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_5-1-training-stuck-small-gradient" class="sidebar-link">5.1 training stuck ≠ small gradient</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_5-2-different-parameters-needs-different-learning-rate" class="sidebar-link">5.2 Different parameters needs different learning rate</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_5-3-root-mean-square-与-adagrad" class="sidebar-link">5.3 Root mean square 与 Adagrad</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_5-4-rmsprop" class="sidebar-link">5.4 RMSProp</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_5-5-adam" class="sidebar-link">5.5 Adam</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_5-6-learning-rate-scheduling" class="sidebar-link">5.6 Learning Rate Scheduling</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/training-tricks/#_6-batch-normalization" class="sidebar-link">6. Batch Normalization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_6-1-introduction" class="sidebar-link">6.1 Introduction</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_6-2-feature-normalization" class="sidebar-link">6.2 Feature Normalization</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_6-3-considering-deep-learning" class="sidebar-link">6.3 Considering Deep Learning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/training-tricks/#_6-4-comparison" class="sidebar-link">6.4 Comparison</a></li></ul></li></ul></li><li><a href="/notebook/pages/lhy/cnn/" class="sidebar-link">CNN</a></li><li><a href="/notebook/pages/lhy/self-attention/" class="sidebar-link">Self-Attention</a></li><li><a href="/notebook/pages/lhy/various-attention/" class="sidebar-link">各式各样的 Attention</a></li><li><a href="/notebook/pages/lhy/pointer-network/" class="sidebar-link">Pointer Network</a></li><li><a href="/notebook/pages/lhy/gnn/" class="sidebar-link">图神经网络</a></li><li><a href="/notebook/pages/lhy/transformer/" class="sidebar-link">Transformer</a></li><li><a href="/notebook/pages/lhy/gan/" class="sidebar-link">生成对抗网络 GAN</a></li><li><a href="/notebook/pages/lhy/self-supervised-learning/" class="sidebar-link">Self Supervised Learning</a></li><li><a href="/notebook/pages/lhy/bert-and-family/" class="sidebar-link">BERT and its family</a></li><li><a href="/notebook/pages/lhy/data-efficient/" class="sidebar-link">Data Efficient &amp; Parameter-Efficient Tuning</a></li><li><a href="/notebook/pages/lhy/auto-encoder/" class="sidebar-link">Auto-Encoder</a></li><li><a href="/notebook/pages/lhy/explainable-ml/" class="sidebar-link">机器学习的可解释性</a></li><li><a href="/notebook/pages/lhy/adversarial-attack/" class="sidebar-link">Adversarial Attack</a></li><li><a href="/notebook/pages/lhy/domain-adaptation/" class="sidebar-link">Domain Adaptation</a></li><li><a href="/notebook/pages/lhy/RL/" class="sidebar-link">强化学习</a></li><li><a href="/notebook/pages/lhy/network-compression/" class="sidebar-link">神经网络压缩</a></li><li><a href="/notebook/pages/lhy/life-long-learning/" class="sidebar-link">Life Long Learning</a></li><li><a href="/notebook/pages/lhy/meta-learning/" class="sidebar-link">Meta Learning</a></li><li><a href="/notebook/pages/595df8/" class="sidebar-link">ChatGPT 是怎样炼成的</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2017版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2019版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>预训练语言模型-邵浩2021版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>王树森</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>机器学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>知识图谱</span> <span class="arrow right"></span></p> <!----></section></li></ul> <div class="sidebar-slot sidebar-slot-bottom"><!-- 正方形 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="3508773082"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div></aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/notebook/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/notebook/categories/?category=AI" title="分类" data-v-06225672>AI</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="分类" data-v-06225672>深度学习</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85" title="分类" data-v-06225672>深度学习-李宏毅</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/yubincloud" target="_blank" title="作者" class="beLink" data-v-06225672>yubin</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2022-04-03</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">神经网络训练不起来怎么办<!----></h1> <div class="page-slot page-slot-top"><!-- 固定100% * 90px可显示，max-height:90px未见显示-->
     <ins class="adsbygoogle"
          style="display:inline-block;width:100%;max-height:90px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6625304284"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="theme-vdoing-content content__default"><h2 id="_1-general-guidance"><a href="#_1-general-guidance" class="header-anchor">#</a> 1. General Guidance</h2> <p>训练模型的过程中，以下就是如何让你做得更好的攻略：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403195635625.png" alt="image-20220403195635625" style="zoom:67%;"> <p>当对训练结果不满意时（testing data 的 loss 太大），首先应检查你的 training data，看看你的 model 有没有在 training data 上学起来，再去看 testing 的结果。<strong>如果你发现你的 training data 的 loss 很大，显然它在训练集上面也没有训练好</strong>，接下来你要分析一下在训练集上面没有学好是什么原因。一种原因是 model bias，一种是 optimization 的问题。</p> <h3 id="_1-1-model-bias"><a href="#_1-1-model-bias" class="header-anchor">#</a> 1.1 model bias</h3> <p>所谓 <strong>model bias 的意思是说，你的 model 太过简单</strong>。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210311205956634.png" alt="img" style="zoom:80%;"> <p>举例来说，我们现在写了一个有未知 parameter 的 function，这个未知的 parameter，我们可以代各种不同的数字，你代 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container> 就可以得到一个 function <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.236em;"><mjx-TeXAtom size="s"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container>，代 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container> 就可以得到一个 function <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.236em;"><mjx-TeXAtom size="s"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container>，把所有的 function 集合起来，可以得到一个 function set。</p> <p><strong>如果 model 太简单，那么这个 function set 太小了，使得它没有包含任何一个 function 可以让我们的 loss 变得够低</strong>。这时即便找到这里面最好的那个 function <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2217"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container>，依然无济于事，这个 loss 还是不够低。</p> <p>这个状况就像你想要在大海里面捞针，这个针指的是一个 loss 低的 function，结果针根本就不在海里面。</p> <p><strong>Solution：重新设计一个 model，给你的 model 更大的弹性</strong>。比如增加输入的 features、设计一个更大的 model …</p> <h3 id="_1-2-optimization-issue"><a href="#_1-2-optimization-issue" class="header-anchor">#</a> 1.2 optimization issue</h3> <p>但是并不是 training 的时候，loss 大就代表一定是 model bias，你可能会遇到另外一个问题：<strong>optimization 做得不好</strong>。</p> <p>我们可能卡在一个 <mark>local minima</mark> 的地方，这时你没有办法找到一个真的可以让 loss 很低的参数，如图：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210311213108040.png" alt="img" style="zoom:67%;"> <p>这就好像是说我们想大海捞针，针确实在海里，但是我们却没有办法把针捞起来。</p> <p>这就产生了一个问题：<strong>training data 的 loss 不够低的时候，到底是 model bias，还是 optimization 的问题呢</strong>？一个建议判断的方法，就是你可以透过比较不同的模型，来得知你的 model 现在到底够不够大。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210311214054168.png" alt="image-20210311214054168" style="zoom:67%;"> <p>举一个例子，如上图，有两个 network，一个有 20 层，一个有 56 层，现在我们把它们测试在测试集上，这个横轴指的是 training 的过程。随着参数的 update，当然你的 loss 会越来越低,但是结果 20 层的 loss 比较 56 层的 loss还高，这说明 56 层的 network 的 optimization 没有做好，因为 20 层 network 能做到的事，56 层可以轻而易举地做到。</p> <p>所以如果 56 层的 optimization 成功的话，它的 loss 应当是比 20 层的 network 低的。</p> <p>那么，<strong>我们怎样知道我们的 optimization 有没有做好</strong>？这边给的建议是：看到一个你从来没有做过的问题，也许你可以先跑一些比较小的、比较浅的network，这些 model 会竭尽全力地找出一组最好的参数，不太会有失败的问题。所以我们可以先 train 一些比较简单的 model，先可以知道它们可以得道什么样的 loss。</p> <p><font color="blue">If deeper networks do not obtain smaller loss on training data, then there is optimization issue.</font></p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210313203557412.png" alt="img" style="zoom:67%;"> <ul><li>这个 5 layer 的 model 就是 optimization 没有做好</li></ul> <p>如果 optimization 没有做好该怎么办？我们会在之后讲。</p> <h3 id="_1-3-overfitting"><a href="#_1-3-overfitting" class="header-anchor">#</a> 1.3 overfitting</h3> <p>假设你现在经过一番的努力，你已经可以让 training data 的 loss 变小了，那接下来你就可以来看 testing data loss，如果它仍很大，那可能真的遇到 overfitting 的问题了。</p> <blockquote><p>注意，<strong>training 的 loss 小，testing 的 loss 大，才有可能是 overfitting</strong>，而不是一看到 testing 上结果不好就说是 overfitting 了。</p></blockquote> <p>什么是 overfitting 不再介绍了。</p> <h2 id="_2-local-minima-与-saddle-point"><a href="#_2-local-minima-与-saddle-point" class="header-anchor">#</a> 2. local minima 与 saddle point</h2> <h3 id="_2-1-critical-point"><a href="#_2-1-critical-point" class="header-anchor">#</a> 2.1 Critical Point</h3> <p>我们只讨论 Optimization 的时候，怎么把 gradient descent 做得更好，为什么 Optimization 会失败呢？</p> <p>常常在做 Optimization 时，你会发现，<strong>随着你的参数不断的 update，你的 training 的 loss 不会再下降，但是你对这个 loss 仍然不满意</strong>。比如你把 deep 的 network 与 shallow network 比较，发现 deep 的并没有做得更好，所以你会觉得 deep network 没有发挥它完整的力量，所以 Optimization 显然是有问题的。<strong>有时候甚至会发现，一开始你的 model 就 train 不起来，不管你怎样 update 你的参数，你的 loss 通通掉不下去</strong>，这时候到底发生了什么事呢？</p> <p>过去常见的一个猜想是我们走到了一个地方，<strong>这个地方参数对 loss 的微分为零</strong>，这样 gradient descent 就没有办法再 update 参数了，loss 当然就不会再下降了。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210314153200619.png" alt="img" style="zoom:67%;"> <p>local minima 和 saddle point 的 gradient 都是 0，所以当你的 loss 没有办法再下降时，也许就是因为卡在了这样的地方，它们统称为 <mark>critical point</mark>。</p> <p>但是今天如果你发现你的 gradient 真的很靠近 0，卡在了某个 critical point，我们有没有办法知道，到底是 local minima 还是 saddle point？其实是有办法的。</p> <p><strong>为什么我们想知道到底是卡在 local minima 还是卡在 saddle point 呢</strong>？</p> <ul><li>如果卡在 local minima，那可能就没有路可以走了。因为四周都比较高，你所在的位置就是最低点了</li> <li>如果卡在 saddle point 的话，它的旁边是还有路可以让 loss 变低的，<strong>只要你逃离 saddle point，你就有可能让你的 loss 更低</strong></li></ul> <p><strong>如何鉴别今天的一个 critical point 是属于 local minima 还是 saddle point 呢</strong>？</p> <h3 id="_2-2-判断是-local-minima-还是-saddle-point"><a href="#_2-2-判断是-local-minima-还是-saddle-point" class="header-anchor">#</a> 2.2 判断是 local minima 还是 saddle point</h3> <p>虽然我们没有办法完整知道整个 loss function 的样子，但如果给定某一组参数，比如说蓝色的这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math></mjx-container>，在 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math></mjx-container> 附近的 loss function 是有办法写出来的。所以 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container> 完整的样子写不出来，但它在 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math></mjx-container> 附近，既可以用一个 Taylor 级数展开来表示它：</p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo space="4" class="mjx-n"><mjx-c c="2248"></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mstyle style="color:blue;"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-mstyle><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mo space="3" class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mstyle space="3" style="color:blue;"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-mstyle><mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-script style="vertical-align:0.413em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="T"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mstyle style="color:green;"><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi></mjx-mstyle><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mn class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mstyle space="3" style="color:blue;"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-mstyle><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mstyle style="color:red;"><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi></mjx-mstyle><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mstyle space="3" style="color:blue;"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-mstyle><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container></p><p></p> <div align="center"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404073642773.png" alt="image-20220404073642773" style="zoom:67%;"></div> <ul><li>第一项是 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container>，当 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi></mjx-math></mjx-container> 与 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math></mjx-container> 很近时，<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container> 与 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container> 也是很靠近的</li> <li>第二项是 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-script style="vertical-align:0.363em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="T"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mstyle style="color:green;"><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi></mjx-mstyle></mjx-math></mjx-container>，这个绿色的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi></mjx-math></mjx-container> 是一个向量，它就是我们的 gradient，它会弥补 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math></mjx-container> 与 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi></mjx-math></mjx-container> 之间的差距，这个向量的第 i 个元素就是 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi></mjx-math></mjx-container> 的第 i 个元素对 L 的微分，如下图：</li></ul> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404074527896.png" alt="image-20220404074527896" style="zoom:50%;"> <ul><li>第三项会再补足加上 gradient 之后与真正的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container> 之间的差距。其中有一个 Hessian 矩阵 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi></mjx-math></mjx-container>，计算方式为：<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="j"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-msup size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="j"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container>。</li></ul> <blockquote><p>比如参数有 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，那分别求出 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c><mjx-c c="1"></mjx-c></mjx-mn></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-msup><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.288em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msubsup></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container>、<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c><mjx-c c="2"></mjx-c></mjx-mn></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-msup><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container>、<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mn class="mjx-n"><mjx-c c="2"></mjx-c><mjx-c c="1"></mjx-c></mjx-mn></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-msup><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container>、<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mn class="mjx-n"><mjx-c c="2"></mjx-c><mjx-c c="2"></mjx-c></mjx-mn></mjx-TeXAtom></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-msup><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.288em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msubsup></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container> 即可得到 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi></mjx-math></mjx-container>。</p></blockquote> <p>当我们走到一个 critical point 时，意味着 gradient 为 0，也就是绿色的这一项完全不见了，只剩下红色的这一项。所以在 critical point 处，它的 loss function 可以被近似为 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container> 加上红色这一项。根据红色这一项，我们就可以判断 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math></mjx-container> 附近的 error surface 长什么样，从而判断是 local minima 还是 saddle point。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404075617386.png" alt="image-20220404075617386" style="zoom:67%;"> <p><strong>怎样根据 Hessian，即红色这一项，来判断附近的地貌呢</strong>？这可以通过将附近的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math></mjx-container> 代入计算：</p> <div align="center"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210314161411744.png" alt="img" style="zoom:50%;"></div> <p>但是我们怎么可能把所有的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi></mjx-math></mjx-container> 都拿来试一下呢，所以需要一个更加简便的方法来确认。这就需要线性代数的知识了，如果对于所有的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi></mjx-math></mjx-container> 而言，<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="T"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi></mjx-math></mjx-container> 都大于 0，那这个矩阵 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi></mjx-math></mjx-container> 叫做正定矩阵（positive definite matrix），它的所有 eigen value（特征值）都是正的。于是我们可以得出如下的结论：</p> <div class="custom-block theorem"><p class="title">判断是 local minima 还是 saddle point</p><p>求出 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi></mjx-math></mjx-container> 的 eigen value，如果：</p> <ul><li>所有 eigen value 都是正的，那代表 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="2200"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-msup space="2"><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="T"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-c c="&gt;"></mjx-c></mjx-mo><mjx-mn space="4" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-math></mjx-container>，这是一个 local minima；</li> <li>所有 eigen value 都是负的，那代表 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="2200"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-msup space="2"><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="T"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-c c="&lt;"></mjx-c></mjx-mo><mjx-mn space="4" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-math></mjx-container>，这是一个 local maxima；</li> <li>如果 eigen valule 有正有负，那代表这是一个 saddle point。</li></ul></div><p>以上我们借助 Hessian 矩阵来判断出了一个 critical point 是属于 local minima 还是 saddle point，但在实际的 implementation 里面，你几乎不会真的把 Hessian 算出来，这个要是二次微分，需要的运算量非常大，更遑论还要求它的eigen value，<strong>所以你几乎看不到有人用这一个方法来逃离 saddle point</strong>。之后我们会讲其他的方法来逃离 saddle point，我们这里讲这个方法，是想说，如果是卡在 saddle point，也许没有那么可怕，最糟的状况下你还有这一招可以告诉你要往哪一个方向走。</p> <h3 id="_2-3-saddle-point-v-s-local-minima"><a href="#_2-3-saddle-point-v-s-local-minima" class="header-anchor">#</a> 2.3  Saddle Point v.s. Local Minima</h3> <p>一个问题是，到底 <strong>saddle point 跟 local minima 谁比较常见呢</strong>？</p> <blockquote><p>我们先讲一个可能不太相关的故事。1543 年东罗马帝国的国王不知道要怎么对抗土耳其人，这时有人找来一个魔法师，叫做<strong>狄奥伦娜</strong>。他有一个能力跟张飞一样，可以“<em>万军从中取上将首级如探囊取物</em>”，这个狄奥伦娜也一样，他可以直接取得那个苏丹的头。大家想让狄奥伦娜展示一下他的能力，于是他一下拿出了一个圣杯，这个圣杯本来是放在圣索菲亚大教堂的地下室，而且它是被放在一个石棺里面，这个石棺是密封的，没有人可以打开它，但狄奥伦娜却取了出来。为什么他可以做到呢？因为这个石棺你觉得它是封闭的，那是因为你是从三维的空间来看，但狄奥伦娜可以进入四维的空间，从高维的空间中这个石棺是有路可以进去的，它并不是封闭的。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404084246805.png" alt="image-20220404084246805" style="zoom:67%;"> <p>总之这个<strong>从三维的空间来看，是没有路可以走的东西，在高维的空间中是有路可以走的，那 error surface 会不会也一样呢</strong>？</p></blockquote> <p>当你在一维的空间中，一维的一个参数的 error surface，你会觉得好像到处都是 local minima，但是会不会在二维空间来看，它就只是一个 saddle point 呢？如下图所示：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210314205016598.png" alt="img" style="zoom:50%;"> <p>略过实验过程，从经验上看起来，<strong>其实 local minima 并没有那么常见</strong>，多数的时候，你觉得你 train 到一个地方，你的 gradient 真的很小，然后你的参数不再 update 了，<strong>往往是因为你卡在了一个 saddle point</strong>。</p> <h2 id="_3-batch"><a href="#_3-batch" class="header-anchor">#</a> 3. Batch</h2> <p>实际上在算微分的时候，并不是真的对所有 data 算出来的 L 作微分，而是把所有的 data 分成一个一个的 mini-batch，每次拿一个 batch 来算 loss、算 gradient，从而 update 参数。所有的 batch 看过一遍，叫做一个 epoch。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315142626597.png" alt="image-20210315142626597" style="zoom:67%;"> <p>Small Batch v.s. Large Batch</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404085703048.png" alt="image-20220404085703048" style="zoom:67%;"> <p>考虑如上的两个极端情况，假设我们有 20 笔训练资料：</p> <ul><li>左边的 case 就是没有用 batch，batch size 设的跟训练资料一样多，这种情况叫做 <strong>Full Batch</strong>，就是没有 batch 的意思</li> <li>右边的 case 就是 batch size = 1</li></ul> <p>比较两者，会发现左边没有用 Batch 的方式，它蓄力的时间比较长，还有它技能冷却的时间比较长，你要把所有的资料都看过一遍才能够 update 一次参数。而右边的方法 batch size = 1 的时候，蓄力的时间比较短，每次看到一笔参数，你就会更新一次你的参数。</p> <h3 id="_3-1-larger-batch-可能花费时间更少"><a href="#_3-1-larger-batch-可能花费时间更少" class="header-anchor">#</a> 3.1 larger batch 可能花费时间更少</h3> <p>但<strong>实际上考虑并行运算的话，左边这个并不一定时间比较长</strong>。从真正的实验结果来看，比较大的 batch size，你要算 loss，再进而算 gradient，所需要的时间不一定比小的 batch size 要花的时间长。</p> <p><font color="blue"> Larger batch size does not require longer time to compute gradient.</font></p> <p>一个实验结果如下图：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404090543327.png" alt="image-20220404090543327" style="zoom:80%;"> <ul><li>纵轴是花费的时间</li> <li>因为 GPU 有平行运算的能力，因此实际上当你的 batch size 小的时候，你要跑完一个 epoch 花的时间是比大的 batch size 还要多的</li> <li>但它平行运算能力终究是有个极限，所以你 batch size 真的很大的时候，时间还是会增加的</li></ul> <h3 id="_3-2-small-batch-训练得到的精确度可能更好"><a href="#_3-2-small-batch-训练得到的精确度可能更好" class="header-anchor">#</a> 3.2 small batch 训练得到的精确度可能更好</h3> <p>可以看到 Large Batch 在时间上是有优势的。那在训练结果上呢？小的 batch 在训练过程中每一步会受到 noisy 的影响，而 <strong>noisy 的 gradient 反而可以帮助 training</strong>。我们来看一个实验结果，来比较不同 batch size 在精确度方面的不同：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315153147903.png" alt="image-20210315153147903" style="zoom:80%;"> <ul><li>横轴代表的是 Batch Size,从左到右越来越大</li> <li>纵轴代表的是正确率,越上面正确率越高,当然正确率越高越好</li></ul> <p>可以看到 batch size 越大，它在 training 和 validation 中的 acc 都在降低，这是一个 optimization issue。<strong>当你用大的 Batch Size 的时候，你的 optimization 可能会有问题</strong>。</p> <p><strong>为什么 small batch 的 noisy update 会在 training 中更好呢</strong>？一个可能的解释是：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315155345489.png" alt="image-20210315155345489" style="zoom:67%;"> <ul><li>假如你是 Full Batch，那你今天在 update 你的参数的时候,你就是沿著一个 loss function 来 update 参数，如果走到一个 critical point，就会停下来了从而不再更新参数</li> <li>假如你是 Small Batch，因为我们是每次挑出一个 batch 来算 loss，这样 update 参数的时候 loss function 是有差异的，比如第一个 batch 中用 L1 来算 gradient，到第二个 batch 时用 L2 来算 gradient，这样假设 L1 算 gradient 是 0，卡住了，但 L2 的 function 与 L1 不同，这样 L2 不会卡住从而 update。</li></ul> <h3 id="_3-3-small-batch-更容易泛化"><a href="#_3-3-small-batch-更容易泛化" class="header-anchor">#</a> 3.3 small batch 更容易泛化</h3> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315160405510.png" alt="image-20210315160405510" style="zoom:67%;"> <ul><li>会发现，小的 batch 居然在 testing 的时候会比较好</li></ul> <p>一个解释是，<strong>大的 Batch Size，会让我们倾向于走到峡谷里面，而小的 Batch Size，倾向于让我们走到盆地里面</strong>：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315161935349.png" alt="img" style="zoom:50%;"> <ul><li>local minima 也有好坏之分，好的 minima 更容易有好的 generalization。</li></ul> <div class="custom-block warning"><p class="custom-block-title">Large Batch 比较 Small Batch</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315164405953.png" alt="image-20210315164405953" style="zoom:67%;"></div> <p>它们各有擅长的地方，所以 batch size 变成另外一个你需要去调整的 hyperparameter。</p> <blockquote><p>那我们能不能够鱼与熊掌兼得呢,我们能不能够截取大的 Batch 的优点,跟小的 Batch 的优点,我们用大的 Batch Size 来做训练,用平行运算的能力来增加训练的效率,但是训练出来的结果同时又得到好的结果呢,又得到好的训练结果呢？这时有可能的，有多篇论文给出了一些思路，这里不再介绍。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404094100122.png" alt="image-20220404094100122" style="zoom:67%;"></blockquote> <h2 id="_4-momentum"><a href="#_4-momentum" class="header-anchor">#</a> 4. Momentum</h2> <p>Momentum 是另外一个可以对抗 critical point 的技术。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404094558500.png" alt="image-20220404094558500" style="zoom:67%;"> <ul><li>考虑物理世界，当一个球滚到 local minima 时，由于惯性，他可能继续向前走，从而翻过小坡逃离 local minima。</li></ul> <p>,一般的 Gradient Descent 长什么样子呢：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315170131552.png" alt="image-20210315170131552" style="zoom:67%;"> <ul><li>有一个初始参数 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container>，然后计算一下 gradient，再往 gradient 的反方向去 update 参数：<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-msup space="4"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container>。</li></ul> <p>而  <strong>Gradient Descent + Momentum</strong> 是不只往 Gradient 的反方向来移动参数，而<strong>是 Gradient 的反方向，加上前一步移动的方向，用两者加起来的结果去调整去到我们的参数</strong>。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210315171104120.png" alt="image-20210315171104120" style="zoom:67%;"> <ul><li>先初始化一个参数 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mn space="4" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-math></mjx-container>，在 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container> 的地方计算 gradient 的方向 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container>，之后决定下一步怎么走：</li></ul> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="3BB"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></p><p></p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-msup space="4"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></p><p></p> <p>之后一直进行这个过程。</p> <p>来看一个简单的例子：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404095604056.png" alt="image-20220404095604056" style="zoom:67%;"> <p>当走到一个 local minima 的点时，已经没有 gradient 的方向了 ，但如果有 momentum 的话，就有办法继续走下去，因为下一步的方向不只看 gradient，这样翻过这个小丘的话，也许就走到了一个更好的 local minima。这就是 Momentum 有可能带来的好处。</p> <h2 id="_5-adaptive-learning-rate"><a href="#_5-adaptive-learning-rate" class="header-anchor">#</a> 5. Adaptive Learning Rate</h2> <p>critical point 可能并不是训练过程的最大障碍，本节介绍一个叫做 Adaptive Learning Rate 的技术，可以给每一个参数不同的 learning rate。</p> <h3 id="_5-1-training-stuck-small-gradient"><a href="#_5-1-training-stuck-small-gradient" class="header-anchor">#</a> 5.1 training stuck ≠ small gradient</h3> <p>为什么我们说 <strong>critical point 不一定是我们训练过程中最大的阻碍呢</strong>？大家训练 network 时会记录 loss，随着参数的 update，loss 会越来越小，最后卡住了，即 loss 不再下降，多数这时候大家会猜想是不是走到了 critical point 导致没有办法再更新参数。但实际真的这样吗？</p> <p>当走到 critical point 时 gradient 会很小，但如果在你 loss 不再下降时确认一下 gradient，它并不一定很小，比如下面这个例子，当 loss不再下降时 gradient 并没有真的很小：</p> <div align="center"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404202604212.png" alt="image-20220404202604212" style="zoom:80%;"></div> <p>gradient 是一个向量，下面是 gradient 的 norm，即 gradient 这个向量的长度，随着参数更新，你会发现说<strong>虽然 loss 不再下降，但是这个 gradient 的 norm 并没有真的变得很小</strong>。这样子的结果也许是遇到了这样的情况：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404202827560.png" alt="image-20220404202827560" style="zoom:67%;"> <ul><li>它的 gradient 仍然很大，只是 loss 不再减小了</li></ul> <blockquote><p>所以你在 train 一个 network 的时候，发现 loss 不再下降，这时不要随便说卡在了 critical point，有时候可能就是单纯地 loss 没有办法再下降了。</p></blockquote> <p>在实际中，用一般的 gradient descend 其实很难让参数走到 critical point，多数时候还没有走到 critical point 就已经停止了。这不代表说 critical point 不是一个问题，而是说，<strong>当你用gradient descend 来做 optimization 的时候，你真正应该要怪罪的对象往往不是 critical point，而是其他的原因</strong>。</p> <p>如果不是 critical point，那我们的 training 为什么会卡住呢？举下面一个简单的 error surface 的例子，我们有两个参数，两个参数值不一样时 loss 值也不一样，这样就画出了一个 convex 形状的 error surface，它的最低点在黄色的 X 处：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210319095748513.png" alt="image-20210319095748513" style="zoom:50%;"> <ul><li>它在横轴的地方 gradient 非常小，也就是坡度变化非常平滑</li> <li>相比于横轴，其纵轴的 gradient 变化较大，也就是坡度变化非常陡峭</li></ul> <p>我们从黑色的点开始走，来做 gradient descend，会发现它做不好：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404210713530.png" alt="image-20220404210713530" style="zoom:80%;"> <ul><li>参数在峡谷的两端来回震荡，使得 loss 掉不下去。</li></ul> <p>这也许你会归因于 learning rate 太大导致步伐太大了，如果将 learning rate 调小之后呢？将 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-math></mjx-container> 调到 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c><mjx-c c="0"></mjx-c></mjx-mn><mjx-script style="vertical-align:0.393em;"><mjx-TeXAtom size="s"><mjx-mo class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c c="7"></mjx-c></mjx-mn></mjx-TeXAtom></mjx-script></mjx-msup></mjx-math></mjx-container> 之后终于不再震荡了，但走到坡度平滑的地方，这么小的 learning rate 根本没有办法再让我们的训练前进：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404211049091.png" alt="image-20220404211049091" style="zoom:80%;"> <ul><li>在左拐的这个小地方有十万个点，所以说很难前进。</li></ul> <p>显然<strong>就算是一个 convex 的 error surface，你用 gradient descend 也很难 train</strong>。所以我们需要更好的 gradient descend 版本，之前的所有参数设同样的 learning rate 是不太行的，而应该为每一个参数定制化其 learning rate。</p> <h3 id="_5-2-different-parameters-needs-different-learning-rate"><a href="#_5-2-different-parameters-needs-different-learning-rate" class="header-anchor">#</a> 5.2 Different parameters needs different learning rate</h3> <p>我们要怎样定制化 learning rate 呢？不同的参数需要什么样的 learning rate 呢？</p> <p>从刚才的例子中可以看到一个大原则：<strong>在某一个方向上，如果 gradient 很小，非常平坦，那我们会希望 learning rate 调大一点；如果非常陡峭，则希望 learning rate 调小一点</strong>。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404212238607.png" alt="image-20220404212238607" style="zoom:67%;"> <p>那这个 learning rate 要如何自动调整呢？我们改一下 gradient descend 原来的式子，这里我们只看某一个参数 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container> 的情况，它在第 t 个 iteration 的值表示为 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container>，原先的 gradient descend 的更新方式为：</p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-TeXAtom><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo space="4" class="mjx-n"><mjx-c c="2190"></mjx-c></mjx-mo><mjx-msubsup space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.247em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.185em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.247em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.185em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container></p><p></p> <p>现在要为每一个参数定制化其 learning rate，我们就把 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-math></mjx-container> 这一项改写成 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mi size="s" class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msubsup size="s"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container>，于是有：</p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-TeXAtom><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo space="4" class="mjx-n"><mjx-c c="2190"></mjx-c></mjx-mo><mjx-msubsup space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.247em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.185em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mi class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.247em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.185em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container></p><p></p> <ul><li>这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container> 有一个上标 t 和一个下标 i，这说明<strong>它是 depend on <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-math></mjx-container> 的，不同的参数我们要给它不同的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container>；同时也是 iteration dependent 的，不同的 iteration 有不同的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container></strong>。</li></ul> <p>所以现在的 learning rate <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mi size="s" class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msubsup size="s"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container> 是一个 parameter dependent 的。那么这个 learning rate 有什么常见的计算方式呢？</p> <h3 id="_5-3-root-mean-square-与-adagrad"><a href="#_5-3-root-mean-square-与-adagrad" class="header-anchor">#</a> 5.3 Root mean square 与 Adagrad</h3> <p>有什么方式可以计算 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.292em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container> 呢？一个常见的方式是计算 gradient 的 Root Mean Square：</p> <div align="center"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210319150808494.png" alt="img" style="zoom:58%;"></div> <p>这一招经常用在 <mark>Adagrad</mark> 中。</p> <p>为什么这一招可以做到坡度比较大的时候 learning rate 就减小，而坡度比较小的时候 learning rate 就放大呢？</p> <div align="center"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210319160639783.png" alt="image-20210319160639783" style="zoom:67%;"></div> <ul><li>在上面蓝色曲线的图中，坡度较小，gradient 值较小，这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 是 gradient 的平方和取平均再开根号，因此算出来的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 也较小，这样 learning rate 就较大。</li> <li>下面绿色曲线则与蓝色的相反，其 learning rate 就相对较大。</li></ul> <p>所以<strong>有了 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 这一项，便可以根据每一个参数的 gradient 不同来自动调整 learning rate 的大小</strong>。</p> <h3 id="_5-4-rmsprop"><a href="#_5-4-rmsprop" class="header-anchor">#</a> 5.4 RMSProp</h3> <p>在刚刚的版本里，同一个参数的 learning rate 也会随着时间改变，但在刚刚假设中，好像同一个参数的 gradient 的大小是固定差不多的值，但事实上并不一定是这个样子的。</p> <p>我们举下面这个新月形的 error surface：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404220401169.png" alt="image-20220404220401169" style="zoom:67%;"> <ul><li>考虑横轴方向，会发现绿色箭头这个地方坡度比较陡峭，所以我们需要比较小的 learning rate；而在红色箭头的时候坡度又变得平滑了起来，需要比较大的 learning rate。</li></ul> <p>所以，就算是<strong>同一个参数同一个方向，我们也期待说 learning rate 是可以动态调整的</strong>，于是就有了一个叫做 <mark>RMSProp</mark> 的新招数：</p> <div align="center"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210319212301760.png" alt="image-20210319212301760" style="zoom:67%;"></div> <ul><li>它的第一步跟刚刚讲的 Adagrad 方法一样</li> <li>在第二步中，刚刚算 Root Mean Square 时每一个 gradient 都有同等重要性，但<strong>在 RMSProp 里面，你可以自己调整现在这个 gradient 的重要程度</strong>：</li></ul> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.247em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.193em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-msqrt space="4"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-lop"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.244em;"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.25em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="0"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mo space="3" class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.247em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.193em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-msup><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-script style="vertical-align:0.413em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container></p><p></p> <p>这里面的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 就像 learning rate 一样，是一个 hyperparameter：</p> <ul><li>如果把 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 设很小，就代表说觉得新鲜热腾的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.284em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container> 相较于之前算出来的 gradient 而言比较重要</li> <li>如果把 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 设很大，就代表我觉得新算出来的 gradient 不如之前的重要</li></ul> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404222210626.png" alt="image-20220404222210626" style="zoom:67%;"> <p>我们形象化地展示一个例子，下面这个黑线代表一个 error surface：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220404222344056.png" alt="image-20220404222344056" style="zoom:67%;"> <ul><li>在陡峭的地方，你可以借助 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 来让 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 的计算更看重当前的 gradient，使得 step 小一些。</li></ul> <h3 id="_5-5-adam"><a href="#_5-5-adam" class="header-anchor">#</a> 5.5 Adam</h3> <p>今天最常用的 optimization 策略就是 <mark>Adam</mark>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210319220458633.png" alt="image-20210319220458633" style="zoom:67%;"></center> <p>Adam 就是 RMSProp + Momentum，<a href="https://gitee.com/link?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1412.6980.pdf" target="_blank" rel="noopener noreferrer">Adam 原始论文<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</p> <p>在 pytorch 里面该 optimizer 已经帮我们写好了，里面也有一些参数需要调，但往往用预设的就够好了。</p> <h3 id="_5-6-learning-rate-scheduling"><a href="#_5-6-learning-rate-scheduling" class="header-anchor">#</a> 5.6 Learning Rate Scheduling</h3> <p>普通的 gradient descend 训练不起来 convex 形的 error surface，那加上 Adaptive Learning Rate 以后呢？</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405063413039.png" alt="image-20220405063413039" style="zoom:67%;"> <p>a 图是普通的 gradient descend 训练的效果，b 图是加上 Adaptive Learning Rate 之后的效果。</p> <p>之前是卡在了左转的地方，现在有了 Adagrad 之后，就可以继续走下去了，因为在左转处，左右方向的 gradient 很小，因此 learning rate 会自动调整，使得步伐变大从而不断前进。</p> <p>接下来的问题是<strong>为什么快到终点时突然爆炸了呢</strong>？想想看，在计算 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 时是把过去所有看到的 gradient 都拿来做平均，这时：</p> <ul><li>在纵轴方向上，初始地方的 gradient 很大</li> <li>在左转走了一段路以后，这个纵轴方向的 gradient 算出来都很小，因此这个纵轴方向就累积了很小的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container>，累积到一定地步后，这个 step 就变很大，然后就暴走喷出去了</li> <li>喷出去以后也没关系，他就走到了一个 gradient 比较大的地方，以后这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 慢慢变大，这个参数 update 的步伐就慢慢变小</li> <li>所以你会发现，突然左右喷了一下，但这个喷了一下不会永远震荡，摩擦力会让他慢慢地又回到中间这个峡谷中。</li></ul> <p>但是累计一段时间后又会喷一下，怎么办呢？有一个办法也许可以解决这个问题，叫做 <mark>learning rate 的 scheduling</mark>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405064432370.png" alt="image-20220405064432370" style="zoom:80%;"></center> <p>我们之前的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-math></mjx-container> 是一个固定值，而 learning rate scheduling 的意思是说我们<strong>不要把 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-math></mjx-container> 当一个常数，而是让它跟时间有关</strong>。最常见的策略是 <mark>Learning Rate Decay</mark>，也就是<strong>随著时间的不断地进行，随著参数不断的 update，我们让这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi></mjx-math></mjx-container> 越来越小。</strong></p> <p>这是合理的，因为一开始我们距离终点很远，随著参数不断 update，我们距离终点越来越近，所以我们把 learning rate 减小，让我们参数的更新踩了一个煞车从而能够慢慢地慢下来。采取了 Learning Rate Decay 之后，可以看到消除了之前的“喷一下”（如下图 b 图）：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405064924437.png" alt="image-20220405064924437" style="zoom:67%;"> <p>还有另外一个经典常用的 Learning Rate Scheduling 方式叫做 <mark>Warm Up</mark>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405065215913.png" alt="image-20220405065215913" style="zoom:67%;"></center> <ul><li>Warm Up 的方法是让 learning rate 先变大后变小。但<strong>这个变化的速度也是一个 hyperparameter</strong>。</li></ul> <p>在很多 network 的训练中都使用了 Warm Up：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20210319222727186.png" alt="img" style="zoom:67%;"> <p>Warm Up 这个黑科技用在了很多知名 network 中，但这些论文里面不解释说为什么用它，却在你不注意的地方告诉你说这个 network 要用这个黑科技才能训练起来。</p> <p>一个可能的解释是说，<strong><mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 是用一个统计结果来告诉我们某一个方向的陡峭程度，但要看多笔数据之后这个统计才精确，所以一开始的统计是不精确的</strong>。</p> <blockquote><p>关于更多，有一个 Adam 的进阶版叫做 RAdam，细节可参考论文 <a href="https://arxiv.org/abs/1908.03265" target="_blank" rel="noopener noreferrer">RAdam<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <div class="custom-block warning"><p class="custom-block-title">小结</p> <p>我们对 optimization 的方法进行了改进，将原始的 gradient descend 进化到了这样一个版本：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220405070202982.png" alt="image-20220405070202982" style="zoom:67%;"></center></div> <p>其实关于 optimizer 还有很多东西，这里不再展开了。</p> <h2 id="_6-batch-normalization"><a href="#_6-batch-normalization" class="header-anchor">#</a> 6. Batch Normalization</h2> <h3 id="_6-1-introduction"><a href="#_6-1-introduction" class="header-anchor">#</a> 6.1 Introduction</h3> <p>之前我们说 error surface 如果很崎岖的话，会比较难 train，而 <strong>Batch Normalization 就是其中一个把山铲平的方法，从而变得容易去 train</strong>。</p> <p>再来看 optimization 的问题，有时会发现 error surface 是 convex 的，即一个碗的形状，会很难去 train，即两个参数的 loss 斜率相差很大，一个很陡峭，一个很平坦，这时使用 adaptive learning rate 会有好的结果。但从另一个角度想，如果能把难做的 error surface 改掉，不就可以更好做了吗？</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411163135642.png" alt="image-20220411163135642" style="zoom:50%;"> <p>假设我们有如下 model，输入是 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，没有 activation function：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411163250731.png" alt="image-20220411163250731" style="zoom:67%;"> <p>计算 loss <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-math></mjx-container> 的过程如上图。</p> <p><strong>那什么样的情况会产生比较不好 train 的 error surface 呢</strong>？在上面的计算 loss 的过程中，如果 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 值很 small，那么当 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 有了一个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="394"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 变化时，由此对最终的 loss <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-math></mjx-container> 产生的变化 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="394"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-math></mjx-container> 也很 small。如下图所示：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411164117732.png" alt="image-20220411164117732" style="zoom:80%;"> <p>反之，如果 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 值很 large，那么当 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 有了一个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="394"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 变化时，由此对最终的 loss <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-math></mjx-container> 产生的变化 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="394"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-math></mjx-container> 也很 large：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411164324945.png" alt="image-20220411164324945" style="zoom:80%;"> <p>由此便产生了之前所说的很难 train 的情况。可以发现，<strong>当 input 的 feature 中每一个 dimension 的值的 scala 差距很大时就可能产生不同方向上斜率非常不同的 error surface 导致难以 train</strong>。</p> <p>一个想法是给 feature 里不同 dimension 有同样的数值范围。其实有很多种不同的方法，合起来统称为 <mark>Feature Normalization</mark>。</p> <h3 id="_6-2-feature-normalization"><a href="#_6-2-feature-normalization" class="header-anchor">#</a> 6.2 Feature Normalization</h3> <p>以下所讲的方法只是Feature Normalization 的一种可能性。</p> <p>假设 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="2" class="mjx-n"><mjx-c c="2026"></mjx-c></mjx-mo><mjx-msup space="2"><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="R"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math></mjx-container> 是我们所有的训练资料的 feature vector，再把所有训练资料的 feature vector 统统集合起来：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411165219525.png" alt="image-20220411165219525" style="zoom:80%;"> <ul><li><mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.288em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msubsup></mjx-math></mjx-container> 表示第二个样本的 feature 1</li></ul> <p>然后我们<strong>把不同笔资料里的同一个 dimension 里面的数值取出来，去计算某一个 dimension 的 mean <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container> 和 standard deviation <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container> 来做标准化</strong>。</p> <p>以后我们用带 tilde 符号的表示被 normalize 后的数值。 经过 normalize 后所有 feature 不同 dimension 的数值都符合标准正态分布。这样的 gradient descent 会让训练更顺利。</p> <h3 id="_6-3-considering-deep-learning"><a href="#_6-3-considering-deep-learning" class="header-anchor">#</a> 6.3 Considering Deep Learning</h3> <h4 id="_6-3-1-对中间层的-feature-也做-normalize"><a href="#_6-3-1-对中间层的-feature-也做-normalize" class="header-anchor">#</a> 6.3.1 对中间层的 feature 也做 normalize</h4> <p>刚刚我们对 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-math></mjx-container> 做了标准化，但将 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.064em;margin-bottom:-0.215em;"><mjx-mo class="mjx-n"><mjx-c c="~"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom></mjx-math></mjx-container> 输入 network 后，中间层得到的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="z"></mjx-c></mjx-mi></mjx-math></mjx-container> 却又有了“different dims have different ranges”的现象，所以可以进一步做 normalize：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411170515131.png" alt="image-20220411170515131" style="zoom:50%;"> <blockquote><p>这里有一个问题是：<strong>对 activation function 的输入做 normalize 还是对输出做？其实差异不大</strong>。一个经验是，如果 activate function 是 sigmoid 的话建议在输入前做。但其实也没差啦。</p></blockquote> <p>怎样对 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="z"></mjx-c></mjx-mi></mjx-math></mjx-container> 做 Feature Normalization 呢？见下图：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411171233086.png" alt="image-20220411171233086" style="zoom:67%;"> <p>之后再往下就是：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411171301431.png" alt="image-20220411171301431" style="zoom:67%;"> <ul><li>注意计算 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.056em;margin-bottom:-0.215em;"><mjx-mo class="mjx-n"><mjx-c c="~"></mjx-c></mjx-mo></mjx-over><mjx-base style="padding-left:0.017em;"><mjx-mi class="mjx-i"><mjx-c c="z"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom><mjx-script style="vertical-align:0.432em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math></mjx-container> 时做的是 element wise 操作。</li></ul> <h4 id="_6-3-2-考虑-batch"><a href="#_6-3-2-考虑-batch" class="header-anchor">#</a> 6.3.2 考虑 “batch”</h4> <p>本来如果我们没有做 normalize，那改变 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="z"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 只会影响 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，但现在改变 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="z"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 会跟着改变 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3BC"></mjx-c></mjx-mi></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi></mjx-math></mjx-container>，因此会接着改变 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="223C"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="3"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>。对 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-math></mjx-container> 也是如此，本来他们是分开独立处理，<strong>现在加了 Feature Normalization 以后，这三个 example，它们变得彼此关联了</strong>。因此，我们可以把 network 视为一个 large network。之前的 network 只吃一个 input 得到一个 output，而<strong>现在的 large network 是吃一堆 input 并产生一堆 output</strong>。</p> <p>由于 GPU 的 memoey 不可能把整个 data set 都 load 进去，因此实际中，我们只会<strong>对一个 batch 里的 data 做 normalization</strong>，所以这招叫做 <mark>Batch Normalization</mark>。</p> <p>这里有一个问题，<strong>你一定要有一个够大的 batch，才可以算得出近似于整个 corpus 的分布</strong>（即足够大的 batch 才能算出合理的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3BC"></mjx-c></mjx-mi></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 作为 approximation）。</p> <h4 id="_6-3-3-normalize-后的额外操作"><a href="#_6-3-3-normalize-后的额外操作" class="header-anchor">#</a> 6.3.3 normalize 后的额外操作</h4> <p>在做 Batch Normalization 的时候，往往还会在算出 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.056em;margin-bottom:-0.215em;"><mjx-mo class="mjx-n"><mjx-c c="~"></mjx-c></mjx-mo></mjx-over><mjx-base style="padding-left:0.017em;"><mjx-mi class="mjx-i"><mjx-c c="z"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom></mjx-math></mjx-container> 后再额外做一个操作：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411172804946.png" alt="image-20220411172804946" style="zoom:67%;"></center> <p>这里的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B3"></mjx-c></mjx-mi></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B2"></mjx-c></mjx-mi></mjx-math></mjx-container> 可以想成是 network 的参数，是需要另外被 learn 出来的。这样网络结构如下：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411173020025.png" alt="image-20220411173020025" style="zoom:80%;"> <blockquote><p>为什么要这个设计呢？因为 normalize 后均值就是 0 了，这给 network 加了限制，也许这个限制会产生一些负面影响，因此把 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B2"></mjx-c></mjx-mi></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B3"></mjx-c></mjx-mi></mjx-math></mjx-container> 加回去，让 network 自己学习两个参数。</p></blockquote> <p>又有人会问<strong>加了这个操作，这不会又让不同的 dimension 有不同的 range 了嘛</strong>？有可能会吧，但设置初始值时 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B3"></mjx-c></mjx-mi></mjx-math></mjx-container> 的元素会都设为 1，<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B2"></mjx-c></mjx-mi></mjx-math></mjx-container> 的元素都初始为 0。这样一开始训练时每个 dimension 的分布还是很接近的，等训练时间够长后，逐渐找到一个比较好的 error surface 后，<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B3"></mjx-c></mjx-mi></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B8"></mjx-c></mjx-mi></mjx-math></mjx-container> 才会逐渐起作用，<strong>这往往对训练是有帮助的</strong>。</p> <h4 id="_6-3-4-inference-过程中的-moving-average"><a href="#_6-3-4-inference-过程中的-moving-average" class="header-anchor">#</a> 6.3.4 inference 过程中的 moving average</h4> <p>以上说的是 training 过程，在 testing 过程中（也称为 <mark>inference</mark>），会产生一个问题：training 时是一个一个 batch 的，但当服务上线成一个 application 后，inference 过程不可能等一个 batch size 的数据来了再运算一次。<strong>那么 inference 时没有了 batch，该怎么计算这个均值 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3BC"></mjx-c></mjx-mi></mjx-math></mjx-container> 和方差 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 呢</strong>？</p> <p>实际上，pytorch 已经帮我们做好了，Batch Normalization 在 testing 的时候，你不需要做什么特别处理，pytorch 就帮我们处理好了。如果有用 batch normalization，在 training 的时候,你每一个 batch 计算出来的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3BC"></mjx-c></mjx-mi></mjx-math></mjx-container> 跟 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="g"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi></mjx-math></mjx-container>，他都会拿出来算 <mark>moving average</mark>。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411185802962.png" alt="image-20220411185802962" style="zoom:80%;"> <ul><li>在 training 时，每取一个 batch 时计算出来的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3BC"></mjx-c></mjx-mi></mjx-math></mjx-container> 都会用来计算更新一个   <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.079em;margin-bottom:-0.544em;"><mjx-mo class="mjx-n"><mjx-c c="AF"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c c="3BC"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom></mjx-math></mjx-container>，过程如上图。</li></ul> <p>等到 inference 的时候，他会这样做：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411190034666.png" alt="image-20220411190034666" style="zoom:80%;"> <ul><li>这样在 testing 的时候就不用算 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3BC"></mjx-c></mjx-mi></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi></mjx-math></mjx-container> 了。</li></ul> <h3 id="_6-4-comparison"><a href="#_6-4-comparison" class="header-anchor">#</a> 6.4 Comparison</h3> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220411190252164.png" alt="image-20220411190252164" style="zoom:67%;"></center> <ul><li>对比黑线和红线，可以看到使用了 batch normalization 后可以更快地得跑到最后收敛的 accuracy。尽管随着数据量的增多，最终收敛结果差不多。</li></ul> <p>关于 Batch Normalization 为什么会起作用，可以参考原论文，但目前也没有特别肯定的说法。但理论上至少<strong>支持了 Batch Normalization 可以改变 error surface，让其比较不崎岖的这个观点</strong>。</p> <p>其实除了 Batch Normalization 还有很多其他的方法，更多可参考如下：</p> <details class="custom-block details"><summary>更多的 normalization 方法</summary> <ul><li><a href="https://arxiv.org/abs/1702.03275" target="_blank" rel="noopener noreferrer">Batch Renormalization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener noreferrer">Layer Normalization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://arxiv.org/abs/1607.08022" target="_blank" rel="noopener noreferrer">Instance Normalization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://arxiv.org/abs/1803.08494" target="_blank" rel="noopener noreferrer">Group Normalization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://arxiv.org/abs/1602.07868" target="_blank" rel="noopener noreferrer">Weight Normalization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://arxiv.org/abs/1705.10941" target="_blank" rel="noopener noreferrer">Spectrum Normalization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></details></div></div> <div class="page-slot page-slot-bottom"><!-- 横向自适应 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6620245489"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="page-edit"><div class="edit-link"><a href="https://github.com/yubincloud/notebook/edit/master/docs/AI/01.深度学习/15.深度学习-李宏毅/05.神经网络训练不起来怎么办.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2022/10/03, 13:55:17</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/notebook/pages/lhy/regression/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Regression</div></a> <a href="/notebook/pages/lhy/cnn/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">CNN</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/notebook/pages/lhy/regression/" class="prev">Regression</a></span> <span class="next"><a href="/notebook/pages/lhy/cnn/">CNN</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/notebook/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/notebook/pages/ml/lhy/drl17/"><div>
            Deep Reinforcement Learning
            <!----></div></a> <span class="date">10-03</span></dt></dl><dl><dd>02</dd> <dt><a href="/notebook/pages/mysql/geektime/misdeletion/"><div>
            误删数据后怎么办
            <!----></div></a> <span class="date">04-06</span></dt></dl><dl><dd>03</dd> <dt><a href="/notebook/pages/mysql/geektime/multi-slaves/"><div>
            MySQL 一主多从
            <!----></div></a> <span class="date">03-22</span></dt></dl> <dl><dd></dd> <dt><a href="/notebook/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="yubin_inbuy@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/yubincloud" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2021-2024
    <span>yubincloud | <a href="https://github.com/yubincloud/notebook/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <div class="custom-html-window custom-html-window-rb" style="display:;"><div class="custom-wrapper"><span class="close-but">×</span> <div><!-- 固定160*160px -->
      <ins class="adsbygoogle"
          style="display:inline-block;max-width:160px;max-height:160px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="8377369658"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      </div></div></div></div><div class="global-ui"><div></div></div></div>
    <script src="/notebook/assets/js/app.2bf3b6c1.js" defer></script><script src="/notebook/assets/js/2.0ad58009.js" defer></script><script src="/notebook/assets/js/77.40db9cc6.js" defer></script>
  </body>
</html>
