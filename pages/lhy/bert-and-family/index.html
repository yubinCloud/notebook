<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>BERT and its family | notebook</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/notebook/img/favicon.ico">
    <script data-ad-client="ca-pub-7828333725993554" async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="学习笔记">
    <meta name="keywords" content="全栈学习笔记">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/notebook/assets/css/0.styles.1b58b254.css" as="style"><link rel="preload" href="/notebook/assets/js/app.2bf3b6c1.js" as="script"><link rel="preload" href="/notebook/assets/js/2.0ad58009.js" as="script"><link rel="preload" href="/notebook/assets/js/87.8fba1553.js" as="script"><link rel="prefetch" href="/notebook/assets/js/10.99522837.js"><link rel="prefetch" href="/notebook/assets/js/100.e3b56889.js"><link rel="prefetch" href="/notebook/assets/js/101.6ea1d00b.js"><link rel="prefetch" href="/notebook/assets/js/102.eca9dfbd.js"><link rel="prefetch" href="/notebook/assets/js/103.6ea477d4.js"><link rel="prefetch" href="/notebook/assets/js/104.ac820d2b.js"><link rel="prefetch" href="/notebook/assets/js/105.58b259a8.js"><link rel="prefetch" href="/notebook/assets/js/106.a86005d0.js"><link rel="prefetch" href="/notebook/assets/js/107.7a79d36f.js"><link rel="prefetch" href="/notebook/assets/js/108.64404e25.js"><link rel="prefetch" href="/notebook/assets/js/109.75f12c0a.js"><link rel="prefetch" href="/notebook/assets/js/11.d26d59e4.js"><link rel="prefetch" href="/notebook/assets/js/110.1155fe36.js"><link rel="prefetch" href="/notebook/assets/js/111.bf8b5871.js"><link rel="prefetch" href="/notebook/assets/js/112.22833ceb.js"><link rel="prefetch" href="/notebook/assets/js/113.6a080233.js"><link rel="prefetch" href="/notebook/assets/js/114.35de9701.js"><link rel="prefetch" href="/notebook/assets/js/115.f598d8c2.js"><link rel="prefetch" href="/notebook/assets/js/116.e3bd29ce.js"><link rel="prefetch" href="/notebook/assets/js/117.c3c02abc.js"><link rel="prefetch" href="/notebook/assets/js/118.136a552a.js"><link rel="prefetch" href="/notebook/assets/js/119.c124f3f8.js"><link rel="prefetch" href="/notebook/assets/js/12.dc66c4f2.js"><link rel="prefetch" href="/notebook/assets/js/120.f835d124.js"><link rel="prefetch" href="/notebook/assets/js/121.367716ae.js"><link rel="prefetch" href="/notebook/assets/js/122.752b0493.js"><link rel="prefetch" href="/notebook/assets/js/123.9f8d6026.js"><link rel="prefetch" href="/notebook/assets/js/124.e8eb61b6.js"><link rel="prefetch" href="/notebook/assets/js/125.cb081200.js"><link rel="prefetch" href="/notebook/assets/js/126.ab87d911.js"><link rel="prefetch" href="/notebook/assets/js/127.ffdbe74d.js"><link rel="prefetch" href="/notebook/assets/js/128.ec526e42.js"><link rel="prefetch" href="/notebook/assets/js/129.71839012.js"><link rel="prefetch" href="/notebook/assets/js/13.32e95b42.js"><link rel="prefetch" href="/notebook/assets/js/130.2bc0bb4d.js"><link rel="prefetch" href="/notebook/assets/js/131.5595b49b.js"><link rel="prefetch" href="/notebook/assets/js/132.4963c5c4.js"><link rel="prefetch" href="/notebook/assets/js/133.44f48cfd.js"><link rel="prefetch" href="/notebook/assets/js/134.cf25626c.js"><link rel="prefetch" href="/notebook/assets/js/135.5ee30fa9.js"><link rel="prefetch" href="/notebook/assets/js/136.bc43f8e6.js"><link rel="prefetch" href="/notebook/assets/js/137.9ab5beac.js"><link rel="prefetch" href="/notebook/assets/js/138.692a33e6.js"><link rel="prefetch" href="/notebook/assets/js/139.08e7c98d.js"><link rel="prefetch" href="/notebook/assets/js/14.c418d170.js"><link rel="prefetch" href="/notebook/assets/js/140.39a861db.js"><link rel="prefetch" href="/notebook/assets/js/141.46678413.js"><link rel="prefetch" href="/notebook/assets/js/142.f7ef5eac.js"><link rel="prefetch" href="/notebook/assets/js/143.c92cbac1.js"><link rel="prefetch" href="/notebook/assets/js/144.d9c61437.js"><link rel="prefetch" href="/notebook/assets/js/145.9f603b31.js"><link rel="prefetch" href="/notebook/assets/js/146.b875f045.js"><link rel="prefetch" href="/notebook/assets/js/147.55e7c4f8.js"><link rel="prefetch" href="/notebook/assets/js/148.4410c365.js"><link rel="prefetch" href="/notebook/assets/js/149.6096ed98.js"><link rel="prefetch" href="/notebook/assets/js/15.e0e7392a.js"><link rel="prefetch" href="/notebook/assets/js/150.24451f07.js"><link rel="prefetch" href="/notebook/assets/js/151.7cff301c.js"><link rel="prefetch" href="/notebook/assets/js/152.035fee1f.js"><link rel="prefetch" href="/notebook/assets/js/153.c61f8ec3.js"><link rel="prefetch" href="/notebook/assets/js/154.7bb549d0.js"><link rel="prefetch" href="/notebook/assets/js/155.1dc494db.js"><link rel="prefetch" href="/notebook/assets/js/156.b87eaf39.js"><link rel="prefetch" href="/notebook/assets/js/157.e3f5a5c0.js"><link rel="prefetch" href="/notebook/assets/js/158.c565c699.js"><link rel="prefetch" href="/notebook/assets/js/159.a22609ef.js"><link rel="prefetch" href="/notebook/assets/js/16.d1aef4ee.js"><link rel="prefetch" href="/notebook/assets/js/160.b29e761c.js"><link rel="prefetch" href="/notebook/assets/js/161.bee1e522.js"><link rel="prefetch" href="/notebook/assets/js/162.c49fca62.js"><link rel="prefetch" href="/notebook/assets/js/163.2cb4d37d.js"><link rel="prefetch" href="/notebook/assets/js/164.4a0dbc64.js"><link rel="prefetch" href="/notebook/assets/js/165.490d05b3.js"><link rel="prefetch" href="/notebook/assets/js/166.df5d2527.js"><link rel="prefetch" href="/notebook/assets/js/167.89a81814.js"><link rel="prefetch" href="/notebook/assets/js/168.9991702e.js"><link rel="prefetch" href="/notebook/assets/js/169.2f9a5dce.js"><link rel="prefetch" href="/notebook/assets/js/17.88ae5445.js"><link rel="prefetch" href="/notebook/assets/js/170.5f23eb3c.js"><link rel="prefetch" href="/notebook/assets/js/171.c521aaa8.js"><link rel="prefetch" href="/notebook/assets/js/172.42110b0a.js"><link rel="prefetch" href="/notebook/assets/js/173.5e36f1bf.js"><link rel="prefetch" href="/notebook/assets/js/174.f48e078a.js"><link rel="prefetch" href="/notebook/assets/js/175.775da6a5.js"><link rel="prefetch" href="/notebook/assets/js/176.9c3c55ea.js"><link rel="prefetch" href="/notebook/assets/js/177.b54d1cff.js"><link rel="prefetch" href="/notebook/assets/js/178.ff08b7f5.js"><link rel="prefetch" href="/notebook/assets/js/179.c6a1af32.js"><link rel="prefetch" href="/notebook/assets/js/18.dcb78196.js"><link rel="prefetch" href="/notebook/assets/js/180.25dd9eba.js"><link rel="prefetch" href="/notebook/assets/js/181.13e6ec84.js"><link rel="prefetch" href="/notebook/assets/js/182.f6849f0d.js"><link rel="prefetch" href="/notebook/assets/js/183.7e664874.js"><link rel="prefetch" href="/notebook/assets/js/184.e6aba86f.js"><link rel="prefetch" href="/notebook/assets/js/185.df07b919.js"><link rel="prefetch" href="/notebook/assets/js/186.02c77e75.js"><link rel="prefetch" href="/notebook/assets/js/187.e8380ed4.js"><link rel="prefetch" href="/notebook/assets/js/188.eddc8bee.js"><link rel="prefetch" href="/notebook/assets/js/189.fbc1840f.js"><link rel="prefetch" href="/notebook/assets/js/19.5997a514.js"><link rel="prefetch" href="/notebook/assets/js/190.a37bfe4c.js"><link rel="prefetch" href="/notebook/assets/js/191.e53a3d4b.js"><link rel="prefetch" href="/notebook/assets/js/192.2f5be408.js"><link rel="prefetch" href="/notebook/assets/js/193.4ca6de49.js"><link rel="prefetch" href="/notebook/assets/js/194.b8e51d9d.js"><link rel="prefetch" href="/notebook/assets/js/195.70e6b23a.js"><link rel="prefetch" href="/notebook/assets/js/196.5d5fbf2d.js"><link rel="prefetch" href="/notebook/assets/js/197.78456dab.js"><link rel="prefetch" href="/notebook/assets/js/198.4308331c.js"><link rel="prefetch" href="/notebook/assets/js/199.2e537849.js"><link rel="prefetch" href="/notebook/assets/js/20.fc057fd7.js"><link rel="prefetch" href="/notebook/assets/js/200.b3309bbf.js"><link rel="prefetch" href="/notebook/assets/js/201.4723461c.js"><link rel="prefetch" href="/notebook/assets/js/202.b15b5177.js"><link rel="prefetch" href="/notebook/assets/js/203.22c50e61.js"><link rel="prefetch" href="/notebook/assets/js/204.5b8b3b00.js"><link rel="prefetch" href="/notebook/assets/js/205.54ee7630.js"><link rel="prefetch" href="/notebook/assets/js/206.f3f20f94.js"><link rel="prefetch" href="/notebook/assets/js/207.a9608973.js"><link rel="prefetch" href="/notebook/assets/js/208.1a80a593.js"><link rel="prefetch" href="/notebook/assets/js/209.586fd293.js"><link rel="prefetch" href="/notebook/assets/js/21.cb4205ee.js"><link rel="prefetch" href="/notebook/assets/js/210.7829dd53.js"><link rel="prefetch" href="/notebook/assets/js/211.3ce139ab.js"><link rel="prefetch" href="/notebook/assets/js/212.84738a64.js"><link rel="prefetch" href="/notebook/assets/js/213.a631830d.js"><link rel="prefetch" href="/notebook/assets/js/214.9d64cf85.js"><link rel="prefetch" href="/notebook/assets/js/215.87030b6b.js"><link rel="prefetch" href="/notebook/assets/js/216.ddbe1944.js"><link rel="prefetch" href="/notebook/assets/js/217.16ae7e40.js"><link rel="prefetch" href="/notebook/assets/js/218.e7780d65.js"><link rel="prefetch" href="/notebook/assets/js/219.abae5e09.js"><link rel="prefetch" href="/notebook/assets/js/22.256014b4.js"><link rel="prefetch" href="/notebook/assets/js/220.8e3a8702.js"><link rel="prefetch" href="/notebook/assets/js/221.4c279d74.js"><link rel="prefetch" href="/notebook/assets/js/222.6c9b2595.js"><link rel="prefetch" href="/notebook/assets/js/223.cc072424.js"><link rel="prefetch" href="/notebook/assets/js/224.c663b40f.js"><link rel="prefetch" href="/notebook/assets/js/225.f3e52654.js"><link rel="prefetch" href="/notebook/assets/js/226.5e00402c.js"><link rel="prefetch" href="/notebook/assets/js/227.1c28ce97.js"><link rel="prefetch" href="/notebook/assets/js/228.42b8c305.js"><link rel="prefetch" href="/notebook/assets/js/229.df9760ec.js"><link rel="prefetch" href="/notebook/assets/js/23.a3d7d66a.js"><link rel="prefetch" href="/notebook/assets/js/230.cfe18f05.js"><link rel="prefetch" href="/notebook/assets/js/231.3a664a46.js"><link rel="prefetch" href="/notebook/assets/js/232.966ce9dc.js"><link rel="prefetch" href="/notebook/assets/js/233.fc06cb57.js"><link rel="prefetch" href="/notebook/assets/js/234.7bb9b7d4.js"><link rel="prefetch" href="/notebook/assets/js/235.b336116e.js"><link rel="prefetch" href="/notebook/assets/js/236.03a38f77.js"><link rel="prefetch" href="/notebook/assets/js/237.0dbda856.js"><link rel="prefetch" href="/notebook/assets/js/238.c1c19749.js"><link rel="prefetch" href="/notebook/assets/js/239.046875c1.js"><link rel="prefetch" href="/notebook/assets/js/24.8e5e267e.js"><link rel="prefetch" href="/notebook/assets/js/240.4bd9cdc0.js"><link rel="prefetch" href="/notebook/assets/js/241.c3dc5804.js"><link rel="prefetch" href="/notebook/assets/js/242.db0b1a91.js"><link rel="prefetch" href="/notebook/assets/js/243.4d9bd61d.js"><link rel="prefetch" href="/notebook/assets/js/244.ee57770b.js"><link rel="prefetch" href="/notebook/assets/js/245.02aab1c1.js"><link rel="prefetch" href="/notebook/assets/js/246.b76a18bb.js"><link rel="prefetch" href="/notebook/assets/js/247.75a673db.js"><link rel="prefetch" href="/notebook/assets/js/248.ad93f81d.js"><link rel="prefetch" href="/notebook/assets/js/249.fb75a938.js"><link rel="prefetch" href="/notebook/assets/js/25.b12f24fe.js"><link rel="prefetch" href="/notebook/assets/js/250.8395c0b6.js"><link rel="prefetch" href="/notebook/assets/js/251.16a6d2a4.js"><link rel="prefetch" href="/notebook/assets/js/252.ef3ee05e.js"><link rel="prefetch" href="/notebook/assets/js/253.78e3471e.js"><link rel="prefetch" href="/notebook/assets/js/254.a5783e07.js"><link rel="prefetch" href="/notebook/assets/js/255.2ab853f6.js"><link rel="prefetch" href="/notebook/assets/js/256.5430831b.js"><link rel="prefetch" href="/notebook/assets/js/257.99c8a0a4.js"><link rel="prefetch" href="/notebook/assets/js/258.4496955b.js"><link rel="prefetch" href="/notebook/assets/js/259.9152b1d2.js"><link rel="prefetch" href="/notebook/assets/js/26.0fce5172.js"><link rel="prefetch" href="/notebook/assets/js/260.072f65e6.js"><link rel="prefetch" href="/notebook/assets/js/261.0bca81af.js"><link rel="prefetch" href="/notebook/assets/js/262.9c9c5337.js"><link rel="prefetch" href="/notebook/assets/js/263.42470957.js"><link rel="prefetch" href="/notebook/assets/js/264.64b5f4fb.js"><link rel="prefetch" href="/notebook/assets/js/265.836a69c5.js"><link rel="prefetch" href="/notebook/assets/js/266.a00cdeb1.js"><link rel="prefetch" href="/notebook/assets/js/267.09dc5ae4.js"><link rel="prefetch" href="/notebook/assets/js/268.6fa6603e.js"><link rel="prefetch" href="/notebook/assets/js/269.3963ce5e.js"><link rel="prefetch" href="/notebook/assets/js/27.47ba3886.js"><link rel="prefetch" href="/notebook/assets/js/270.2826382d.js"><link rel="prefetch" href="/notebook/assets/js/271.3c746c23.js"><link rel="prefetch" href="/notebook/assets/js/272.30698dda.js"><link rel="prefetch" href="/notebook/assets/js/273.b06e3fd2.js"><link rel="prefetch" href="/notebook/assets/js/274.2016c7fa.js"><link rel="prefetch" href="/notebook/assets/js/275.f4aff624.js"><link rel="prefetch" href="/notebook/assets/js/276.e682aa74.js"><link rel="prefetch" href="/notebook/assets/js/277.0c3f41db.js"><link rel="prefetch" href="/notebook/assets/js/278.3c2d5251.js"><link rel="prefetch" href="/notebook/assets/js/279.a9af5703.js"><link rel="prefetch" href="/notebook/assets/js/28.6bac56c6.js"><link rel="prefetch" href="/notebook/assets/js/280.a5da28a3.js"><link rel="prefetch" href="/notebook/assets/js/281.8cc5a3ba.js"><link rel="prefetch" href="/notebook/assets/js/282.55227ff2.js"><link rel="prefetch" href="/notebook/assets/js/283.13f54ae9.js"><link rel="prefetch" href="/notebook/assets/js/284.88644dec.js"><link rel="prefetch" href="/notebook/assets/js/285.0670211f.js"><link rel="prefetch" href="/notebook/assets/js/286.afa43d34.js"><link rel="prefetch" href="/notebook/assets/js/287.9e98e933.js"><link rel="prefetch" href="/notebook/assets/js/288.175a8a9b.js"><link rel="prefetch" href="/notebook/assets/js/289.0d712953.js"><link rel="prefetch" href="/notebook/assets/js/29.3476ca1f.js"><link rel="prefetch" href="/notebook/assets/js/290.4b258761.js"><link rel="prefetch" href="/notebook/assets/js/291.e7ded33e.js"><link rel="prefetch" href="/notebook/assets/js/292.fcfca63e.js"><link rel="prefetch" href="/notebook/assets/js/293.4d6c0f7d.js"><link rel="prefetch" href="/notebook/assets/js/294.59b7e2de.js"><link rel="prefetch" href="/notebook/assets/js/295.0b8dc8f3.js"><link rel="prefetch" href="/notebook/assets/js/296.65434eb0.js"><link rel="prefetch" href="/notebook/assets/js/297.957ba4a7.js"><link rel="prefetch" href="/notebook/assets/js/298.dd81e487.js"><link rel="prefetch" href="/notebook/assets/js/299.eba0d36a.js"><link rel="prefetch" href="/notebook/assets/js/3.a80649d1.js"><link rel="prefetch" href="/notebook/assets/js/30.51a26022.js"><link rel="prefetch" href="/notebook/assets/js/300.23a6a024.js"><link rel="prefetch" href="/notebook/assets/js/301.eb4276c9.js"><link rel="prefetch" href="/notebook/assets/js/302.2c696c44.js"><link rel="prefetch" href="/notebook/assets/js/303.a748a576.js"><link rel="prefetch" href="/notebook/assets/js/304.95020a99.js"><link rel="prefetch" href="/notebook/assets/js/305.c4bc6072.js"><link rel="prefetch" href="/notebook/assets/js/306.74133b05.js"><link rel="prefetch" href="/notebook/assets/js/307.6ea724f3.js"><link rel="prefetch" href="/notebook/assets/js/308.fc7b065c.js"><link rel="prefetch" href="/notebook/assets/js/309.56497801.js"><link rel="prefetch" href="/notebook/assets/js/31.c351e10d.js"><link rel="prefetch" href="/notebook/assets/js/310.692379f9.js"><link rel="prefetch" href="/notebook/assets/js/311.b7393f95.js"><link rel="prefetch" href="/notebook/assets/js/312.f3eec1e1.js"><link rel="prefetch" href="/notebook/assets/js/313.9227351c.js"><link rel="prefetch" href="/notebook/assets/js/314.6960877d.js"><link rel="prefetch" href="/notebook/assets/js/315.f55a1979.js"><link rel="prefetch" href="/notebook/assets/js/316.6121039c.js"><link rel="prefetch" href="/notebook/assets/js/317.7ba118c8.js"><link rel="prefetch" href="/notebook/assets/js/318.2b71444c.js"><link rel="prefetch" href="/notebook/assets/js/319.bc0d5ccf.js"><link rel="prefetch" href="/notebook/assets/js/32.8a802a22.js"><link rel="prefetch" href="/notebook/assets/js/320.79f13ae1.js"><link rel="prefetch" href="/notebook/assets/js/321.21d3b0cf.js"><link rel="prefetch" href="/notebook/assets/js/322.87e4c143.js"><link rel="prefetch" href="/notebook/assets/js/323.4dee2eb7.js"><link rel="prefetch" href="/notebook/assets/js/324.f8c64322.js"><link rel="prefetch" href="/notebook/assets/js/325.c82057d6.js"><link rel="prefetch" href="/notebook/assets/js/326.3ea0d22b.js"><link rel="prefetch" href="/notebook/assets/js/327.90b878d9.js"><link rel="prefetch" href="/notebook/assets/js/328.59e55f0a.js"><link rel="prefetch" href="/notebook/assets/js/329.95fb2ef0.js"><link rel="prefetch" href="/notebook/assets/js/33.18cd7b09.js"><link rel="prefetch" href="/notebook/assets/js/330.ed1fb0e9.js"><link rel="prefetch" href="/notebook/assets/js/331.b84d88a9.js"><link rel="prefetch" href="/notebook/assets/js/332.20dffd14.js"><link rel="prefetch" href="/notebook/assets/js/333.d625fbd2.js"><link rel="prefetch" href="/notebook/assets/js/334.4fedc08a.js"><link rel="prefetch" href="/notebook/assets/js/335.c3b6c886.js"><link rel="prefetch" href="/notebook/assets/js/336.cf000555.js"><link rel="prefetch" href="/notebook/assets/js/337.891a7e6c.js"><link rel="prefetch" href="/notebook/assets/js/338.23da071e.js"><link rel="prefetch" href="/notebook/assets/js/339.92d07729.js"><link rel="prefetch" href="/notebook/assets/js/34.f39f39b2.js"><link rel="prefetch" href="/notebook/assets/js/340.09cb4417.js"><link rel="prefetch" href="/notebook/assets/js/341.3591e649.js"><link rel="prefetch" href="/notebook/assets/js/342.568a9320.js"><link rel="prefetch" href="/notebook/assets/js/343.e61b523f.js"><link rel="prefetch" href="/notebook/assets/js/344.61bb135b.js"><link rel="prefetch" href="/notebook/assets/js/345.f861e5aa.js"><link rel="prefetch" href="/notebook/assets/js/346.c5c70e0f.js"><link rel="prefetch" href="/notebook/assets/js/347.9b389847.js"><link rel="prefetch" href="/notebook/assets/js/348.eb62b86e.js"><link rel="prefetch" href="/notebook/assets/js/349.d4852195.js"><link rel="prefetch" href="/notebook/assets/js/35.c31fd7ed.js"><link rel="prefetch" href="/notebook/assets/js/350.f1db6bfd.js"><link rel="prefetch" href="/notebook/assets/js/351.4d86adaf.js"><link rel="prefetch" href="/notebook/assets/js/36.624192b1.js"><link rel="prefetch" href="/notebook/assets/js/37.680f8e12.js"><link rel="prefetch" href="/notebook/assets/js/38.f9ecec66.js"><link rel="prefetch" href="/notebook/assets/js/39.afab4ce6.js"><link rel="prefetch" href="/notebook/assets/js/4.03ba6111.js"><link rel="prefetch" href="/notebook/assets/js/40.f66ecac0.js"><link rel="prefetch" href="/notebook/assets/js/41.87cdca0e.js"><link rel="prefetch" href="/notebook/assets/js/42.08461558.js"><link rel="prefetch" href="/notebook/assets/js/43.ad5cf182.js"><link rel="prefetch" href="/notebook/assets/js/44.0bb6ad3f.js"><link rel="prefetch" href="/notebook/assets/js/45.5d2af6d4.js"><link rel="prefetch" href="/notebook/assets/js/46.8a06257e.js"><link rel="prefetch" href="/notebook/assets/js/47.3e37541c.js"><link rel="prefetch" href="/notebook/assets/js/48.024eda4c.js"><link rel="prefetch" href="/notebook/assets/js/49.a0685cf7.js"><link rel="prefetch" href="/notebook/assets/js/5.1071c8dd.js"><link rel="prefetch" href="/notebook/assets/js/50.130eaac4.js"><link rel="prefetch" href="/notebook/assets/js/51.0fe4dbd0.js"><link rel="prefetch" href="/notebook/assets/js/52.9d0ae64a.js"><link rel="prefetch" href="/notebook/assets/js/53.1ca09933.js"><link rel="prefetch" href="/notebook/assets/js/54.679cd78c.js"><link rel="prefetch" href="/notebook/assets/js/55.95cbe3a2.js"><link rel="prefetch" href="/notebook/assets/js/56.a58ec2af.js"><link rel="prefetch" href="/notebook/assets/js/57.0e59339a.js"><link rel="prefetch" href="/notebook/assets/js/58.487f643f.js"><link rel="prefetch" href="/notebook/assets/js/59.a8e9a1e3.js"><link rel="prefetch" href="/notebook/assets/js/6.707a1f11.js"><link rel="prefetch" href="/notebook/assets/js/60.c3080f7a.js"><link rel="prefetch" href="/notebook/assets/js/61.7f77e449.js"><link rel="prefetch" href="/notebook/assets/js/62.a5528e33.js"><link rel="prefetch" href="/notebook/assets/js/63.a787a8ee.js"><link rel="prefetch" href="/notebook/assets/js/64.7d3edfda.js"><link rel="prefetch" href="/notebook/assets/js/65.80e083e6.js"><link rel="prefetch" href="/notebook/assets/js/66.4076f29c.js"><link rel="prefetch" href="/notebook/assets/js/67.cf46f254.js"><link rel="prefetch" href="/notebook/assets/js/68.6fc8b1fd.js"><link rel="prefetch" href="/notebook/assets/js/69.4a344d72.js"><link rel="prefetch" href="/notebook/assets/js/7.c507c0e3.js"><link rel="prefetch" href="/notebook/assets/js/70.b13eef1a.js"><link rel="prefetch" href="/notebook/assets/js/71.20ad9776.js"><link rel="prefetch" href="/notebook/assets/js/72.30f44ef6.js"><link rel="prefetch" href="/notebook/assets/js/73.857a629d.js"><link rel="prefetch" href="/notebook/assets/js/74.a2b5a703.js"><link rel="prefetch" href="/notebook/assets/js/75.252e6fc0.js"><link rel="prefetch" href="/notebook/assets/js/76.d64e4a53.js"><link rel="prefetch" href="/notebook/assets/js/77.40db9cc6.js"><link rel="prefetch" href="/notebook/assets/js/78.7a635d12.js"><link rel="prefetch" href="/notebook/assets/js/79.b2249421.js"><link rel="prefetch" href="/notebook/assets/js/8.a5f34392.js"><link rel="prefetch" href="/notebook/assets/js/80.d325f684.js"><link rel="prefetch" href="/notebook/assets/js/81.2e8d667e.js"><link rel="prefetch" href="/notebook/assets/js/82.885af8d1.js"><link rel="prefetch" href="/notebook/assets/js/83.b601bf2e.js"><link rel="prefetch" href="/notebook/assets/js/84.758d5dba.js"><link rel="prefetch" href="/notebook/assets/js/85.2e75fb85.js"><link rel="prefetch" href="/notebook/assets/js/86.6c68d815.js"><link rel="prefetch" href="/notebook/assets/js/88.e30608d9.js"><link rel="prefetch" href="/notebook/assets/js/89.be2f87c8.js"><link rel="prefetch" href="/notebook/assets/js/9.1c775f56.js"><link rel="prefetch" href="/notebook/assets/js/90.88dd69c4.js"><link rel="prefetch" href="/notebook/assets/js/91.59a69041.js"><link rel="prefetch" href="/notebook/assets/js/92.b46ca339.js"><link rel="prefetch" href="/notebook/assets/js/93.aeaec51d.js"><link rel="prefetch" href="/notebook/assets/js/94.5a852633.js"><link rel="prefetch" href="/notebook/assets/js/95.4f445663.js"><link rel="prefetch" href="/notebook/assets/js/96.6299f802.js"><link rel="prefetch" href="/notebook/assets/js/97.6cf3ba23.js"><link rel="prefetch" href="/notebook/assets/js/98.b48d73e6.js"><link rel="prefetch" href="/notebook/assets/js/99.f61a2e23.js">
    <link rel="stylesheet" href="/notebook/assets/css/0.styles.1b58b254.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/notebook/" class="home-link router-link-active"><img src="/notebook/img/logo.png" alt="notebook" class="logo"> <span class="site-name can-hide">notebook</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/head.jpg"> <div class="blogger-info"><h3>学习笔记</h3> <span>啦啦啦，向太阳~</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Posts</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>PyTorch 入门</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>鱼书进阶-自然语言处理</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>深度学习-李宏毅</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/notebook/pages/lhy/regression/" class="sidebar-link">Regression</a></li><li><a href="/notebook/pages/lhy/training-tricks/" class="sidebar-link">神经网络训练不起来怎么办</a></li><li><a href="/notebook/pages/lhy/cnn/" class="sidebar-link">CNN</a></li><li><a href="/notebook/pages/lhy/self-attention/" class="sidebar-link">Self-Attention</a></li><li><a href="/notebook/pages/lhy/various-attention/" class="sidebar-link">各式各样的 Attention</a></li><li><a href="/notebook/pages/lhy/pointer-network/" class="sidebar-link">Pointer Network</a></li><li><a href="/notebook/pages/lhy/gnn/" class="sidebar-link">图神经网络</a></li><li><a href="/notebook/pages/lhy/transformer/" class="sidebar-link">Transformer</a></li><li><a href="/notebook/pages/lhy/gan/" class="sidebar-link">生成对抗网络 GAN</a></li><li><a href="/notebook/pages/lhy/self-supervised-learning/" class="sidebar-link">Self Supervised Learning</a></li><li><a href="/notebook/pages/lhy/bert-and-family/" aria-current="page" class="active sidebar-link">BERT and its family</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/bert-and-family/#_1-what-is-pre-train-model" class="sidebar-link">1. What is pre-train model?</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_1-1-pre-train-model-的历史" class="sidebar-link">1.1 Pre-train Model 的历史</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_1-2-contextualized-word-embedding" class="sidebar-link">1.2 Contextualized Word Embedding</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_1-3-bigger-model" class="sidebar-link">1.3 Bigger Model</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_1-4-smaller-model" class="sidebar-link">1.4 Smaller Model</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/bert-and-family/#_2-how-to-fine-tune" class="sidebar-link">2. How to fine-tune?</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_2-1-nlp-tasks" class="sidebar-link">2.1 NLP tasks</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_2-2-how-to-fine-tune" class="sidebar-link">2.2 How to fine-tune</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_2-3-adaptor" class="sidebar-link">2.3 Adaptor</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_2-4-weighted-features" class="sidebar-link">2.4 Weighted Features</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/bert-and-family/#_3-why-pre-train-models" class="sidebar-link">3. Why Pre-train Models?</a></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/bert-and-family/#_4-how-to-pre-train" class="sidebar-link">4. How to Pre-train?</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-1-pre-training-by-translation" class="sidebar-link">4.1 Pre-training by Translation</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-2-self-supervised-learning" class="sidebar-link">4.2 Self-supervised Learning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-3-predict-next-token" class="sidebar-link">4.3 Predict Next Token</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-4-predict-next-token-bidirectional" class="sidebar-link">4.4 Predict Next Token - Bidirectional</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-5-masking-input" class="sidebar-link">4.5 Masking Input</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-6-span-boundary-objective-sbo" class="sidebar-link">4.6 Span Boundary Objective（SBO）</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-7-xlnet" class="sidebar-link">4.7 XLNet</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-8-mass-bart" class="sidebar-link">4.8 MASS / BART</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-9-unilm" class="sidebar-link">4.9 UniLM</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-10-electra" class="sidebar-link">4.10 ELECTRA</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-11-sentence-level" class="sidebar-link">4.11 Sentence Level</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-12-nsp-sop" class="sidebar-link">4.12 NSP -&gt; SOP</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-13-t5" class="sidebar-link">4.13 T5</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/bert-and-family/#_4-14-others" class="sidebar-link">4.14 Others</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/bert-and-family/#_5-multilingual-bert" class="sidebar-link">5. Multilingual BERT</a></li></ul></li><li><a href="/notebook/pages/lhy/data-efficient/" class="sidebar-link">Data Efficient &amp; Parameter-Efficient Tuning</a></li><li><a href="/notebook/pages/lhy/auto-encoder/" class="sidebar-link">Auto-Encoder</a></li><li><a href="/notebook/pages/lhy/explainable-ml/" class="sidebar-link">机器学习的可解释性</a></li><li><a href="/notebook/pages/lhy/adversarial-attack/" class="sidebar-link">Adversarial Attack</a></li><li><a href="/notebook/pages/lhy/domain-adaptation/" class="sidebar-link">Domain Adaptation</a></li><li><a href="/notebook/pages/lhy/RL/" class="sidebar-link">强化学习</a></li><li><a href="/notebook/pages/lhy/network-compression/" class="sidebar-link">神经网络压缩</a></li><li><a href="/notebook/pages/lhy/life-long-learning/" class="sidebar-link">Life Long Learning</a></li><li><a href="/notebook/pages/lhy/meta-learning/" class="sidebar-link">Meta Learning</a></li><li><a href="/notebook/pages/595df8/" class="sidebar-link">ChatGPT 是怎样炼成的</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2017版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2019版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>预训练语言模型-邵浩2021版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>王树森</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>机器学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>知识图谱</span> <span class="arrow right"></span></p> <!----></section></li></ul> <div class="sidebar-slot sidebar-slot-bottom"><!-- 正方形 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="3508773082"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div></aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/notebook/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/notebook/categories/?category=AI" title="分类" data-v-06225672>AI</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="分类" data-v-06225672>深度学习</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85" title="分类" data-v-06225672>深度学习-李宏毅</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/yubincloud" target="_blank" title="作者" class="beLink" data-v-06225672>yubin</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2022-09-10</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">BERT and its family<!----></h1> <div class="page-slot page-slot-top"><!-- 固定100% * 90px可显示，max-height:90px未见显示-->
     <ins class="adsbygoogle"
          style="display:inline-block;width:100%;max-height:90px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6625304284"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="theme-vdoing-content content__default"><h2 id="_1-what-is-pre-train-model"><a href="#_1-what-is-pre-train-model" class="header-anchor">#</a> 1. What is pre-train model?</h2> <h3 id="_1-1-pre-train-model-的历史"><a href="#_1-1-pre-train-model-的历史" class="header-anchor">#</a> 1.1 Pre-train Model 的历史</h3> <p>以前，Pre-train Model 想要做的是：Represent each token by a embedding vector. 如下图所示：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910092744562.png" alt="image-20220910092744562" style="zoom:67%;"></center> <p>以前所做的往往是 simply a table look-up，也就是静态查表，这有一个问题：The token with the same type has the same embedding.</p> <p>上面所说的技术就有知名的 <mark>Word2vec</mark> [Mikolov,er al., NIPS’13]、<mark>Glove</mark> [Pennington, et al., EMNLP’14]。</p> <p>除此之外还有什么技术呢？</p> <p>如果你考虑的是英文，如果将 English word 作为 token 的话，英文单词实在太多了，这样静态查表总会有找不到的词汇。也许我们可以把 model 改成将英文 character 作为 input，输出就是一个 vector，这就是 <mark>FastText</mark> 做的：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910093518145.png" alt="image-20220910093518145" style="zoom:72%;"></center> <p>如果考虑中文，那每一个中文其实也可以看做一个 image，然后丢到 CNN 里从而输出对应的 vector，这也许也是可以的：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910093735265.png" alt="image-20220910093735265" style="zoom:80%;"></center> <p>上面所说的技术不会考虑到 context，比如同一个“狗”出现在“圈养狗”和“单身狗”中意思不是一样的。</p> <h3 id="_1-2-contextualized-word-embedding"><a href="#_1-2-contextualized-word-embedding" class="header-anchor">#</a> 1.2 Contextualized Word Embedding</h3> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910094424787.png" alt="image-20220910094424787" style="zoom:75%;"></center> <ul><li>Tree-based model 好像没有特别强，所以目前也用的不多。</li></ul> <h3 id="_1-3-bigger-model"><a href="#_1-3-bigger-model" class="header-anchor">#</a> 1.3 Bigger Model</h3> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/1365470-20211026224307862-217905674.png" alt="1365470-20211026224307862-217905674" style="zoom:40%;"></center> <h3 id="_1-4-smaller-model"><a href="#_1-4-smaller-model" class="header-anchor">#</a> 1.4 Smaller Model</h3> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910095704359.png" alt="image-20220910095704359" style="zoom:67%;"></center> <ul><li>这里面很出名的是 ALBERT</li></ul> <p>这些让 BERT 变小的技术是 Network Compression。</p> <p>还有一些在这些模型的 Network Architecture 上也有很多的突破：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910095915587.png" alt="image-20220910095915587" style="zoom:80%;"></center> <ul><li>Transformer-XL 解决了 BERT 一次只能读 512 个 token 的问题</li> <li>Reformer 和 Longformer 是为了减少 self-attention 过程中的运算复杂度</li></ul> <h2 id="_2-how-to-fine-tune"><a href="#_2-how-to-fine-tune" class="header-anchor">#</a> 2. How to fine-tune?</h2> <h3 id="_2-1-nlp-tasks"><a href="#_2-1-nlp-tasks" class="header-anchor">#</a> 2.1 NLP tasks</h3> <p>可以根据 input 和 output 的类型进行一个分类：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910100216657.png" alt="image-20220910100216657" style="zoom:67%;"></center> <h4 id="_2-1-1-input"><a href="#_2-1-1-input" class="header-anchor">#</a> 2.1.1 Input</h4> <p>对于 multiple sentences 的情况，假如有两种不同类型的 sentence，可以在两者之间加一个特殊分隔符 <code>[SEP]</code>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910100807708.png" alt="image-20220910100807708" style="zoom:80%;"></center> <p>然后把它们丢到 model 里面就可以了。</p> <h4 id="_2-1-2-output"><a href="#_2-1-2-output" class="header-anchor">#</a> 2.1.2 Output</h4> <p>output 有多个类型，我们逐一看一下。</p> <h5 id="_1-one-class"><a href="#_1-one-class" class="header-anchor">#</a> 1）one class</h5> <p>一种做法如下图所示：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101027092.png" alt="image-20220910101027092" style="zoom:80%;"></center> <ul><li>输入的时候加一个特别的 token：<code>[CLS]</code>，pretrain 的时候就要告诉 model 当看到 [CLS] 就要产生一个与整个句子有关的 Embedding</li> <li>然后把这个与整个句子有关的 Embedding 丢到一个 task specific model 中得到一个 class，这个 model 什么样要看任务有多复杂，但很多情况下直接是一个 linear transform 就可以了，或者多叠几层 linear transform。</li></ul> <p>还有另外一种做法是将整个句子的 Embedding 都进行处理得到一个 class：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101447795.png" alt="image-20220910101447795" style="zoom:80%;"></center> <h5 id="_2-class-for-each-token"><a href="#_2-class-for-each-token" class="header-anchor">#</a> 2）class for each token</h5> <p>这种就是对每个 token 都给一个 class，做法如下图：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101645179.png" alt="image-20220910101645179" style="zoom:80%;"></center> <ul><li>这里的 task specific model 可以是一个 LSTM，也可以是其他的。</li></ul> <h5 id="_3-copy-from-input"><a href="#_3-copy-from-input" class="header-anchor">#</a> 3）copy from input</h5> <p>完全从 input 做 copy，这种类型的任务也没有很多，其中最经典的就是 Extraction-based QA，这个任务是：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910101915337.png" alt="image-20220910101915337" style="zoom:80%;"></center> <p>原始的 BERT 的 paper 里提供了这种任务的解决方法，可以参考论文。</p> <h5 id="_4-general-sequence"><a href="#_4-general-sequence" class="header-anchor">#</a> 4）general sequence</h5> <p>按照 seq2seq model 的设计，可以这样做：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910102404705.png" alt="image-20220910102404705" style="zoom:80%;"></center> <p>但问题是，作为 decoder 的 task specific model 是完全没有 pretrain 过的，所以这样做也许不是最好的。</p> <p>来看第二种版本：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910102738639.png" alt="image-20220910102738639" style="zoom:70%;"></center> <ul><li>这种做法就是输入 input sequence 后再输入一个 [SEP]，这时 Model 会输出与 [SEP] 相对应的 Embedding Vector，把这个 vector 输入到 task specific model 得到输出 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="3"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，然后接着把 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="3"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 当做 input 得到 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="4"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，一直如此直到得到 <code>&lt;EOS&gt;</code>。</li></ul> <h3 id="_2-2-how-to-fine-tune"><a href="#_2-2-how-to-fine-tune" class="header-anchor">#</a> 2.2 How to fine-tune</h3> <p>假设你有一些 task specific data，该怎样去 fine-tune 呢？这里有两种做法。</p> <p><strong>第一种做法</strong>，固定住 pre-trained model 从而变成一个 feature extractor，然后我们只 fine-tune 那些 task specific 的部分：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910103323315.png" alt="image-20220910103323315" style="zoom:80%;"></center> <p><strong>第二种做法</strong>，把 pre-trained model 和 task-specific model 合在一起 fine-tune，当成一个巨大的 model 来解这个 down-stream task：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910103554759.png" alt="image-20220910103554759" style="zoom:80%;"></center> <p>在以前，巨大 model 在 train 时很容易 overfitting，但第二种做法由于只有 task-specific 的 param 是随机初始化的，pre-trained model 的 param 并不是随机初始化的，所以也许这种方式并没有那么容易 overfitting。在一些文献中指出，<u>第二种做法的 performance 往往比第一种要好一些</u>。</p> <h3 id="_2-3-adaptor"><a href="#_2-3-adaptor" class="header-anchor">#</a> 2.3 Adaptor</h3> <p>刚刚讲的第二种 fine-tune 方法有一个问题，对每一个 task specific 进行 fine-tune 后，pretrained model 会变得不一样：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910105412511.png" alt="image-20220910105412511" style="zoom:70%;"></center> <p>这样每个任务都需要存一个新的 model，而这些 model 往往非常巨大，所以这可能行不通的，因此有了 Adaptor 的概念。也就是说，我们能不能只能调 pretrained model 的一部分就好了，于是我们在 pretrained model 里面加了一个很小的 layer，这些 layer 就叫做 <mark>Adaptor</mark>，下图用 Apt 来简称。这样 fine-tune 时只调整 Pretrained Model 的 Adaptor 部分，这样存储时就只需要存一份 pretrained model 的主体部分，从而减小存储压力：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910105923387.png" alt="image-20220910105923387" style="zoom:70%;"></center> <p>这边举一个使用 Adaptor 的例子，当然还有很多其他做法：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910110135189.png" alt="image-20220910110135189" style="zoom:80%;"></center> <ul><li>pretrain 的时候是没有 Adaptor 的，而是在准备 fine-tune 时才加入并只调整 Adaptor 的参数。</li></ul> <p>这个例子的表现结果如下：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910110516588.png" alt="image-20220910110516588" style="zoom:80%;"></center> <ul><li>Accuracy delta 中的 0 代表当我们 fine-tune 整个 model 时的 performance</li> <li>蓝色这条线表示越往右边微调参数越多</li></ul> <h3 id="_2-4-weighted-features"><a href="#_2-4-weighted-features" class="header-anchor">#</a> 2.4 Weighted Features</h3> <p>我们之前是把 input sequence 后扔进整个 model 后，拿到最后的 Embedding 输给 down-stream task layer 中。但还有一种做法，因为每一层他所抽取的资讯是不一样的，所以一种做法是<strong>把不同层的 feature 给 weighted sum 起来得到一个新的 embedding</strong>，这个 embedding vector 同时综合了多层抽取的资讯，然后再把它丢到 down-stream 中：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910111338745.png" alt="image-20220910111338745" style="zoom:70%;"></center> <p>这里的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="4" class="mjx-n"><mjx-utext variant="normal" style="font-family:serif;">、</mjx-utext></mjx-mo><mjx-msup space="4"><mjx-mi class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="2" class="mjx-n"><mjx-c c="2026"></mjx-c></mjx-mo></mjx-math></mjx-container> 该设为多少呢？它们可以被视为 task specific layer 的参数的一部分从而一起 learn 出来。</p> <h2 id="_3-why-pre-train-models"><a href="#_3-why-pre-train-models" class="header-anchor">#</a> 3. Why Pre-train Models?</h2> <p>GLUE 是检测一个模型了解人类语言的能力：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910135932970.png" alt="image-20220910135932970" style="zoom:75%;"></center> <ul><li>黑色的这条线表示人类理解这个数据集的能力，可以看到后面提出的 pretrain model 都至少可以在这个 corpus 上超越人类的 performance 了。</li></ul> <p>有很多 paper 讨论了为什么 pretrain model 是有效的，这里选其中一篇来讲解。这里选了 arxiv 上 1908.05620 这一篇，分析其中的两个结果</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910140557668.png" alt="image-20220910140557668" style="zoom:75%;"></center> <ul><li>虚线代表没有 pretrain 的 model，实线代表有 pretrain 的 model，它们的 model 大小是一样的。</li> <li>可以看到，有 pretrain 的 model 在 training 时 training loss 下降的非常快。</li></ul> <p>那只看 training loss 是不是有可能是因为 pretrain model 会 overfitting 呢？它在面对新的数据时 generalize 的能力如何呢？我们来看下图：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220910161032304.png" alt="image-20220910161032304" style="zoom:80%;"></center> <ul><li>可以看到，fine-tuning BERT 所处在的 local minima 是一个盆地，而左图是一个峡谷。一般来说，local minima 处在盆地时的泛化能力是要比处在峡谷时要强的。</li></ul> <h2 id="_4-how-to-pre-train"><a href="#_4-how-to-pre-train" class="header-anchor">#</a> 4. How to Pre-train?</h2> <p>这章讲如何得到 pre-train 的模型。</p> <p>我们希望什么样的 pre-train 的 model 呢？我们希望它能把 token sequence 吃进去，然后把每个 token 变成一个 embedding vector，而且希望这些 embedding vector 是 contextulize 的，即考虑上下文的。</p> <h3 id="_4-1-pre-training-by-translation"><a href="#_4-1-pre-training-by-translation" class="header-anchor">#</a> 4.1 Pre-training by Translation</h3> <p>像这种抽取 contextulize 的 embedding 的方法最早是 <mark>CoVe</mark> 这篇 paper，它是通过 translation 而不是现在常用的 unsupervised learning 的方法得到的这个 model：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911105537473.png" alt="image-20220911105537473" style="zoom:70%;"></center> <ul><li>把这个 model 当作是 translation 的 Encoder，最终训练好 Encoder 和 Decoder，这里的 Encoder 就是我们 pretrain 好的 contextulize word embedding 的 model。</li></ul> <p>这篇 paper 选择 translation 也是有合理性的，因为他会把input sequence 里面的资讯都如实呈现到输出里面，而像 summarize 之类的任务就不行。但这种方式的问题是收集大量的 pair data 作为 training data 是很困难的，于是就有了之后的 unsupervised 的方法，现在也常称为 self-supervised 方法。</p> <h3 id="_4-2-self-supervised-learning"><a href="#_4-2-self-supervised-learning" class="header-anchor">#</a> 4.2 Self-supervised Learning</h3> <p>Self-supervised Learning 是 Yann LeCun 在 Twitter 上提出的，其基本概念如下：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911110848667.png" alt="image-20220911110848667" style="zoom:67%;"></center> <h3 id="_4-3-predict-next-token"><a href="#_4-3-predict-next-token" class="header-anchor">#</a> 4.3 Predict Next Token</h3> <p>给这个 model 一个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 得到 representation <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，然后用 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 来预测下一个 token <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>。怎样从 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 预测 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 呢，如下图：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911111325008.png" alt="image-20220911111325008" style="zoom:67%;"></center> <p>这样就用从 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-TeXAtom></mjx-script></mjx-msub></mjx-math></mjx-container> 预测 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container> 这个任务来训练 model：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911111557442.png" alt="image-20220911111557442" style="zoom:63%;"></center> <p>注意，在训练时<strong>不可以</strong>让 model 一次把 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="223C"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="4"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 同时读进去，然后让他预测 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="223C"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="4"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，否则 model 可能直接把下一个 token 给输出出去，从而学不到什么东西。</p> <p>以上所讲的 <mark>Predict Next Token</mark> 任务是最早的 unsupervised pretrain 技术，这样得到的 model 就是一个 language model（<strong>LM</strong>）。</p> <p>这个 model 可以使用 LSTM，也可以使用 Self-attention。使用 LSTM 的 language model 有 ELMo、ULMFiT 等，使用 Self-attention 的有 GPT、Megatron、Turing NLG 等。</p> <p>使用 Self-Attention 作为 model 来做 predict next token 时，注意要给 self-attention 加一个 constraint，不能让它看到后面的，比如 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 位置只能 attend 到 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，而不能 attend 到 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="3"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 及 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="4"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>，从而防止它看到它不该看的以后的答案：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220911112705093.png" alt="image-20220911112705093" style="zoom:67%;"></center> <h3 id="_4-4-predict-next-token-bidirectional"><a href="#_4-4-predict-next-token-bidirectional" class="header-anchor">#</a> 4.4 Predict Next Token - Bidirectional</h3> <p>刚刚讲的最终得到的 contextualize representation 只考通过 predict next token 得到的，而且是只考虑了 left context，并没有考虑 right context。</p> <p><mark>ELMo</mark> 就是同时考虑了 left context 和 right context 用来得到一个 token 的 contextualize representation：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912225943109.png" alt="image-20220912225943109" style="zoom:67%;"></center> <ul><li>它有一个由左向右的 LSTM，看 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="223C"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="4"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>  去预测 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="5"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>；又有一个由右向左的 LSTM，看 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="7"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="223C"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="5"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 来预测 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="4"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container>。</li> <li>然后 ELMo 把正向的 LSTM 和逆向的 LSTM 输出的 vector 给 concatenate 起来，当作代表 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="4"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 的 contextualize representation。</li></ul> <p>所以说 ELMo 通过两个方向的 LSTM 来考虑了一个 token 的 left context 和 right context。但这样还不够，因为 machine 在看 left context 进行 encoding 时没有看到 right context，即两次 encoding 的过程都只是看到句子的一半而非全部，两个 LSTM 得到的 vector 是没有交互的。</p> <h3 id="_4-5-masking-input"><a href="#_4-5-masking-input" class="header-anchor">#</a> 4.5 Masking Input</h3> <p>BERT 弥补了刚刚讲的 ELMo 的 encoding 过程没有左右 context 交互的问题，而且 BERT 做的也不再是 predict next token，而是 masking input：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912231915822.png" alt="image-20220912231915822" style="zoom:67%;"></center> <ul><li>BERT 对 input sequence 的一些 token 给 masking 掉，对 masking 的部分有随机两种做法：一种是替换成一个 special token [MASK]，一种是随机 sample 一个 token 替换到这里。</li> <li>然后用所 masking 掉的部分输出的 vector 来预测 masking 掉的是哪个 token。</li></ul> <p>BERT 用的 Model 是 Transformer Encoder 堆叠起来的。</p> <blockquote><p>其实考古一下的话，Word2Vec 的 CBOW 模型的 training 过程就与 BERT 很像，只不过 BERT 内部结构更加复杂，所考虑的东西也更多了。</p></blockquote> <p>原始 BERT 中要 masking 掉哪些位置是随机决定的，但这不一定很好。具体改进的做法有：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912232846634.png" alt="image-20220912231915822" style="zoom:67%;"></center> <ul><li>上面的 WWM 是 masking 掉一整词，防止出现要预测“黑 [MASK] 江”这种简单的任务；</li> <li>下面是指讲 entity 先识别出来，然后 masking 掉 entity，这么做的就是 ERNIE。</li></ul> <p>还有一种 masking 的方法叫做 <mark>SpanBert</mark>，它就是一次 masking 掉很长的一个范围，原来的 BERT 只是每次随机选一个 token 来 masking 掉。SpanBert 每次盖住多长是有一个几率分布：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912233424453.png" alt="image-20220912233424453" style="zoom:67%;"></center> <p>这篇 paper 也对比多不同类型的 masking 方法的效果：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912233711412.png" alt="image-20220912233711412" style="zoom:80%;"></center> <ul><li>Geometric Spans 就是 SpanBert 提出的 masking 方法。</li></ul> <h3 id="_4-6-span-boundary-objective-sbo"><a href="#_4-6-span-boundary-objective-sbo" class="header-anchor">#</a> 4.6 Span Boundary Objective（SBO）</h3> <p>SpanBert 同时提出了一个新的预训练方法：Span Boundary Objective（<mark>SBO</mark>）。做法如下：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220912234424681.png" alt="image-20220912233424453" style="zoom:67%;"></center> <ul><li>先 masking 一个范围，同时有一个 SBO 的 model，它把这个所 masking 部分的左右两边的 token 吃进去，同时给 SBO model 一个数字 2，这个数字表示要预测所 masking 掉 span 里的第 2 个 token <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="5"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container></li></ul> <p>这个训练方法也许看上去匪夷所思，其实这种设计所期待的是：一个 span 的左右两边的 token 可以包含它内部整个 span 的资讯。为什么这样呢？以后讲到 coreference 时会有用处。</p> <h3 id="_4-7-xlnet"><a href="#_4-7-xlnet" class="header-anchor">#</a> 4.7 XLNet</h3> <p>“XL” 指的是 Transformer-XL，具体是什么可以参考 paper。</p> <p>以往的 predict next token 是根据 left context 去 predict 下一个  token，而 XLNet 是把 input sequence 里面的 token 给随机打乱，比如下图中的下面部分：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913084531392.png" alt="image-20220913084531392" style="zoom:67%;"></center> <ul><li>下面就是把“深度学习”打乱成“习度学深”，然后让 model 根据“习度”去预测“学”。</li></ul> <p>另外，原始 BERT 是根据整个句子的资讯加上 mask 掉的 [MASK] 本身去预测 mask 掉的 token，如下图的上面部分所示。而 XLNet 是只根据 sentence 的一部分而非全部来预测，而且到底是根据哪些部分是随机决定的，还有一个特别的是它不会给 model 看到 mask 的部分（但也会给它 positional information），如下图的下面部分所示：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913085318325.png" alt="image-20220913085318325" style="zoom:67%;"></center> <p>其实 XLNet 为了实现上面所说的，在架构上也做了很多改变，具体可参考原 paper。</p> <h3 id="_4-8-mass-bart"><a href="#_4-8-mass-bart" class="header-anchor">#</a> 4.8 MASS / BART</h3> <p>其实 BERT 本身有点不善言辞，也就是说它不太适合 generation 的任务。而我们想把它用于 sequence-to-sequence，就要让它具有产生句子的能力，也就是”Given paritial sequence, predict the next token“。但 BERT 的训练任务却是从来没有只看 partial sequence 来 predict。</p> <p>下面我们的讨论只局限于 autoregressive model 的情况下，即根据前面的 sequence 来产生下一个 token，由左向右地产生，而不讨论 non-autoregressive 的情况。</p> <p>BERT 本质是一个 encoder，不太适合 seq2seq 的任务，所以我们可以直接 pretain 一个 seq2seq 的 model：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913091212560.png" alt="image-20220913091212560" style="zoom:67%;"></center> <p>但要注意，我们<strong>要对 input 的做某种程度的破坏</strong>，否则这个 seq2seq model 学不到什么东西。怎么来对 input 进行破坏呢，这里有两篇 paper 来探讨这个事情：</p> <ul><li>MAsked Sequence to Sequence pre-training （<mark>MASS</mark>）</li> <li>Bidirectional and Auto-Regressive Transformers （<mark>BART</mark>）</li></ul> <p>那么他俩做的是什么事呢？MASS 的想法与 BERT 很像，就是随机把一些地方 mask 掉，实际上原始 MASS 的 paper 只要求能 reconstruct 一开始 mask 掉的部分就可以了：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913091815205.png" alt="image-20220913091815205.png" style="zoom:67%;"></center> <p>在 BART 里面还提出了各式各样的方法，比如 delete 掉一个 token、做 permutation、做 rotation（即改变起始位置）、做 Text Infilling（随机插一个 [MASK] 来误导 model）：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913092204603.png" alt="image-20220913092204603" style="zoom:63%;"></center> <p>BART 做了这么方法，结果表明：</p> <ul><li>Permutation / Rotation do not perform well.</li> <li>Text Infilling is consistenly good.</li></ul> <h3 id="_4-9-unilm"><a href="#_4-9-unilm" class="header-anchor">#</a> 4.9 UniLM</h3> <p>UniLM 同时是 Encoder 和 Decoder，同时还是一个 seq2seq 的 model：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913092618180.png" alt="image-20220913092618180" style="zoom:67%;"></center> <p>UniLM 不像之前 encoder 和 decoder 拆开，而是只有一个 model 来做三种类型 pretrain：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913125600417.png" alt="image-20220913125600417" style="zoom:67%;"></center> <ul><li>既当作 BERT 一样训练，又当作 GPT 来训练，还当成一个 seq2seq 来训练。</li></ul> <h3 id="_4-10-electra"><a href="#_4-10-electra" class="header-anchor">#</a> 4.10 ELECTRA</h3> <p>ELECTRA 做的是先将一个 sentence 中的某个 token 给 replace 掉，然后让 model 来判断每个输入的 token 是否有被 replace，比如我们将“the chef cooked the meal”中的 “cooked” 给 replace 成 “ate”：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913130327366.png" alt="image-20220913130327366" style="zoom:67%;"></center> <p>但问题来了，怎么把一个词 replace 后可以语法上没有错但语义会怪怪的呢？因为太离谱的 replace 很容易就会被发现从而使 model 学不到什么东西。做法就是加一个 small BERT 来产生用来 replace 的 token：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913130652447.png" alt="image-20220913130652447" style="zoom:67%;"></center> <p>注意，因为下面的这个 small BERT 不是要效果很好，希望它预测后有点错误。而且这不是一个 GAN。</p> <p>神奇的是，ELECTRA 训练的效果还很好：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913131002607.png" alt="image-20220913131002607" style="zoom:67%;"></center> <ul><li>横轴是所需的计算量，纵轴是在 GLUE 上的得分</li></ul> <h3 id="_4-11-sentence-level"><a href="#_4-11-sentence-level" class="header-anchor">#</a> 4.11 Sentence Level</h3> <p>有时候我们希望的不是给一个 token 一个 embedding，而是给一个 sentence 一个 embedding，也就是用一个 global embedding 来表示整个 input 的 token sequence。</p> <p>基于“You shall know a sentence by the conpany it keeps” 的想法，有一个叫做 <mark>Skip Thought</mark> 的想法：有一个 seq2seq 的 model，encoder 读入一个 sentence 变成一个 vector，然后 decoder 用这个 vector 来预测输入 sentence 的下一个 sentence：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913131739146.png" alt="image-20220913131739146" style="zoom:67%;"></center> <p>这样如果两个句子所接的下一句很像，那这两个 sentence 的 embedding 也应该很相近。</p> <p>但我们说过，如果让 model 去生成东西的话，运算量就往往比较大，于是有了一个进阶版：<mark>Quick Thought</mark>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913132033136.png" alt="image-20220913132033136" style="zoom:67%;"></center> <p>这个方法是说，有两个 encoder，吃进去两个 sentence，得到两个 vector，如果两个 sentence 是相邻的话，那得到的两个 vector 越相近越好，反之则越远越好。从而避开了做”生成“这个事情。</p> <h3 id="_4-12-nsp-sop"><a href="#_4-12-nsp-sop" class="header-anchor">#</a> 4.12 NSP -&gt; SOP</h3> <p>原始 BERT 的 pretrain 中有一个任务是 NSP，指的是给 BERT 随便两个 sentence，通过 [CLS] 的输出来让 BERT 告诉我们这两句是否具有上下句关系：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913132901812.png" alt="image-20220913132901812" style="zoom:67%;"></center> <p>后来 RoBERTa 等模型发现说 NSP 没什么用，于是墙倒众人推，很多人都说 NSP 没什么用。于是有了 <mark>SOP</mark>（Sentence order prediction）任务，就是对给的两个相邻但可能颠倒前后顺序的 sentence，model 要告诉我们给的第一句是否是给的第二句的在语义上的前一句，是就回答 Yes，不是就回答 None。这个方法用在了 <mark>ALBERT</mark> 上。</p> <p>之所以 SOP 比 NSP 有用，也许是因为 NSP 这个任务本身很简单，而 SOP 相比较来说更难一些。</p> <h3 id="_4-13-t5"><a href="#_4-13-t5" class="header-anchor">#</a> 4.13 T5</h3> <p>讲了这么多 pretrain 的方法，哪种最好呢？Google 就展现了自己雄厚的财力，把当时几乎所有 pretrain 的方法都做了一遍，发表了 T5 这篇 paper，它长达五十几页。</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913133753191.png" alt="image-20220913133753191" style="zoom:67%;"></center> <ul><li>paper 叫做 T5，用的训练数据集叫做 C4。命名大师！</li></ul> <h3 id="_4-14-others"><a href="#_4-14-others" class="header-anchor">#</a> 4.14 Others</h3> <p>除了刚刚提到的 ERNIE，还有另外一个同名的 ERNIE：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913135117986.png" alt="image-20220913135117986" style="zoom:67%;"></center> <p>另外，还有一种语音版的 BERT，即 Audio BERT：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220913135227152.png" alt="image-20220913135227152" style="zoom:67%;"></center> <h2 id="_5-multilingual-bert"><a href="#_5-multilingual-bert" class="header-anchor">#</a> 5. Multilingual BERT</h2></div></div> <div class="page-slot page-slot-bottom"><!-- 横向自适应 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6620245489"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="page-edit"><div class="edit-link"><a href="https://github.com/yubincloud/notebook/edit/master/docs/AI/01.深度学习/15.深度学习-李宏毅/36.BERT and its family.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2022/10/03, 13:55:17</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/notebook/pages/lhy/self-supervised-learning/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Self Supervised Learning</div></a> <a href="/notebook/pages/lhy/data-efficient/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Data Efficient &amp; Parameter-Efficient Tuning</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/notebook/pages/lhy/self-supervised-learning/" class="prev">Self Supervised Learning</a></span> <span class="next"><a href="/notebook/pages/lhy/data-efficient/">Data Efficient &amp; Parameter-Efficient Tuning</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/notebook/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/notebook/pages/ml/lhy/drl17/"><div>
            Deep Reinforcement Learning
            <!----></div></a> <span class="date">10-03</span></dt></dl><dl><dd>02</dd> <dt><a href="/notebook/pages/mysql/geektime/misdeletion/"><div>
            误删数据后怎么办
            <!----></div></a> <span class="date">04-06</span></dt></dl><dl><dd>03</dd> <dt><a href="/notebook/pages/mysql/geektime/multi-slaves/"><div>
            MySQL 一主多从
            <!----></div></a> <span class="date">03-22</span></dt></dl> <dl><dd></dd> <dt><a href="/notebook/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="yubin_inbuy@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/yubincloud" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2021-2024
    <span>yubincloud | <a href="https://github.com/yubincloud/notebook/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <div class="custom-html-window custom-html-window-rb" style="display:;"><div class="custom-wrapper"><span class="close-but">×</span> <div><!-- 固定160*160px -->
      <ins class="adsbygoogle"
          style="display:inline-block;max-width:160px;max-height:160px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="8377369658"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      </div></div></div></div><div class="global-ui"><div></div></div></div>
    <script src="/notebook/assets/js/app.2bf3b6c1.js" defer></script><script src="/notebook/assets/js/2.0ad58009.js" defer></script><script src="/notebook/assets/js/87.8fba1553.js" defer></script>
  </body>
</html>
