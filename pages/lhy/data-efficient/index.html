<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Data Efficient &amp; Parameter-Efficient Tuning | notebook</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/notebook/img/favicon.ico">
    <script data-ad-client="ca-pub-7828333725993554" async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="学习笔记">
    <meta name="keywords" content="全栈学习笔记">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/notebook/assets/css/0.styles.1b58b254.css" as="style"><link rel="preload" href="/notebook/assets/js/app.2bf3b6c1.js" as="script"><link rel="preload" href="/notebook/assets/js/2.0ad58009.js" as="script"><link rel="preload" href="/notebook/assets/js/88.e30608d9.js" as="script"><link rel="prefetch" href="/notebook/assets/js/10.99522837.js"><link rel="prefetch" href="/notebook/assets/js/100.e3b56889.js"><link rel="prefetch" href="/notebook/assets/js/101.6ea1d00b.js"><link rel="prefetch" href="/notebook/assets/js/102.eca9dfbd.js"><link rel="prefetch" href="/notebook/assets/js/103.6ea477d4.js"><link rel="prefetch" href="/notebook/assets/js/104.ac820d2b.js"><link rel="prefetch" href="/notebook/assets/js/105.58b259a8.js"><link rel="prefetch" href="/notebook/assets/js/106.a86005d0.js"><link rel="prefetch" href="/notebook/assets/js/107.7a79d36f.js"><link rel="prefetch" href="/notebook/assets/js/108.64404e25.js"><link rel="prefetch" href="/notebook/assets/js/109.75f12c0a.js"><link rel="prefetch" href="/notebook/assets/js/11.d26d59e4.js"><link rel="prefetch" href="/notebook/assets/js/110.1155fe36.js"><link rel="prefetch" href="/notebook/assets/js/111.bf8b5871.js"><link rel="prefetch" href="/notebook/assets/js/112.22833ceb.js"><link rel="prefetch" href="/notebook/assets/js/113.6a080233.js"><link rel="prefetch" href="/notebook/assets/js/114.35de9701.js"><link rel="prefetch" href="/notebook/assets/js/115.f598d8c2.js"><link rel="prefetch" href="/notebook/assets/js/116.e3bd29ce.js"><link rel="prefetch" href="/notebook/assets/js/117.c3c02abc.js"><link rel="prefetch" href="/notebook/assets/js/118.136a552a.js"><link rel="prefetch" href="/notebook/assets/js/119.c124f3f8.js"><link rel="prefetch" href="/notebook/assets/js/12.dc66c4f2.js"><link rel="prefetch" href="/notebook/assets/js/120.f835d124.js"><link rel="prefetch" href="/notebook/assets/js/121.367716ae.js"><link rel="prefetch" href="/notebook/assets/js/122.752b0493.js"><link rel="prefetch" href="/notebook/assets/js/123.9f8d6026.js"><link rel="prefetch" href="/notebook/assets/js/124.e8eb61b6.js"><link rel="prefetch" href="/notebook/assets/js/125.cb081200.js"><link rel="prefetch" href="/notebook/assets/js/126.ab87d911.js"><link rel="prefetch" href="/notebook/assets/js/127.ffdbe74d.js"><link rel="prefetch" href="/notebook/assets/js/128.ec526e42.js"><link rel="prefetch" href="/notebook/assets/js/129.71839012.js"><link rel="prefetch" href="/notebook/assets/js/13.32e95b42.js"><link rel="prefetch" href="/notebook/assets/js/130.2bc0bb4d.js"><link rel="prefetch" href="/notebook/assets/js/131.5595b49b.js"><link rel="prefetch" href="/notebook/assets/js/132.4963c5c4.js"><link rel="prefetch" href="/notebook/assets/js/133.44f48cfd.js"><link rel="prefetch" href="/notebook/assets/js/134.cf25626c.js"><link rel="prefetch" href="/notebook/assets/js/135.5ee30fa9.js"><link rel="prefetch" href="/notebook/assets/js/136.bc43f8e6.js"><link rel="prefetch" href="/notebook/assets/js/137.9ab5beac.js"><link rel="prefetch" href="/notebook/assets/js/138.692a33e6.js"><link rel="prefetch" href="/notebook/assets/js/139.08e7c98d.js"><link rel="prefetch" href="/notebook/assets/js/14.c418d170.js"><link rel="prefetch" href="/notebook/assets/js/140.39a861db.js"><link rel="prefetch" href="/notebook/assets/js/141.46678413.js"><link rel="prefetch" href="/notebook/assets/js/142.f7ef5eac.js"><link rel="prefetch" href="/notebook/assets/js/143.c92cbac1.js"><link rel="prefetch" href="/notebook/assets/js/144.d9c61437.js"><link rel="prefetch" href="/notebook/assets/js/145.9f603b31.js"><link rel="prefetch" href="/notebook/assets/js/146.b875f045.js"><link rel="prefetch" href="/notebook/assets/js/147.55e7c4f8.js"><link rel="prefetch" href="/notebook/assets/js/148.4410c365.js"><link rel="prefetch" href="/notebook/assets/js/149.6096ed98.js"><link rel="prefetch" href="/notebook/assets/js/15.e0e7392a.js"><link rel="prefetch" href="/notebook/assets/js/150.24451f07.js"><link rel="prefetch" href="/notebook/assets/js/151.7cff301c.js"><link rel="prefetch" href="/notebook/assets/js/152.035fee1f.js"><link rel="prefetch" href="/notebook/assets/js/153.c61f8ec3.js"><link rel="prefetch" href="/notebook/assets/js/154.7bb549d0.js"><link rel="prefetch" href="/notebook/assets/js/155.1dc494db.js"><link rel="prefetch" href="/notebook/assets/js/156.b87eaf39.js"><link rel="prefetch" href="/notebook/assets/js/157.e3f5a5c0.js"><link rel="prefetch" href="/notebook/assets/js/158.c565c699.js"><link rel="prefetch" href="/notebook/assets/js/159.a22609ef.js"><link rel="prefetch" href="/notebook/assets/js/16.d1aef4ee.js"><link rel="prefetch" href="/notebook/assets/js/160.b29e761c.js"><link rel="prefetch" href="/notebook/assets/js/161.bee1e522.js"><link rel="prefetch" href="/notebook/assets/js/162.c49fca62.js"><link rel="prefetch" href="/notebook/assets/js/163.2cb4d37d.js"><link rel="prefetch" href="/notebook/assets/js/164.4a0dbc64.js"><link rel="prefetch" href="/notebook/assets/js/165.490d05b3.js"><link rel="prefetch" href="/notebook/assets/js/166.df5d2527.js"><link rel="prefetch" href="/notebook/assets/js/167.89a81814.js"><link rel="prefetch" href="/notebook/assets/js/168.9991702e.js"><link rel="prefetch" href="/notebook/assets/js/169.2f9a5dce.js"><link rel="prefetch" href="/notebook/assets/js/17.88ae5445.js"><link rel="prefetch" href="/notebook/assets/js/170.5f23eb3c.js"><link rel="prefetch" href="/notebook/assets/js/171.c521aaa8.js"><link rel="prefetch" href="/notebook/assets/js/172.42110b0a.js"><link rel="prefetch" href="/notebook/assets/js/173.5e36f1bf.js"><link rel="prefetch" href="/notebook/assets/js/174.f48e078a.js"><link rel="prefetch" href="/notebook/assets/js/175.775da6a5.js"><link rel="prefetch" href="/notebook/assets/js/176.9c3c55ea.js"><link rel="prefetch" href="/notebook/assets/js/177.b54d1cff.js"><link rel="prefetch" href="/notebook/assets/js/178.ff08b7f5.js"><link rel="prefetch" href="/notebook/assets/js/179.c6a1af32.js"><link rel="prefetch" href="/notebook/assets/js/18.dcb78196.js"><link rel="prefetch" href="/notebook/assets/js/180.25dd9eba.js"><link rel="prefetch" href="/notebook/assets/js/181.13e6ec84.js"><link rel="prefetch" href="/notebook/assets/js/182.f6849f0d.js"><link rel="prefetch" href="/notebook/assets/js/183.7e664874.js"><link rel="prefetch" href="/notebook/assets/js/184.e6aba86f.js"><link rel="prefetch" href="/notebook/assets/js/185.df07b919.js"><link rel="prefetch" href="/notebook/assets/js/186.02c77e75.js"><link rel="prefetch" href="/notebook/assets/js/187.e8380ed4.js"><link rel="prefetch" href="/notebook/assets/js/188.eddc8bee.js"><link rel="prefetch" href="/notebook/assets/js/189.fbc1840f.js"><link rel="prefetch" href="/notebook/assets/js/19.5997a514.js"><link rel="prefetch" href="/notebook/assets/js/190.a37bfe4c.js"><link rel="prefetch" href="/notebook/assets/js/191.e53a3d4b.js"><link rel="prefetch" href="/notebook/assets/js/192.2f5be408.js"><link rel="prefetch" href="/notebook/assets/js/193.4ca6de49.js"><link rel="prefetch" href="/notebook/assets/js/194.b8e51d9d.js"><link rel="prefetch" href="/notebook/assets/js/195.70e6b23a.js"><link rel="prefetch" href="/notebook/assets/js/196.5d5fbf2d.js"><link rel="prefetch" href="/notebook/assets/js/197.78456dab.js"><link rel="prefetch" href="/notebook/assets/js/198.4308331c.js"><link rel="prefetch" href="/notebook/assets/js/199.2e537849.js"><link rel="prefetch" href="/notebook/assets/js/20.fc057fd7.js"><link rel="prefetch" href="/notebook/assets/js/200.b3309bbf.js"><link rel="prefetch" href="/notebook/assets/js/201.4723461c.js"><link rel="prefetch" href="/notebook/assets/js/202.b15b5177.js"><link rel="prefetch" href="/notebook/assets/js/203.22c50e61.js"><link rel="prefetch" href="/notebook/assets/js/204.5b8b3b00.js"><link rel="prefetch" href="/notebook/assets/js/205.54ee7630.js"><link rel="prefetch" href="/notebook/assets/js/206.f3f20f94.js"><link rel="prefetch" href="/notebook/assets/js/207.a9608973.js"><link rel="prefetch" href="/notebook/assets/js/208.1a80a593.js"><link rel="prefetch" href="/notebook/assets/js/209.586fd293.js"><link rel="prefetch" href="/notebook/assets/js/21.cb4205ee.js"><link rel="prefetch" href="/notebook/assets/js/210.7829dd53.js"><link rel="prefetch" href="/notebook/assets/js/211.3ce139ab.js"><link rel="prefetch" href="/notebook/assets/js/212.84738a64.js"><link rel="prefetch" href="/notebook/assets/js/213.a631830d.js"><link rel="prefetch" href="/notebook/assets/js/214.9d64cf85.js"><link rel="prefetch" href="/notebook/assets/js/215.87030b6b.js"><link rel="prefetch" href="/notebook/assets/js/216.ddbe1944.js"><link rel="prefetch" href="/notebook/assets/js/217.16ae7e40.js"><link rel="prefetch" href="/notebook/assets/js/218.e7780d65.js"><link rel="prefetch" href="/notebook/assets/js/219.abae5e09.js"><link rel="prefetch" href="/notebook/assets/js/22.256014b4.js"><link rel="prefetch" href="/notebook/assets/js/220.8e3a8702.js"><link rel="prefetch" href="/notebook/assets/js/221.4c279d74.js"><link rel="prefetch" href="/notebook/assets/js/222.6c9b2595.js"><link rel="prefetch" href="/notebook/assets/js/223.cc072424.js"><link rel="prefetch" href="/notebook/assets/js/224.c663b40f.js"><link rel="prefetch" href="/notebook/assets/js/225.f3e52654.js"><link rel="prefetch" href="/notebook/assets/js/226.5e00402c.js"><link rel="prefetch" href="/notebook/assets/js/227.1c28ce97.js"><link rel="prefetch" href="/notebook/assets/js/228.42b8c305.js"><link rel="prefetch" href="/notebook/assets/js/229.df9760ec.js"><link rel="prefetch" href="/notebook/assets/js/23.a3d7d66a.js"><link rel="prefetch" href="/notebook/assets/js/230.cfe18f05.js"><link rel="prefetch" href="/notebook/assets/js/231.3a664a46.js"><link rel="prefetch" href="/notebook/assets/js/232.966ce9dc.js"><link rel="prefetch" href="/notebook/assets/js/233.fc06cb57.js"><link rel="prefetch" href="/notebook/assets/js/234.7bb9b7d4.js"><link rel="prefetch" href="/notebook/assets/js/235.b336116e.js"><link rel="prefetch" href="/notebook/assets/js/236.03a38f77.js"><link rel="prefetch" href="/notebook/assets/js/237.0dbda856.js"><link rel="prefetch" href="/notebook/assets/js/238.c1c19749.js"><link rel="prefetch" href="/notebook/assets/js/239.046875c1.js"><link rel="prefetch" href="/notebook/assets/js/24.8e5e267e.js"><link rel="prefetch" href="/notebook/assets/js/240.4bd9cdc0.js"><link rel="prefetch" href="/notebook/assets/js/241.c3dc5804.js"><link rel="prefetch" href="/notebook/assets/js/242.db0b1a91.js"><link rel="prefetch" href="/notebook/assets/js/243.4d9bd61d.js"><link rel="prefetch" href="/notebook/assets/js/244.ee57770b.js"><link rel="prefetch" href="/notebook/assets/js/245.02aab1c1.js"><link rel="prefetch" href="/notebook/assets/js/246.b76a18bb.js"><link rel="prefetch" href="/notebook/assets/js/247.75a673db.js"><link rel="prefetch" href="/notebook/assets/js/248.ad93f81d.js"><link rel="prefetch" href="/notebook/assets/js/249.fb75a938.js"><link rel="prefetch" href="/notebook/assets/js/25.b12f24fe.js"><link rel="prefetch" href="/notebook/assets/js/250.8395c0b6.js"><link rel="prefetch" href="/notebook/assets/js/251.16a6d2a4.js"><link rel="prefetch" href="/notebook/assets/js/252.ef3ee05e.js"><link rel="prefetch" href="/notebook/assets/js/253.78e3471e.js"><link rel="prefetch" href="/notebook/assets/js/254.a5783e07.js"><link rel="prefetch" href="/notebook/assets/js/255.2ab853f6.js"><link rel="prefetch" href="/notebook/assets/js/256.5430831b.js"><link rel="prefetch" href="/notebook/assets/js/257.99c8a0a4.js"><link rel="prefetch" href="/notebook/assets/js/258.4496955b.js"><link rel="prefetch" href="/notebook/assets/js/259.9152b1d2.js"><link rel="prefetch" href="/notebook/assets/js/26.0fce5172.js"><link rel="prefetch" href="/notebook/assets/js/260.072f65e6.js"><link rel="prefetch" href="/notebook/assets/js/261.0bca81af.js"><link rel="prefetch" href="/notebook/assets/js/262.9c9c5337.js"><link rel="prefetch" href="/notebook/assets/js/263.42470957.js"><link rel="prefetch" href="/notebook/assets/js/264.64b5f4fb.js"><link rel="prefetch" href="/notebook/assets/js/265.836a69c5.js"><link rel="prefetch" href="/notebook/assets/js/266.a00cdeb1.js"><link rel="prefetch" href="/notebook/assets/js/267.09dc5ae4.js"><link rel="prefetch" href="/notebook/assets/js/268.6fa6603e.js"><link rel="prefetch" href="/notebook/assets/js/269.3963ce5e.js"><link rel="prefetch" href="/notebook/assets/js/27.47ba3886.js"><link rel="prefetch" href="/notebook/assets/js/270.2826382d.js"><link rel="prefetch" href="/notebook/assets/js/271.3c746c23.js"><link rel="prefetch" href="/notebook/assets/js/272.30698dda.js"><link rel="prefetch" href="/notebook/assets/js/273.b06e3fd2.js"><link rel="prefetch" href="/notebook/assets/js/274.2016c7fa.js"><link rel="prefetch" href="/notebook/assets/js/275.f4aff624.js"><link rel="prefetch" href="/notebook/assets/js/276.e682aa74.js"><link rel="prefetch" href="/notebook/assets/js/277.0c3f41db.js"><link rel="prefetch" href="/notebook/assets/js/278.3c2d5251.js"><link rel="prefetch" href="/notebook/assets/js/279.a9af5703.js"><link rel="prefetch" href="/notebook/assets/js/28.6bac56c6.js"><link rel="prefetch" href="/notebook/assets/js/280.a5da28a3.js"><link rel="prefetch" href="/notebook/assets/js/281.8cc5a3ba.js"><link rel="prefetch" href="/notebook/assets/js/282.55227ff2.js"><link rel="prefetch" href="/notebook/assets/js/283.13f54ae9.js"><link rel="prefetch" href="/notebook/assets/js/284.88644dec.js"><link rel="prefetch" href="/notebook/assets/js/285.0670211f.js"><link rel="prefetch" href="/notebook/assets/js/286.afa43d34.js"><link rel="prefetch" href="/notebook/assets/js/287.9e98e933.js"><link rel="prefetch" href="/notebook/assets/js/288.175a8a9b.js"><link rel="prefetch" href="/notebook/assets/js/289.0d712953.js"><link rel="prefetch" href="/notebook/assets/js/29.3476ca1f.js"><link rel="prefetch" href="/notebook/assets/js/290.4b258761.js"><link rel="prefetch" href="/notebook/assets/js/291.e7ded33e.js"><link rel="prefetch" href="/notebook/assets/js/292.fcfca63e.js"><link rel="prefetch" href="/notebook/assets/js/293.4d6c0f7d.js"><link rel="prefetch" href="/notebook/assets/js/294.59b7e2de.js"><link rel="prefetch" href="/notebook/assets/js/295.0b8dc8f3.js"><link rel="prefetch" href="/notebook/assets/js/296.65434eb0.js"><link rel="prefetch" href="/notebook/assets/js/297.957ba4a7.js"><link rel="prefetch" href="/notebook/assets/js/298.dd81e487.js"><link rel="prefetch" href="/notebook/assets/js/299.eba0d36a.js"><link rel="prefetch" href="/notebook/assets/js/3.a80649d1.js"><link rel="prefetch" href="/notebook/assets/js/30.51a26022.js"><link rel="prefetch" href="/notebook/assets/js/300.23a6a024.js"><link rel="prefetch" href="/notebook/assets/js/301.eb4276c9.js"><link rel="prefetch" href="/notebook/assets/js/302.2c696c44.js"><link rel="prefetch" href="/notebook/assets/js/303.a748a576.js"><link rel="prefetch" href="/notebook/assets/js/304.95020a99.js"><link rel="prefetch" href="/notebook/assets/js/305.c4bc6072.js"><link rel="prefetch" href="/notebook/assets/js/306.74133b05.js"><link rel="prefetch" href="/notebook/assets/js/307.6ea724f3.js"><link rel="prefetch" href="/notebook/assets/js/308.fc7b065c.js"><link rel="prefetch" href="/notebook/assets/js/309.56497801.js"><link rel="prefetch" href="/notebook/assets/js/31.c351e10d.js"><link rel="prefetch" href="/notebook/assets/js/310.692379f9.js"><link rel="prefetch" href="/notebook/assets/js/311.b7393f95.js"><link rel="prefetch" href="/notebook/assets/js/312.f3eec1e1.js"><link rel="prefetch" href="/notebook/assets/js/313.9227351c.js"><link rel="prefetch" href="/notebook/assets/js/314.6960877d.js"><link rel="prefetch" href="/notebook/assets/js/315.f55a1979.js"><link rel="prefetch" href="/notebook/assets/js/316.6121039c.js"><link rel="prefetch" href="/notebook/assets/js/317.7ba118c8.js"><link rel="prefetch" href="/notebook/assets/js/318.2b71444c.js"><link rel="prefetch" href="/notebook/assets/js/319.bc0d5ccf.js"><link rel="prefetch" href="/notebook/assets/js/32.8a802a22.js"><link rel="prefetch" href="/notebook/assets/js/320.79f13ae1.js"><link rel="prefetch" href="/notebook/assets/js/321.21d3b0cf.js"><link rel="prefetch" href="/notebook/assets/js/322.87e4c143.js"><link rel="prefetch" href="/notebook/assets/js/323.4dee2eb7.js"><link rel="prefetch" href="/notebook/assets/js/324.f8c64322.js"><link rel="prefetch" href="/notebook/assets/js/325.c82057d6.js"><link rel="prefetch" href="/notebook/assets/js/326.3ea0d22b.js"><link rel="prefetch" href="/notebook/assets/js/327.90b878d9.js"><link rel="prefetch" href="/notebook/assets/js/328.59e55f0a.js"><link rel="prefetch" href="/notebook/assets/js/329.95fb2ef0.js"><link rel="prefetch" href="/notebook/assets/js/33.18cd7b09.js"><link rel="prefetch" href="/notebook/assets/js/330.ed1fb0e9.js"><link rel="prefetch" href="/notebook/assets/js/331.b84d88a9.js"><link rel="prefetch" href="/notebook/assets/js/332.20dffd14.js"><link rel="prefetch" href="/notebook/assets/js/333.d625fbd2.js"><link rel="prefetch" href="/notebook/assets/js/334.4fedc08a.js"><link rel="prefetch" href="/notebook/assets/js/335.c3b6c886.js"><link rel="prefetch" href="/notebook/assets/js/336.cf000555.js"><link rel="prefetch" href="/notebook/assets/js/337.891a7e6c.js"><link rel="prefetch" href="/notebook/assets/js/338.23da071e.js"><link rel="prefetch" href="/notebook/assets/js/339.92d07729.js"><link rel="prefetch" href="/notebook/assets/js/34.f39f39b2.js"><link rel="prefetch" href="/notebook/assets/js/340.09cb4417.js"><link rel="prefetch" href="/notebook/assets/js/341.3591e649.js"><link rel="prefetch" href="/notebook/assets/js/342.568a9320.js"><link rel="prefetch" href="/notebook/assets/js/343.e61b523f.js"><link rel="prefetch" href="/notebook/assets/js/344.61bb135b.js"><link rel="prefetch" href="/notebook/assets/js/345.f861e5aa.js"><link rel="prefetch" href="/notebook/assets/js/346.c5c70e0f.js"><link rel="prefetch" href="/notebook/assets/js/347.9b389847.js"><link rel="prefetch" href="/notebook/assets/js/348.eb62b86e.js"><link rel="prefetch" href="/notebook/assets/js/349.d4852195.js"><link rel="prefetch" href="/notebook/assets/js/35.c31fd7ed.js"><link rel="prefetch" href="/notebook/assets/js/350.f1db6bfd.js"><link rel="prefetch" href="/notebook/assets/js/351.4d86adaf.js"><link rel="prefetch" href="/notebook/assets/js/36.624192b1.js"><link rel="prefetch" href="/notebook/assets/js/37.680f8e12.js"><link rel="prefetch" href="/notebook/assets/js/38.f9ecec66.js"><link rel="prefetch" href="/notebook/assets/js/39.afab4ce6.js"><link rel="prefetch" href="/notebook/assets/js/4.03ba6111.js"><link rel="prefetch" href="/notebook/assets/js/40.f66ecac0.js"><link rel="prefetch" href="/notebook/assets/js/41.87cdca0e.js"><link rel="prefetch" href="/notebook/assets/js/42.08461558.js"><link rel="prefetch" href="/notebook/assets/js/43.ad5cf182.js"><link rel="prefetch" href="/notebook/assets/js/44.0bb6ad3f.js"><link rel="prefetch" href="/notebook/assets/js/45.5d2af6d4.js"><link rel="prefetch" href="/notebook/assets/js/46.8a06257e.js"><link rel="prefetch" href="/notebook/assets/js/47.3e37541c.js"><link rel="prefetch" href="/notebook/assets/js/48.024eda4c.js"><link rel="prefetch" href="/notebook/assets/js/49.a0685cf7.js"><link rel="prefetch" href="/notebook/assets/js/5.1071c8dd.js"><link rel="prefetch" href="/notebook/assets/js/50.130eaac4.js"><link rel="prefetch" href="/notebook/assets/js/51.0fe4dbd0.js"><link rel="prefetch" href="/notebook/assets/js/52.9d0ae64a.js"><link rel="prefetch" href="/notebook/assets/js/53.1ca09933.js"><link rel="prefetch" href="/notebook/assets/js/54.679cd78c.js"><link rel="prefetch" href="/notebook/assets/js/55.95cbe3a2.js"><link rel="prefetch" href="/notebook/assets/js/56.a58ec2af.js"><link rel="prefetch" href="/notebook/assets/js/57.0e59339a.js"><link rel="prefetch" href="/notebook/assets/js/58.487f643f.js"><link rel="prefetch" href="/notebook/assets/js/59.a8e9a1e3.js"><link rel="prefetch" href="/notebook/assets/js/6.707a1f11.js"><link rel="prefetch" href="/notebook/assets/js/60.c3080f7a.js"><link rel="prefetch" href="/notebook/assets/js/61.7f77e449.js"><link rel="prefetch" href="/notebook/assets/js/62.a5528e33.js"><link rel="prefetch" href="/notebook/assets/js/63.a787a8ee.js"><link rel="prefetch" href="/notebook/assets/js/64.7d3edfda.js"><link rel="prefetch" href="/notebook/assets/js/65.80e083e6.js"><link rel="prefetch" href="/notebook/assets/js/66.4076f29c.js"><link rel="prefetch" href="/notebook/assets/js/67.cf46f254.js"><link rel="prefetch" href="/notebook/assets/js/68.6fc8b1fd.js"><link rel="prefetch" href="/notebook/assets/js/69.4a344d72.js"><link rel="prefetch" href="/notebook/assets/js/7.c507c0e3.js"><link rel="prefetch" href="/notebook/assets/js/70.b13eef1a.js"><link rel="prefetch" href="/notebook/assets/js/71.20ad9776.js"><link rel="prefetch" href="/notebook/assets/js/72.30f44ef6.js"><link rel="prefetch" href="/notebook/assets/js/73.857a629d.js"><link rel="prefetch" href="/notebook/assets/js/74.a2b5a703.js"><link rel="prefetch" href="/notebook/assets/js/75.252e6fc0.js"><link rel="prefetch" href="/notebook/assets/js/76.d64e4a53.js"><link rel="prefetch" href="/notebook/assets/js/77.40db9cc6.js"><link rel="prefetch" href="/notebook/assets/js/78.7a635d12.js"><link rel="prefetch" href="/notebook/assets/js/79.b2249421.js"><link rel="prefetch" href="/notebook/assets/js/8.a5f34392.js"><link rel="prefetch" href="/notebook/assets/js/80.d325f684.js"><link rel="prefetch" href="/notebook/assets/js/81.2e8d667e.js"><link rel="prefetch" href="/notebook/assets/js/82.885af8d1.js"><link rel="prefetch" href="/notebook/assets/js/83.b601bf2e.js"><link rel="prefetch" href="/notebook/assets/js/84.758d5dba.js"><link rel="prefetch" href="/notebook/assets/js/85.2e75fb85.js"><link rel="prefetch" href="/notebook/assets/js/86.6c68d815.js"><link rel="prefetch" href="/notebook/assets/js/87.8fba1553.js"><link rel="prefetch" href="/notebook/assets/js/89.be2f87c8.js"><link rel="prefetch" href="/notebook/assets/js/9.1c775f56.js"><link rel="prefetch" href="/notebook/assets/js/90.88dd69c4.js"><link rel="prefetch" href="/notebook/assets/js/91.59a69041.js"><link rel="prefetch" href="/notebook/assets/js/92.b46ca339.js"><link rel="prefetch" href="/notebook/assets/js/93.aeaec51d.js"><link rel="prefetch" href="/notebook/assets/js/94.5a852633.js"><link rel="prefetch" href="/notebook/assets/js/95.4f445663.js"><link rel="prefetch" href="/notebook/assets/js/96.6299f802.js"><link rel="prefetch" href="/notebook/assets/js/97.6cf3ba23.js"><link rel="prefetch" href="/notebook/assets/js/98.b48d73e6.js"><link rel="prefetch" href="/notebook/assets/js/99.f61a2e23.js">
    <link rel="stylesheet" href="/notebook/assets/css/0.styles.1b58b254.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/notebook/" class="home-link router-link-active"><img src="/notebook/img/logo.png" alt="notebook" class="logo"> <span class="site-name can-hide">notebook</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/head.jpg"> <div class="blogger-info"><h3>学习笔记</h3> <span>啦啦啦，向太阳~</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Posts</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>PyTorch 入门</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>鱼书进阶-自然语言处理</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>深度学习-李宏毅</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/notebook/pages/lhy/regression/" class="sidebar-link">Regression</a></li><li><a href="/notebook/pages/lhy/training-tricks/" class="sidebar-link">神经网络训练不起来怎么办</a></li><li><a href="/notebook/pages/lhy/cnn/" class="sidebar-link">CNN</a></li><li><a href="/notebook/pages/lhy/self-attention/" class="sidebar-link">Self-Attention</a></li><li><a href="/notebook/pages/lhy/various-attention/" class="sidebar-link">各式各样的 Attention</a></li><li><a href="/notebook/pages/lhy/pointer-network/" class="sidebar-link">Pointer Network</a></li><li><a href="/notebook/pages/lhy/gnn/" class="sidebar-link">图神经网络</a></li><li><a href="/notebook/pages/lhy/transformer/" class="sidebar-link">Transformer</a></li><li><a href="/notebook/pages/lhy/gan/" class="sidebar-link">生成对抗网络 GAN</a></li><li><a href="/notebook/pages/lhy/self-supervised-learning/" class="sidebar-link">Self Supervised Learning</a></li><li><a href="/notebook/pages/lhy/bert-and-family/" class="sidebar-link">BERT and its family</a></li><li><a href="/notebook/pages/lhy/data-efficient/" aria-current="page" class="active sidebar-link">Data Efficient &amp; Parameter-Efficient Tuning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/data-efficient/#_1-background-pre-trained-language-models" class="sidebar-link">1. Background: Pre-trained Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_1-1-什么是-language-model" class="sidebar-link">1.1 什么是 Language Model？</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_1-2-怎样训练的这些-model" class="sidebar-link">1.2 怎样训练的这些 model？</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_1-3-pre-trained-language-models-plms" class="sidebar-link">1.3 Pre-trained Language Models（PLMs）</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/data-efficient/#_2-the-problem-of-plms" class="sidebar-link">2. The problem of PLMs</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_2-1-problem-1-data-scarcity-in-downstream-tasks" class="sidebar-link">2.1 Problem 1:  in downstream tasks</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_2-2-problem-2-the-plm-is-too-big" class="sidebar-link">2.2 Problem 2: The PLM is too big</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/data-efficient/#_3-labeled-data-scarcity-data-efficient-fine-tuning" class="sidebar-link">3. Labeled Data Scarcity -&gt; Data-Efficient Fine-tuning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_3-1-prompt-tuning" class="sidebar-link">3.1 Prompt Tuning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_3-2-few-shot-learning" class="sidebar-link">3.2 Few-shot Learning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_3-3-semi-supervised-learning" class="sidebar-link">3.3 Semi-supervised Learning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_3-4-zero-shot-learning" class="sidebar-link">3.4 Zero-shot Learning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_3-5-summary" class="sidebar-link">3.5 Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/data-efficient/#_4-plms-are-gigantic-reducing-the-number-of-parameters" class="sidebar-link">4. PLMs Are Gigantic -&gt; Reducing the Number of Parameters</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-1-pre-train-a-large-model-but-use-a-smaller-model-for-the-downstream-tasks" class="sidebar-link">4.1 Pre-train a large model, but use a smaller model for the downstream tasks.</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-2-share-the-parameters-among-the-transformer-layers-albert" class="sidebar-link">4.2 Share the parameters among the transformer layers: ALBERT</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-3-parameter-efficient-fine-tuning" class="sidebar-link">4.3 Parameter-Efficient Fine-tuning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-4-parameter-efficient-fine-tuning-adapter" class="sidebar-link">4.4 Parameter-Efficient Fine-tuning: Adapter</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-5-parameter-efficient-fine-tuning-lora" class="sidebar-link">4.5 Parameter-Efficient Fine-tuning: LoRA</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-6-parameter-efficient-fine-tuning-prefix-tuning" class="sidebar-link">4.6 Parameter-Efficient Fine-tuning: Prefix Tuning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-7-parameter-efficient-fine-tuning-soft-prompting" class="sidebar-link">4.7 Parameter-Efficient Fine-tuning: Soft Prompting</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-8-总结-parameter-efficient-fine-tuning" class="sidebar-link">4.8 总结 Parameter-Efficient Fine-tuning</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-9-early-exit" class="sidebar-link">4.9 Early Exit</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/lhy/data-efficient/#_4-10-summary" class="sidebar-link">4.10 Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/lhy/data-efficient/#_5-closing-remarks" class="sidebar-link">5. Closing Remarks</a></li></ul></li><li><a href="/notebook/pages/lhy/auto-encoder/" class="sidebar-link">Auto-Encoder</a></li><li><a href="/notebook/pages/lhy/explainable-ml/" class="sidebar-link">机器学习的可解释性</a></li><li><a href="/notebook/pages/lhy/adversarial-attack/" class="sidebar-link">Adversarial Attack</a></li><li><a href="/notebook/pages/lhy/domain-adaptation/" class="sidebar-link">Domain Adaptation</a></li><li><a href="/notebook/pages/lhy/RL/" class="sidebar-link">强化学习</a></li><li><a href="/notebook/pages/lhy/network-compression/" class="sidebar-link">神经网络压缩</a></li><li><a href="/notebook/pages/lhy/life-long-learning/" class="sidebar-link">Life Long Learning</a></li><li><a href="/notebook/pages/lhy/meta-learning/" class="sidebar-link">Meta Learning</a></li><li><a href="/notebook/pages/595df8/" class="sidebar-link">ChatGPT 是怎样炼成的</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2017版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2019版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>预训练语言模型-邵浩2021版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>王树森</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>机器学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>知识图谱</span> <span class="arrow right"></span></p> <!----></section></li></ul> <div class="sidebar-slot sidebar-slot-bottom"><!-- 正方形 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="3508773082"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div></aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/notebook/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/notebook/categories/?category=AI" title="分类" data-v-06225672>AI</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="分类" data-v-06225672>深度学习</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85" title="分类" data-v-06225672>深度学习-李宏毅</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/yubincloud" target="_blank" title="作者" class="beLink" data-v-06225672>yubin</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2022-10-29</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Data Efficient &amp; Parameter-Efficient Tuning<!----></h1> <div class="page-slot page-slot-top"><!-- 固定100% * 90px可显示，max-height:90px未见显示-->
     <ins class="adsbygoogle"
          style="display:inline-block;width:100%;max-height:90px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6625304284"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="theme-vdoing-content content__default"><h2 id="_1-background-pre-trained-language-models"><a href="#_1-background-pre-trained-language-models" class="header-anchor">#</a> 1. Background: Pre-trained Language Models</h2> <h3 id="_1-1-什么是-language-model"><a href="#_1-1-什么是-language-model" class="header-anchor">#</a> 1.1 什么是 Language Model？</h3> <p><mark>Neural Language Models</mark>: A neural network that defines the probability over sequences of words.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210149304.png" alt="image-20221029210149304" style="zoom:90%;"> <h3 id="_1-2-怎样训练的这些-model"><a href="#_1-2-怎样训练的这些-model" class="header-anchor">#</a> 1.2 怎样训练的这些 model？</h3> <p>Given an incomplete sentence, predict the rest of the sentence.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210325093.png" alt="image-20221029210325093" style="zoom:90%;"> <p>不完整的句子怎么构造呢？根据不完整的句子的构造方式，可以将 Language Model 的训练分成两种：</p> <ul><li><strong>Autoregressive Language Model</strong>（<mark>ALMs</mark>）: Complete the sentence given its prefix.</li></ul> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210716452.png" alt="image-20221029210716452" style="zoom:80%;"> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210806588.png" alt="image-20221029210806588" style="zoom:80%;"> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029210806588.png" alt="image-20221029210806588" style="zoom:80%;"> <p>我们看一下 Transformer-based PLM 长什么样子：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029211126556.png" alt="image-20221029211126556" style="zoom:90%;"></center> <p>上图中，“气”这个字经过一系列 layer，得到了它的 embedding，然后把这个 embedding 输入到一个 LM Head 中，可以得到预测下一个 token 的概率。</p> <p>训练一个 Language Model 的方式就是 self-supervised learning，但它没有一个明确的定义，这里我们说：</p> <p><mark>Self-supervised learning</mark>: Predicting any part of the input from any other part.</p> <p>还存在另外一种 Language Model，即 Masked Language Models（MLMs）：</p> <p><strong>Masked Language Models</strong>（<mark>MLMs</mark>）: Use the unmarked words to predict the masked word.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029211842262.png" alt="image-20221029211842262" style="zoom:80%;"> <h3 id="_1-3-pre-trained-language-models-plms"><a href="#_1-3-pre-trained-language-models-plms" class="header-anchor">#</a> 1.3 Pre-trained Language Models（PLMs）</h3> <p><strong>Pre</strong>-training: Using a large corpora to train a neural language model.</p> <ul><li>Autoregressive pre-trained: GPT 系列（GPT, GPT-2, GPT-3）</li> <li>MLM-based pre-trained: BERT 系列（BERT, RoBERTa, ALBERT）</li></ul> <p>为什么要这样做呢？We believe that after pre-training, the PLM learns some knowledge, encoded in its hidden representations, that can transfer to downstream tasks.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029214502721.png" alt="image-20221029214502721" style="zoom:72%;"> <p><strong>fine-tuning</strong>: Using the pre-trained weights of the PLM to initialize a model for a downstream task.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221029214742084.png" alt="image-20221029214742084" style="zoom:67%;"> <p>PLMs has shown great success on a variety of benchmark datasets in NLP. <strong>The next goal is to make PLMs fit in real-life use case</strong>. 但我们将 PLMs 用到现实情况时，却会遇到各种问题。</p> <h2 id="_2-the-problem-of-plms"><a href="#_2-the-problem-of-plms" class="header-anchor">#</a> 2. The problem of PLMs</h2> <h3 id="_2-1-problem-1-data-scarcity-in-downstream-tasks"><a href="#_2-1-problem-1-data-scarcity-in-downstream-tasks" class="header-anchor">#</a> 2.1 Problem 1: <mark>Data scarcity</mark> in downstream tasks</h3> <p>A large amount of labeled data is not easy to obtain for each downstream task. 下面是一个训练 BERT 所用的 dataset：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161440398.png" alt="image-20221030161440398" style="zoom:80%;"> <p>这里面的数据集最少都是几千级的，但在现实中想要弄到这么多的数据还是非常困难的。</p> <h3 id="_2-2-problem-2-the-plm-is-too-big"><a href="#_2-2-problem-2-the-plm-is-too-big" class="header-anchor">#</a> 2.2 Problem 2: The PLM is too big</h3> <p>The PLM is too big, and they are still getting bigger:</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161617975.png" alt="image-20221030161617975" style="zoom:80%;"> <p>在实际应用时，这么大的模型，我们需要为每个 downstream 都弄一份 copy，这会特别占据空间：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030161959783.png" alt="image-20221030161959783" style="zoom:80%;"> <p>而且这么多层的模型，计算一次 inference 都需要花费特别多的时间。</p> <p>因此，模型越来越大的问题可以总结为两个：</p> <ul><li>Inference takes too long.</li> <li>Consume too much space.</li></ul> <h2 id="_3-labeled-data-scarcity-data-efficient-fine-tuning"><a href="#_3-labeled-data-scarcity-data-efficient-fine-tuning" class="header-anchor">#</a> 3. Labeled Data Scarcity -&gt; Data-Efficient Fine-tuning</h2> <h3 id="_3-1-prompt-tuning"><a href="#_3-1-prompt-tuning" class="header-anchor">#</a> 3.1 Prompt Tuning</h3> <h4 id="_3-1-1-什么是-prompt-tuning"><a href="#_3-1-1-什么是-prompt-tuning" class="header-anchor">#</a> 3.1.1 什么是 Prompt Tuning？</h4> <p>以往在做 natural language inference 时，我们往往会这么做：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165310825.png" alt="image-20221030165310825" style="zoom:70%;"> <p>但如果 training data 较少的话，这往往是难以做出来的：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165458079.png" alt="image-20221030165458079" style="zoom:70%;"> <p>此时一种方法是，都加上一句 “Is is true that” 来表示询问后面这个句子与前面句子的关系，如下图所示：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030165701921.png" alt="image-20221030165701921" style="zoom:70%;"> <p>这个东西就是 <mark>Prompt Tuning</mark> 的核心概念，也就是设置一些东西告诉 model 我们在做什么。所以说，By converting the data points in the dataset into natural language prompts, the model may be easier to know what it should do.</p> <p>什么是 Prompt Tuning 呢？<u>Format the downstream task as a language modelling task with predefined</u> <u>templates into natural language <strong>prompts</strong></u>.</p> <h4 id="_3-1-2-prompt-tuning-需要什么"><a href="#_3-1-2-prompt-tuning-需要什么" class="header-anchor">#</a> 3.1.2 Prompt Tuning 需要什么</h4> <p>What you need in prompt tuning:</p> <ol><li>A prompt template</li> <li>A PLM</li> <li>A verbalizer</li></ol> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030171916101.png" alt="image-20221030171916101" style="zoom:80%;"></center> <h5 id="_1-a-prompt-template"><a href="#_1-a-prompt-template" class="header-anchor">#</a> 1）A prompt template</h5> <p>A <mark>prompt template</mark>: convert data points into a natural language prompt.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172215732.png" alt="image-20221030172215732" style="zoom:80%;"> <p>在得到 natural language prompt 后，就可以将它输入到 PLM 中，来预测 [MASK] 的部分是什么。</p> <h5 id="_2-a-plm"><a href="#_2-a-plm" class="header-anchor">#</a> 2）A PLM</h5> <p>A <mark>PLM</mark>: perform language modeling.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172527863.png" alt="image-20221030172527863" style="zoom:80%;"> <h5 id="_3-a-verbalizer"><a href="#_3-a-verbalizer" class="header-anchor">#</a> 3）A verbalizer</h5> <p>A <mark>verbalizer</mark>: A mapping between the label and the vocabulary. For example, which vocabulary should represents the class “entailment”:</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172809882.png" alt="image-20221030172809882" style="zoom:80%;"> <p>然后在神经网络中，我们就可以这么干了：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030172845084.png" alt="image-20221030172845084" style="zoom:80%;"> <h4 id="_3-1-3-prompt-tuning-v-s-standard-fine-tuning"><a href="#_3-1-3-prompt-tuning-v-s-standard-fine-tuning" class="header-anchor">#</a> 3.1.3 Prompt tuning v.s. Standard fine-tuning</h4> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221030173244999.png" alt="image-20221030173244999" style="zoom:80%;"></center> <ul><li>在 standard fine-tuning 中，我们会丢掉 LM Head 并重新 initialize 一个 Classifier Head；</li> <li>而在 prompt tuning 中，我们就是要利用 language model 的能力，因此不会丢弃这个 language model 的 head。</li></ul> <p>Prompt tuning has better performance under data scarcity <strong>because</strong>：</p> <ul><li>It incorporates human knowledge（因为 prompt template 的设计本身就融入了 human knowledge）</li> <li>It introduces no new parameters</li></ul> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102201439206.png" alt="image-20221102201439206" style="zoom:73%;"></center> <p>下面，Lets see how prompts can help us under different level of data scarcity.</p> <h3 id="_3-2-few-shot-learning"><a href="#_3-2-few-shot-learning" class="header-anchor">#</a> 3.2 Few-shot Learning</h3> <p><mark>Few-shot Learning</mark>: We have <strong>some</strong> labeled training data.</p> <p>但 Few-shot Learning 也是一个没有明确定义的词，具体多少是 few-shot，并没有具体的范围，在这里假设 few-shot 是指的 “Some ≈ 10GB training data”。</p> <p>Good News 是 GPT-3 可以被用于做 few-shot learning，但 bad news 是：GPT-3 is not freely available and contains 175B parameters.</p> <p>Can we use smaller PLMs and make them to perform well in few-shot learning?</p> <p><mark>LM-BFF</mark>: <strong>b</strong>etter <strong>f</strong>ew-shot <strong>f</strong>ine-tuning of <strong>l</strong>anguage <strong>m</strong>odels. 它的核心概念：<strong>prompt</strong> + <strong>demonstration</strong>。</p> <p>prompt 是指下面这个样子：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102203929560.png" alt="image-20221102203929560" style="zoom:75%;"> <p>而 <mark>demonstration</mark> 是说，我要让 model 知道，当它看到这样的 prompt 之后，它该去怎么做。所以 demonstration 的做法就是，在 prompt 的部分后面加了两个 demonstration 的句子：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102204222689.png" alt="image-20221102204222689" style="zoom:80%;"> <p>给他一个正面的 review，当 model 看到正面的 review 之后，它应该知道后面 “It was ___” 这里应该填 “great”，类似的，当他看到负面的 review 之后，它应该知道后面的 “It was ___” 这里应该填 “terrible”。 这样的形式就可以更加帮助 language model 在 few-shot learning 上面的表现。</p> <p>其 performance 如下：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102210720916.png" alt="image-20221102210720916" style="zoom:67%;"> <h3 id="_3-3-semi-supervised-learning"><a href="#_3-3-semi-supervised-learning" class="header-anchor">#</a> 3.3 Semi-supervised Learning</h3> <p><mark>Semi-supervised Learning</mark>: We have some labeled training data and a large amount of unlabeled data.</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221102212710576.png" alt="image-20221102212710576" style="zoom:80%;"> <p>这里主要看一下：<a href="https://aclanthology.org/2021.naacl-main.185/?utm_campaign=%E6%AF%8E%E9%80%B1%20NLP%20%E8%AB%96%E6%96%87&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener noreferrer">It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>，因为 GPT-3 的论文就是 <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">Language Models are Few-Shot Learners<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>，所以前者就像有点在打脸 GPT-3 的样子。</p> <p>它提出的方法叫做 <strong>Pattern-Exploiting Training</strong> (<mark>PET</mark>)，具体分成了三个步骤：</p> <ul><li>Step 1: Use different prompts and verbalizer to prompt-tune different PLMs on the labeled dataset.</li></ul> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221104221640334.png" alt="image-20221104221640334" style="zoom:67%;"></center> <ul><li>Step 2: Predict the unlabeled dataset and combine the predictions from different models.</li></ul> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221104222639420.png" alt="image-20221104222639420" style="zoom:67%;"></center> <p>对同一笔数据，不同 prompt-tune 得到的 model 给出的 prediction 也许是不一样的，我们拿到这些 prediction 后再将他们 combine 到一起，在这里可以只是简单地相加。</p> <ul><li>Step 3: Use a PLM with classifier head to train on the soft-labeled data set.</li></ul> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221104223312301.png" alt="image-20221104223312301" style="zoom:67%;"></center> <p>这一步是在所有的 dataset 进行 fine-tuning，对于 labeled data 则是用的原 label，而对于 unlabeled data，则用的是 soft label，即我们在 step 2 中得到的 label，然后进行 standard fine-tuning，也就是拿掉 LM Head 再加一个 Classifier Head 进行 fine-tuning。</p> <h3 id="_3-4-zero-shot-learning"><a href="#_3-4-zero-shot-learning" class="header-anchor">#</a> 3.4 Zero-shot Learning</h3> <h4 id="_3-4-1-什么是-zero-shot"><a href="#_3-4-1-什么是-zero-shot" class="header-anchor">#</a> 3.4.1 什么是 zero-shot？</h4> <p><mark>Zero-shot inference</mark>: inference on the downstream task without any training data. If you don’t have training data, then we need a model that can zero-shot inference on downstream tasks.</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106190303964.png" alt="image-20221106190303964" style="zoom:80%;"></center> <p>什么样的 model 可以做 zero-shot inference 呢？GPT-3 可以！GPT-3 的可以告诉了我们一件事情：Only if your model is large enough.</p> <p><mark>Zero-shot</mark>: The model predicts the answer given only a natural language description of the task. No gradient updates are performed. 比如如下面所示，你告诉要做的 task description，然后再给一个 prompt：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106191126794.png" alt="image-20221106191126794" style="zoom:90%;"> <p>它的 performance 如下图，可以看到随着参数量越来越多，performance 会越老越好，但 zero-shot 其实也没有特别好，还是有很多的提升空间：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106191256438.png" alt="image-20221106191256438" style="zoom:95%;"></center> <h4 id="_3-4-2-where-does-this-zero-shot-ability-spring-from"><a href="#_3-4-2-where-does-this-zero-shot-ability-spring-from" class="header-anchor">#</a> 3.4.2 Where does this zero-shot ability spring from?</h4> <p>💡 <strong>Hypothesis</strong>: during pre-training, the training datasets implicitly contains a mixture of different tasks.</p> <p>为什么 GPT-3 可以做到 zero-shot inference 呢？一个假说认为，我们 pre-training 的过程就很像是一个 multi-task learning 的环境，pre-training data 里面有各种各样的 task，因此能让他学到 multi-task learning 的能力。</p> <p>比如说它可能会看到 QA 的文本，如下所示，一个 Q 开头，一个 A 开头，这其中就暗示了 QA task 了：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106192401111.png" alt="image-20221106192401111" style="zoom:80%;"> <p>又比如下面这个例子，当他看到 one-sentence summary 时，他就知道上面的 abstract 的那一大段话用一句话来表述的话，就是后面这句话的样子，因此这也暗示了 summarization 的 task：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106192900248.png" alt="image-20221106192900248" style="zoom:90%;"> <p>所以说，<strong>during pre-training, the training datasets implicitly contains a mixture of different tasks.</strong></p> <p>到了这里，有人就想到说，与其让他 implicitly 学习这种能力，不如让他 explicitly 学习这种能力。</p> <p>💡 <strong>Hypothesis</strong>: multi-task training enables zero-shot generalization.</p> <p>有人就对 T5 模型用 multi-task learning 的方式进行 fine-tuning，然后测试看在 fine-tuning 之后，这个 model 有没有能力做 zero-shot learning:</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106194527011.png" alt="image-20221106194527011" style="zoom:95%;"></center> <p>具体的做法与之前是一样的，就是将非常多的 dataset 转换成多种类型的 prompts 的形式，然后给他看很多的 prompt。比如下面这个例子就是在做 Natural Language Inference (<strong>NLI</strong>) 时，将 NLI dataset 转换成 Natural language prompt 的示例：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106195058586.png" alt="image-20221106195058586" style="zoom:80%;"></center> <p>我们要 train 的 task 有很多不同种类，我们将这些 tasks 分成两类，如下图所示，黄色的一类 tasks 会用于 fine-tuning 过程，然后剩下的绿色的一类 tasks 会用于 zero-shot inference 的测试。</p> <p>上面所讲的这种做法，sometimes achieves performance better than GPT-3 (175B parameters) with <em><strong>only 11B</strong></em> parameters. 效果还是很不错的。</p> <h3 id="_3-5-summary"><a href="#_3-5-summary" class="header-anchor">#</a> 3.5 Summary</h3> <p>我们总结一下做了什么，在 dataset 比较少的时候，我们可以 use natural language prompts and add scenario-specific designs：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106200118470.png" alt="image-20221106200118470" style="zoom:67%;"></center> <h2 id="_4-plms-are-gigantic-reducing-the-number-of-parameters"><a href="#_4-plms-are-gigantic-reducing-the-number-of-parameters" class="header-anchor">#</a> 4. PLMs Are Gigantic -&gt; Reducing the Number of Parameters</h2> <p>这一章就是讲当 PLM 太大的时候，应该怎么样让他小一点。</p> <p>最直接的想法就是让 PLM 更加小一点，但是这其实是有问题的，因为直接对小的 PLM 进行 pre-training 后，其实你还是用的同样的 corpus、同样的时间来预训练，最终结果的 performance 相比于大的 model 相差了很大一截，<strong>所以直接用小的 PLM 来做 pre-training 是不太可行的</strong>。</p> <p>比较可行的一个方法是减少我们在 fine-tuning 时所需要用到的参数的量。这其实就有很多不同的方式了。</p> <h3 id="_4-1-pre-train-a-large-model-but-use-a-smaller-model-for-the-downstream-tasks"><a href="#_4-1-pre-train-a-large-model-but-use-a-smaller-model-for-the-downstream-tasks" class="header-anchor">#</a> 4.1 Pre-train a large model, but use a smaller model for the downstream tasks.</h3> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106211227014.png" alt="image-20221106211227014" style="zoom:80%;"></center> <p>下面这个 pruning 的做法得到的 BERT-base 虽然结构还是比较大，但是由于它的 sparsity 比较高，因此占用的空间就小了很多了。</p> <h3 id="_4-2-share-the-parameters-among-the-transformer-layers-albert"><a href="#_4-2-share-the-parameters-among-the-transformer-layers-albert" class="header-anchor">#</a> 4.2 Share the parameters among the transformer layers: ALBERT</h3> <p>以往的不同 Transformer Layer 的参数是不一样的，但在 <mark>ALBERT</mark> 中，每一层的参数都是一样的：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106211559280.png" alt="image-20221106211559280" style="zoom:80%;"></center> <h3 id="_4-3-parameter-efficient-fine-tuning"><a href="#_4-3-parameter-efficient-fine-tuning" class="header-anchor">#</a> 4.3 Parameter-Efficient Fine-tuning</h3> <p>这种方法就是希望在 fine-tuning 的时候，怎样用少一点的 parameters。一种方法是下面这种：</p> <p>🚀 <strong>Use a small amount of parameters for each downstream task</strong></p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106212201920.png" alt="image-20221106212201920" style="zoom:70%;"></center> <p>这样我们就只需要一个 BERT 的参数，再加上对每个 task 有一个专属的 parameters，从而大大降低我们所需要的空间。</p> <p>这件事情怎么做呢？</p> <p>要做这件事情之前，我们先看一下 standard fine-tuning 真正做了什么。standard fine-tuning 真正做的事情是：Modify the <em><u>hidden representation</u></em> (<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container>) of the PLM such that it can perform well on downstream task. 在 fine-tuning 之前，PLM 如下图所示：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106212649354.png" alt="image-20221106212649354" style="zoom:75%;"></center> <p>经过了微调之后，PLM 内部的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container> 发生了改变：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106213453467.png" alt="image-20221106213453467" style="zoom:75%;"></center> <p>这个微调后的 hidden representation 能够被 classifier head 很好地利用。以上就是 standard fine-tuning 所做的事情。</p> <p>所以说，<strong>standard fine-tuning = modifying the hidden representation based on PLM</strong>. 也就是将 hidden representation 从 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container> 改变到了 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-n"><mjx-c c="394"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106215144810.png" alt="image-20221106215144810" style="zoom:72%;"></center> <p>但现在我们想问的是，<strong>我们有没有办法不要调整整个 model 的参数，而是只要改变少部分的参数就可以达到改变 hidden representation 的目标呢</strong>？这里 parameter-efficient fine-tuning 所要做的就是这件事情。下面就看一下不同的 parameter-efficient fine-tuning 是怎么做的。</p> <h3 id="_4-4-parameter-efficient-fine-tuning-adapter"><a href="#_4-4-parameter-efficient-fine-tuning-adapter" class="header-anchor">#</a> 4.4 Parameter-Efficient Fine-tuning: Adapter</h3> <p>Adapter 就是 use special submodules to modify hidden representations：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106220651389.png" alt="image-20221106220651389" style="zoom:67%;"> <p><mark>Adapters</mark>: small trainable submodules inserted in transformers.</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106220902149.png" alt="image-20221106220902149" style="zoom:80%;"></center> <p>这里的 Adapter 长什么样呢？如下图所示，hidden representation <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container> 首先经过一个 MLP 进行降维，再经过非线性转化层得到一个较低维度的 vector，再然后经过 MLP 变成与原来相同维度的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-n"><mjx-c c="394"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221106221255363.png" alt="image-20221106221255363" style="zoom:80%;"></center> <p>所以在这个技术中，我们 fine-tuning 的只需要去 update 我们的 adapters 和 classifier head 就可以了。</p> <h3 id="_4-5-parameter-efficient-fine-tuning-lora"><a href="#_4-5-parameter-efficient-fine-tuning-lora" class="header-anchor">#</a> 4.5 Parameter-Efficient Fine-tuning: LoRA</h3> <p>从整体上看，LoRA 所要做的事情与刚刚是一样的：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107200646345.png" alt="image-20221107200646345" style="zoom:80%;"></center> <p><mark>LoRA</mark>: Low-Rank Adaptation of Large Language Models. 如下图所示，<strong>它所做的就是在 Feed-forward 部分上平行地加上了一个 submodule</strong>：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107201122582.png" alt="image-20221107201122582" style="zoom:67%;"></center> <p>Feed-forward 部分原来就是一个两层的 MLP，现在加上 LoRA 后，结构如下图：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107201450844.png" alt="image-20221107201450844" style="zoom:67%;"></center> <p>LoRA 的 submodule 会加上原来的 MLP 的输出，从而共同构成输出。如果我们放大红色方框的部分，可以看到具体做法如下：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107201840192.png" alt="image-20221107201840192" style="zoom:72%;"></center> <p>原来的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="o"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="l"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub></mjx-math></mjx-container> 先投影成一个维度很低的 vector，再经过投影还原为 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-TeXAtom size="s"><mjx-mi class="mjx-i"><mjx-c c="F"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="F"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi></mjx-TeXAtom></mjx-script></mjx-msub></mjx-math></mjx-container>。</p> <p>这样的话，所有的 downstream tasks 共享一个 PLM，然后每一层的 LoRA submodule 和 classifier head 才是 task-specific modules。</p> <p>LoRA 的卖点之一是，它的 submodule 是平行地插在 feed-forward 部分上，这样不会增加 inference 的时间，而 Adapter 技术则是加深了网络的深度，这导致了 inference 时间的增加。</p> <h3 id="_4-6-parameter-efficient-fine-tuning-prefix-tuning"><a href="#_4-6-parameter-efficient-fine-tuning-prefix-tuning" class="header-anchor">#</a> 4.6 Parameter-Efficient Fine-tuning: Prefix Tuning</h3> <p>Prefix Tuning 所做的事情也是一样：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107203116951.png" alt="image-20221107203116951" style="zoom:67%;"></center> <p><mark>Prefix Tuning</mark>: Insert trainable prefix in each layer.</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107203332817.png" alt="image-20221107203332817" style="zoom:72%;"></center> <p>为了讲 prefix tuning，我们先回顾一下 standard self-attention 的操作，如下图所示，为了计算 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math></mjx-container> 的对应输出 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.288em;"><mjx-TeXAtom size="s"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="2032"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-TeXAtom><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msubsup></mjx-math></mjx-container>，<mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="q"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container> 会依次去 query 其他人来得到相似度：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107204721625.png" alt="image-20221107204721625" style="zoom:67%;"></center> <p>而 prefix tuning 则是在 standard self-attention 的基础上加了一些东西，如下图所示，所增加 prefix <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="p"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mo size="s" class="mjx-n"><mjx-c c="."></mjx-c></mjx-mo></mjx-script></mjx-msub></mjx-math></mjx-container> 部分只会被其他人所 query，而不会去 query 其他人：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107205217541.png" alt="image-20221107205217541" style="zoom:67%;"></center> <p>在 prefix tuning 中，<strong>Only the prefix (key and value) are updated during fine-tuning</strong>. 在最后，我们只需要保留 k v 就好，可以把 prefix <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="p"></mjx-c></mjx-mi></mjx-math></mjx-container> 给抛弃掉，因此最后每一层 Transformer 都会有自己的 prefix 的 key 和 value：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107210224329.png" alt="image-20221107210224329" style="zoom:67%;"></center> <h3 id="_4-7-parameter-efficient-fine-tuning-soft-prompting"><a href="#_4-7-parameter-efficient-fine-tuning-soft-prompting" class="header-anchor">#</a> 4.7 Parameter-Efficient Fine-tuning: Soft Prompting</h3> <p>这个 <mark>Soft Prompting</mark> 可以看成是一个 prefix tuning 的一个简化版，它就是只在 input layer 上插入了几个 prefix embedding：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107211927837.png" alt="image-20221107211927837" style="zoom:67%;"></center> <p>为什么这叫做 soft prompt 呢？之前的 prompt 方式是 <strong>Hard Prompting</strong>：add words in the input sentence (fine-tune the model while fixing the prompts)：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107212311262.png" alt="image-20221107212311262" style="zoom:67%;"></center> <p>在这种 hard prompting 中，你是没有办法直接调这个 prompt 的字，因为它们又不是可以微分的，因此比较 hard。而 <strong>soft prompting can be considered as the soften version of prompting</strong>.</p> <div class="custom-block note"><p class="custom-block-title">Soft Prompt v.s. Hard Prompt</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107214024042.png" alt="image-20221107214024042" style="zoom:67%;"></div> <h3 id="_4-8-总结-parameter-efficient-fine-tuning"><a href="#_4-8-总结-parameter-efficient-fine-tuning" class="header-anchor">#</a> 4.8 总结 Parameter-Efficient Fine-tuning</h3> <ul><li>🍨 Benefit 1：<strong>Drastically decreases the task-specific parameters</strong>.</li></ul> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107214433435.png" alt="image-20221107214433435" style="zoom:80%;"></center> <p>Adapter 和 LoRA 的 percent trainable 差距很大的原因在于 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="r"></mjx-c></mjx-mi></mjx-math></mjx-container> 的选择范围不太一样，Adapter 往往选 16、32、64 这种，而 LoRA 往往选 1 或者 2。</p> <p>Soft Prompt 往往需要在 model 很大的时候，效果才会比较好，而其余的则没有要求 model 很大。</p> <ul><li>🍨 Benefit 2: <strong>Less easier to overfit on training data; better out-of-domain performance</strong>.</li></ul> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107215121305.png" alt="image-20221107215121305" style="zoom:75%;"></center> <p>可以看到，soft prompt 虽然参数比较少，但是效果还是很好的，尽管在后面两个 dataset 上 performance 略有降低，但也没有降低很多了。</p> <ul><li>🍨 Benefit 3: <strong>Fewer parameters to fine-tune; a good candidate when training with small dataset</strong>.</li></ul> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107215416800.png" alt="image-20221107215416800" style="zoom:80%;"></center> <p>可以看到，在 low-resource 的情况下可以表现更好，而在 high-resource 的情况下，即使用了较少的参数，但 performance 也没有掉太多。</p> <h3 id="_4-9-early-exit"><a href="#_4-9-early-exit" class="header-anchor">#</a> 4.9 Early Exit</h3> <p>这个主题的目的也是想要减少 downstream task 的 parameters 数量，不过它是<strong>动态减少</strong>的。</p> <p>传统的 PLM 是用最后一层的 hidden representation 来训练一个 classifier，而<strong>问题是使用整个模型来做 inference 太花时间了</strong>，于是有人提出 Simpler data may require less effort to obtain the answer. 于是有了这么一个想法：<u>Reduce the number of layers used during inference</u>。</p> <p>这样的做法就是在每一层上加一个 classifier：</p> <center><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107220211972.png" alt="image-20221107220211972" style="zoom:80%;"></center> <p>但这样的话，现在的问题就是：How do we know which classifier to use? 这其实有很多不同的做法，这里讲一个最新的做法：</p> <p>我们额外又训练一个叫做 <strong>Confidence predictor</strong> 的 submodule，这个 predictor 会根据 classifier 和 hidden representation 来 predict 说这个 classifier 的结果够不够有信心：</p> <p><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20221107220613232.png" alt="image-20221107220613232" style="zoom:80%;">****</p> <p>比如上图中，在第一层的 classifier 被认为是不够有信心的，于是会来到 classifier 2，这时 predictor 认为有足够信心了，于是就可以直接拿这个 classifier 的 output 当做最终的 output，而不需要再去看后面的部分了。</p> <p>所以 <strong>Early exit reduces the inference time while keeping the performance</strong>。</p> <h3 id="_4-10-summary"><a href="#_4-10-summary" class="header-anchor">#</a> 4.10 Summary</h3> <ul><li><strong>Parameter-efficient fine-tuning</strong>: Reduce the task-specific parameters in downstream task.</li> <li><strong>Early exit</strong>: Reduce the models that are involved during inference.</li></ul> <h2 id="_5-closing-remarks"><a href="#_5-closing-remarks" class="header-anchor">#</a> 5. Closing Remarks</h2> <p>What we address in this lecture:</p> <ul><li>Making PLM smaller, faster, and more parameter-efficient</li> <li>Deploying PLMs when the labeled data in the downstream task is scarce</li></ul> <p>The problems we discuss are just a small part of problems of PLMs, and the problems are not completely solved yet:</p> <ul><li>Why does self-supervised pre-training work</li> <li>Interpretability of the model's prediction</li> <li>Domain adaptation</li> <li>Continual learning/lifelong learning</li> <li>Security and privacy</li></ul></div></div> <div class="page-slot page-slot-bottom"><!-- 横向自适应 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6620245489"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="page-edit"><div class="edit-link"><a href="https://github.com/yubincloud/notebook/edit/master/docs/AI/01.深度学习/15.深度学习-李宏毅/38.Data Efficient &amp; Parameter-Efficient Tuning.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2022/11/08, 06:45:28</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/notebook/pages/lhy/bert-and-family/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">BERT and its family</div></a> <a href="/notebook/pages/lhy/auto-encoder/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Auto-Encoder</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/notebook/pages/lhy/bert-and-family/" class="prev">BERT and its family</a></span> <span class="next"><a href="/notebook/pages/lhy/auto-encoder/">Auto-Encoder</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/notebook/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/notebook/pages/ml/lhy/drl17/"><div>
            Deep Reinforcement Learning
            <!----></div></a> <span class="date">10-03</span></dt></dl><dl><dd>02</dd> <dt><a href="/notebook/pages/mysql/geektime/misdeletion/"><div>
            误删数据后怎么办
            <!----></div></a> <span class="date">04-06</span></dt></dl><dl><dd>03</dd> <dt><a href="/notebook/pages/mysql/geektime/multi-slaves/"><div>
            MySQL 一主多从
            <!----></div></a> <span class="date">03-22</span></dt></dl> <dl><dd></dd> <dt><a href="/notebook/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="yubin_inbuy@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/yubincloud" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2021-2024
    <span>yubincloud | <a href="https://github.com/yubincloud/notebook/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <div class="custom-html-window custom-html-window-rb" style="display:;"><div class="custom-wrapper"><span class="close-but">×</span> <div><!-- 固定160*160px -->
      <ins class="adsbygoogle"
          style="display:inline-block;max-width:160px;max-height:160px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="8377369658"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      </div></div></div></div><div class="global-ui"><div></div></div></div>
    <script src="/notebook/assets/js/app.2bf3b6c1.js" defer></script><script src="/notebook/assets/js/2.0ad58009.js" defer></script><script src="/notebook/assets/js/88.e30608d9.js" defer></script>
  </body>
</html>
