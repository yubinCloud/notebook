<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Attention | notebook</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/notebook/img/favicon.ico">
    <script data-ad-client="ca-pub-7828333725993554" async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="学习笔记">
    <meta name="keywords" content="全栈学习笔记">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/notebook/assets/css/0.styles.1b58b254.css" as="style"><link rel="preload" href="/notebook/assets/js/app.2bf3b6c1.js" as="script"><link rel="preload" href="/notebook/assets/js/2.0ad58009.js" as="script"><link rel="preload" href="/notebook/assets/js/75.252e6fc0.js" as="script"><link rel="prefetch" href="/notebook/assets/js/10.99522837.js"><link rel="prefetch" href="/notebook/assets/js/100.e3b56889.js"><link rel="prefetch" href="/notebook/assets/js/101.6ea1d00b.js"><link rel="prefetch" href="/notebook/assets/js/102.eca9dfbd.js"><link rel="prefetch" href="/notebook/assets/js/103.6ea477d4.js"><link rel="prefetch" href="/notebook/assets/js/104.ac820d2b.js"><link rel="prefetch" href="/notebook/assets/js/105.58b259a8.js"><link rel="prefetch" href="/notebook/assets/js/106.a86005d0.js"><link rel="prefetch" href="/notebook/assets/js/107.7a79d36f.js"><link rel="prefetch" href="/notebook/assets/js/108.64404e25.js"><link rel="prefetch" href="/notebook/assets/js/109.75f12c0a.js"><link rel="prefetch" href="/notebook/assets/js/11.d26d59e4.js"><link rel="prefetch" href="/notebook/assets/js/110.1155fe36.js"><link rel="prefetch" href="/notebook/assets/js/111.bf8b5871.js"><link rel="prefetch" href="/notebook/assets/js/112.22833ceb.js"><link rel="prefetch" href="/notebook/assets/js/113.6a080233.js"><link rel="prefetch" href="/notebook/assets/js/114.35de9701.js"><link rel="prefetch" href="/notebook/assets/js/115.f598d8c2.js"><link rel="prefetch" href="/notebook/assets/js/116.e3bd29ce.js"><link rel="prefetch" href="/notebook/assets/js/117.c3c02abc.js"><link rel="prefetch" href="/notebook/assets/js/118.136a552a.js"><link rel="prefetch" href="/notebook/assets/js/119.c124f3f8.js"><link rel="prefetch" href="/notebook/assets/js/12.dc66c4f2.js"><link rel="prefetch" href="/notebook/assets/js/120.f835d124.js"><link rel="prefetch" href="/notebook/assets/js/121.367716ae.js"><link rel="prefetch" href="/notebook/assets/js/122.752b0493.js"><link rel="prefetch" href="/notebook/assets/js/123.9f8d6026.js"><link rel="prefetch" href="/notebook/assets/js/124.e8eb61b6.js"><link rel="prefetch" href="/notebook/assets/js/125.cb081200.js"><link rel="prefetch" href="/notebook/assets/js/126.ab87d911.js"><link rel="prefetch" href="/notebook/assets/js/127.ffdbe74d.js"><link rel="prefetch" href="/notebook/assets/js/128.ec526e42.js"><link rel="prefetch" href="/notebook/assets/js/129.71839012.js"><link rel="prefetch" href="/notebook/assets/js/13.32e95b42.js"><link rel="prefetch" href="/notebook/assets/js/130.2bc0bb4d.js"><link rel="prefetch" href="/notebook/assets/js/131.5595b49b.js"><link rel="prefetch" href="/notebook/assets/js/132.4963c5c4.js"><link rel="prefetch" href="/notebook/assets/js/133.44f48cfd.js"><link rel="prefetch" href="/notebook/assets/js/134.cf25626c.js"><link rel="prefetch" href="/notebook/assets/js/135.5ee30fa9.js"><link rel="prefetch" href="/notebook/assets/js/136.bc43f8e6.js"><link rel="prefetch" href="/notebook/assets/js/137.9ab5beac.js"><link rel="prefetch" href="/notebook/assets/js/138.692a33e6.js"><link rel="prefetch" href="/notebook/assets/js/139.08e7c98d.js"><link rel="prefetch" href="/notebook/assets/js/14.c418d170.js"><link rel="prefetch" href="/notebook/assets/js/140.39a861db.js"><link rel="prefetch" href="/notebook/assets/js/141.46678413.js"><link rel="prefetch" href="/notebook/assets/js/142.f7ef5eac.js"><link rel="prefetch" href="/notebook/assets/js/143.c92cbac1.js"><link rel="prefetch" href="/notebook/assets/js/144.d9c61437.js"><link rel="prefetch" href="/notebook/assets/js/145.9f603b31.js"><link rel="prefetch" href="/notebook/assets/js/146.b875f045.js"><link rel="prefetch" href="/notebook/assets/js/147.55e7c4f8.js"><link rel="prefetch" href="/notebook/assets/js/148.4410c365.js"><link rel="prefetch" href="/notebook/assets/js/149.6096ed98.js"><link rel="prefetch" href="/notebook/assets/js/15.e0e7392a.js"><link rel="prefetch" href="/notebook/assets/js/150.24451f07.js"><link rel="prefetch" href="/notebook/assets/js/151.7cff301c.js"><link rel="prefetch" href="/notebook/assets/js/152.035fee1f.js"><link rel="prefetch" href="/notebook/assets/js/153.c61f8ec3.js"><link rel="prefetch" href="/notebook/assets/js/154.7bb549d0.js"><link rel="prefetch" href="/notebook/assets/js/155.1dc494db.js"><link rel="prefetch" href="/notebook/assets/js/156.b87eaf39.js"><link rel="prefetch" href="/notebook/assets/js/157.e3f5a5c0.js"><link rel="prefetch" href="/notebook/assets/js/158.c565c699.js"><link rel="prefetch" href="/notebook/assets/js/159.a22609ef.js"><link rel="prefetch" href="/notebook/assets/js/16.d1aef4ee.js"><link rel="prefetch" href="/notebook/assets/js/160.b29e761c.js"><link rel="prefetch" href="/notebook/assets/js/161.bee1e522.js"><link rel="prefetch" href="/notebook/assets/js/162.c49fca62.js"><link rel="prefetch" href="/notebook/assets/js/163.2cb4d37d.js"><link rel="prefetch" href="/notebook/assets/js/164.4a0dbc64.js"><link rel="prefetch" href="/notebook/assets/js/165.490d05b3.js"><link rel="prefetch" href="/notebook/assets/js/166.df5d2527.js"><link rel="prefetch" href="/notebook/assets/js/167.89a81814.js"><link rel="prefetch" href="/notebook/assets/js/168.9991702e.js"><link rel="prefetch" href="/notebook/assets/js/169.2f9a5dce.js"><link rel="prefetch" href="/notebook/assets/js/17.88ae5445.js"><link rel="prefetch" href="/notebook/assets/js/170.5f23eb3c.js"><link rel="prefetch" href="/notebook/assets/js/171.c521aaa8.js"><link rel="prefetch" href="/notebook/assets/js/172.42110b0a.js"><link rel="prefetch" href="/notebook/assets/js/173.5e36f1bf.js"><link rel="prefetch" href="/notebook/assets/js/174.f48e078a.js"><link rel="prefetch" href="/notebook/assets/js/175.775da6a5.js"><link rel="prefetch" href="/notebook/assets/js/176.9c3c55ea.js"><link rel="prefetch" href="/notebook/assets/js/177.b54d1cff.js"><link rel="prefetch" href="/notebook/assets/js/178.ff08b7f5.js"><link rel="prefetch" href="/notebook/assets/js/179.c6a1af32.js"><link rel="prefetch" href="/notebook/assets/js/18.dcb78196.js"><link rel="prefetch" href="/notebook/assets/js/180.25dd9eba.js"><link rel="prefetch" href="/notebook/assets/js/181.13e6ec84.js"><link rel="prefetch" href="/notebook/assets/js/182.f6849f0d.js"><link rel="prefetch" href="/notebook/assets/js/183.7e664874.js"><link rel="prefetch" href="/notebook/assets/js/184.e6aba86f.js"><link rel="prefetch" href="/notebook/assets/js/185.df07b919.js"><link rel="prefetch" href="/notebook/assets/js/186.02c77e75.js"><link rel="prefetch" href="/notebook/assets/js/187.e8380ed4.js"><link rel="prefetch" href="/notebook/assets/js/188.eddc8bee.js"><link rel="prefetch" href="/notebook/assets/js/189.fbc1840f.js"><link rel="prefetch" href="/notebook/assets/js/19.5997a514.js"><link rel="prefetch" href="/notebook/assets/js/190.a37bfe4c.js"><link rel="prefetch" href="/notebook/assets/js/191.e53a3d4b.js"><link rel="prefetch" href="/notebook/assets/js/192.2f5be408.js"><link rel="prefetch" href="/notebook/assets/js/193.4ca6de49.js"><link rel="prefetch" href="/notebook/assets/js/194.b8e51d9d.js"><link rel="prefetch" href="/notebook/assets/js/195.70e6b23a.js"><link rel="prefetch" href="/notebook/assets/js/196.5d5fbf2d.js"><link rel="prefetch" href="/notebook/assets/js/197.78456dab.js"><link rel="prefetch" href="/notebook/assets/js/198.4308331c.js"><link rel="prefetch" href="/notebook/assets/js/199.2e537849.js"><link rel="prefetch" href="/notebook/assets/js/20.fc057fd7.js"><link rel="prefetch" href="/notebook/assets/js/200.b3309bbf.js"><link rel="prefetch" href="/notebook/assets/js/201.4723461c.js"><link rel="prefetch" href="/notebook/assets/js/202.b15b5177.js"><link rel="prefetch" href="/notebook/assets/js/203.22c50e61.js"><link rel="prefetch" href="/notebook/assets/js/204.5b8b3b00.js"><link rel="prefetch" href="/notebook/assets/js/205.54ee7630.js"><link rel="prefetch" href="/notebook/assets/js/206.f3f20f94.js"><link rel="prefetch" href="/notebook/assets/js/207.a9608973.js"><link rel="prefetch" href="/notebook/assets/js/208.1a80a593.js"><link rel="prefetch" href="/notebook/assets/js/209.586fd293.js"><link rel="prefetch" href="/notebook/assets/js/21.cb4205ee.js"><link rel="prefetch" href="/notebook/assets/js/210.7829dd53.js"><link rel="prefetch" href="/notebook/assets/js/211.3ce139ab.js"><link rel="prefetch" href="/notebook/assets/js/212.84738a64.js"><link rel="prefetch" href="/notebook/assets/js/213.a631830d.js"><link rel="prefetch" href="/notebook/assets/js/214.9d64cf85.js"><link rel="prefetch" href="/notebook/assets/js/215.87030b6b.js"><link rel="prefetch" href="/notebook/assets/js/216.ddbe1944.js"><link rel="prefetch" href="/notebook/assets/js/217.16ae7e40.js"><link rel="prefetch" href="/notebook/assets/js/218.e7780d65.js"><link rel="prefetch" href="/notebook/assets/js/219.abae5e09.js"><link rel="prefetch" href="/notebook/assets/js/22.256014b4.js"><link rel="prefetch" href="/notebook/assets/js/220.8e3a8702.js"><link rel="prefetch" href="/notebook/assets/js/221.4c279d74.js"><link rel="prefetch" href="/notebook/assets/js/222.6c9b2595.js"><link rel="prefetch" href="/notebook/assets/js/223.cc072424.js"><link rel="prefetch" href="/notebook/assets/js/224.c663b40f.js"><link rel="prefetch" href="/notebook/assets/js/225.f3e52654.js"><link rel="prefetch" href="/notebook/assets/js/226.5e00402c.js"><link rel="prefetch" href="/notebook/assets/js/227.1c28ce97.js"><link rel="prefetch" href="/notebook/assets/js/228.42b8c305.js"><link rel="prefetch" href="/notebook/assets/js/229.df9760ec.js"><link rel="prefetch" href="/notebook/assets/js/23.a3d7d66a.js"><link rel="prefetch" href="/notebook/assets/js/230.cfe18f05.js"><link rel="prefetch" href="/notebook/assets/js/231.3a664a46.js"><link rel="prefetch" href="/notebook/assets/js/232.966ce9dc.js"><link rel="prefetch" href="/notebook/assets/js/233.fc06cb57.js"><link rel="prefetch" href="/notebook/assets/js/234.7bb9b7d4.js"><link rel="prefetch" href="/notebook/assets/js/235.b336116e.js"><link rel="prefetch" href="/notebook/assets/js/236.03a38f77.js"><link rel="prefetch" href="/notebook/assets/js/237.0dbda856.js"><link rel="prefetch" href="/notebook/assets/js/238.c1c19749.js"><link rel="prefetch" href="/notebook/assets/js/239.046875c1.js"><link rel="prefetch" href="/notebook/assets/js/24.8e5e267e.js"><link rel="prefetch" href="/notebook/assets/js/240.4bd9cdc0.js"><link rel="prefetch" href="/notebook/assets/js/241.c3dc5804.js"><link rel="prefetch" href="/notebook/assets/js/242.db0b1a91.js"><link rel="prefetch" href="/notebook/assets/js/243.4d9bd61d.js"><link rel="prefetch" href="/notebook/assets/js/244.ee57770b.js"><link rel="prefetch" href="/notebook/assets/js/245.02aab1c1.js"><link rel="prefetch" href="/notebook/assets/js/246.b76a18bb.js"><link rel="prefetch" href="/notebook/assets/js/247.75a673db.js"><link rel="prefetch" href="/notebook/assets/js/248.ad93f81d.js"><link rel="prefetch" href="/notebook/assets/js/249.fb75a938.js"><link rel="prefetch" href="/notebook/assets/js/25.b12f24fe.js"><link rel="prefetch" href="/notebook/assets/js/250.8395c0b6.js"><link rel="prefetch" href="/notebook/assets/js/251.16a6d2a4.js"><link rel="prefetch" href="/notebook/assets/js/252.ef3ee05e.js"><link rel="prefetch" href="/notebook/assets/js/253.78e3471e.js"><link rel="prefetch" href="/notebook/assets/js/254.a5783e07.js"><link rel="prefetch" href="/notebook/assets/js/255.2ab853f6.js"><link rel="prefetch" href="/notebook/assets/js/256.5430831b.js"><link rel="prefetch" href="/notebook/assets/js/257.99c8a0a4.js"><link rel="prefetch" href="/notebook/assets/js/258.4496955b.js"><link rel="prefetch" href="/notebook/assets/js/259.9152b1d2.js"><link rel="prefetch" href="/notebook/assets/js/26.0fce5172.js"><link rel="prefetch" href="/notebook/assets/js/260.072f65e6.js"><link rel="prefetch" href="/notebook/assets/js/261.0bca81af.js"><link rel="prefetch" href="/notebook/assets/js/262.9c9c5337.js"><link rel="prefetch" href="/notebook/assets/js/263.42470957.js"><link rel="prefetch" href="/notebook/assets/js/264.64b5f4fb.js"><link rel="prefetch" href="/notebook/assets/js/265.836a69c5.js"><link rel="prefetch" href="/notebook/assets/js/266.a00cdeb1.js"><link rel="prefetch" href="/notebook/assets/js/267.09dc5ae4.js"><link rel="prefetch" href="/notebook/assets/js/268.6fa6603e.js"><link rel="prefetch" href="/notebook/assets/js/269.3963ce5e.js"><link rel="prefetch" href="/notebook/assets/js/27.47ba3886.js"><link rel="prefetch" href="/notebook/assets/js/270.2826382d.js"><link rel="prefetch" href="/notebook/assets/js/271.3c746c23.js"><link rel="prefetch" href="/notebook/assets/js/272.30698dda.js"><link rel="prefetch" href="/notebook/assets/js/273.b06e3fd2.js"><link rel="prefetch" href="/notebook/assets/js/274.2016c7fa.js"><link rel="prefetch" href="/notebook/assets/js/275.f4aff624.js"><link rel="prefetch" href="/notebook/assets/js/276.e682aa74.js"><link rel="prefetch" href="/notebook/assets/js/277.0c3f41db.js"><link rel="prefetch" href="/notebook/assets/js/278.3c2d5251.js"><link rel="prefetch" href="/notebook/assets/js/279.a9af5703.js"><link rel="prefetch" href="/notebook/assets/js/28.6bac56c6.js"><link rel="prefetch" href="/notebook/assets/js/280.a5da28a3.js"><link rel="prefetch" href="/notebook/assets/js/281.8cc5a3ba.js"><link rel="prefetch" href="/notebook/assets/js/282.55227ff2.js"><link rel="prefetch" href="/notebook/assets/js/283.13f54ae9.js"><link rel="prefetch" href="/notebook/assets/js/284.88644dec.js"><link rel="prefetch" href="/notebook/assets/js/285.0670211f.js"><link rel="prefetch" href="/notebook/assets/js/286.afa43d34.js"><link rel="prefetch" href="/notebook/assets/js/287.9e98e933.js"><link rel="prefetch" href="/notebook/assets/js/288.175a8a9b.js"><link rel="prefetch" href="/notebook/assets/js/289.0d712953.js"><link rel="prefetch" href="/notebook/assets/js/29.3476ca1f.js"><link rel="prefetch" href="/notebook/assets/js/290.4b258761.js"><link rel="prefetch" href="/notebook/assets/js/291.e7ded33e.js"><link rel="prefetch" href="/notebook/assets/js/292.fcfca63e.js"><link rel="prefetch" href="/notebook/assets/js/293.4d6c0f7d.js"><link rel="prefetch" href="/notebook/assets/js/294.59b7e2de.js"><link rel="prefetch" href="/notebook/assets/js/295.0b8dc8f3.js"><link rel="prefetch" href="/notebook/assets/js/296.65434eb0.js"><link rel="prefetch" href="/notebook/assets/js/297.957ba4a7.js"><link rel="prefetch" href="/notebook/assets/js/298.dd81e487.js"><link rel="prefetch" href="/notebook/assets/js/299.eba0d36a.js"><link rel="prefetch" href="/notebook/assets/js/3.a80649d1.js"><link rel="prefetch" href="/notebook/assets/js/30.51a26022.js"><link rel="prefetch" href="/notebook/assets/js/300.23a6a024.js"><link rel="prefetch" href="/notebook/assets/js/301.eb4276c9.js"><link rel="prefetch" href="/notebook/assets/js/302.2c696c44.js"><link rel="prefetch" href="/notebook/assets/js/303.a748a576.js"><link rel="prefetch" href="/notebook/assets/js/304.95020a99.js"><link rel="prefetch" href="/notebook/assets/js/305.c4bc6072.js"><link rel="prefetch" href="/notebook/assets/js/306.74133b05.js"><link rel="prefetch" href="/notebook/assets/js/307.6ea724f3.js"><link rel="prefetch" href="/notebook/assets/js/308.fc7b065c.js"><link rel="prefetch" href="/notebook/assets/js/309.56497801.js"><link rel="prefetch" href="/notebook/assets/js/31.c351e10d.js"><link rel="prefetch" href="/notebook/assets/js/310.692379f9.js"><link rel="prefetch" href="/notebook/assets/js/311.b7393f95.js"><link rel="prefetch" href="/notebook/assets/js/312.f3eec1e1.js"><link rel="prefetch" href="/notebook/assets/js/313.9227351c.js"><link rel="prefetch" href="/notebook/assets/js/314.6960877d.js"><link rel="prefetch" href="/notebook/assets/js/315.f55a1979.js"><link rel="prefetch" href="/notebook/assets/js/316.6121039c.js"><link rel="prefetch" href="/notebook/assets/js/317.7ba118c8.js"><link rel="prefetch" href="/notebook/assets/js/318.2b71444c.js"><link rel="prefetch" href="/notebook/assets/js/319.bc0d5ccf.js"><link rel="prefetch" href="/notebook/assets/js/32.8a802a22.js"><link rel="prefetch" href="/notebook/assets/js/320.79f13ae1.js"><link rel="prefetch" href="/notebook/assets/js/321.21d3b0cf.js"><link rel="prefetch" href="/notebook/assets/js/322.87e4c143.js"><link rel="prefetch" href="/notebook/assets/js/323.4dee2eb7.js"><link rel="prefetch" href="/notebook/assets/js/324.f8c64322.js"><link rel="prefetch" href="/notebook/assets/js/325.c82057d6.js"><link rel="prefetch" href="/notebook/assets/js/326.3ea0d22b.js"><link rel="prefetch" href="/notebook/assets/js/327.90b878d9.js"><link rel="prefetch" href="/notebook/assets/js/328.59e55f0a.js"><link rel="prefetch" href="/notebook/assets/js/329.95fb2ef0.js"><link rel="prefetch" href="/notebook/assets/js/33.18cd7b09.js"><link rel="prefetch" href="/notebook/assets/js/330.ed1fb0e9.js"><link rel="prefetch" href="/notebook/assets/js/331.b84d88a9.js"><link rel="prefetch" href="/notebook/assets/js/332.20dffd14.js"><link rel="prefetch" href="/notebook/assets/js/333.d625fbd2.js"><link rel="prefetch" href="/notebook/assets/js/334.4fedc08a.js"><link rel="prefetch" href="/notebook/assets/js/335.c3b6c886.js"><link rel="prefetch" href="/notebook/assets/js/336.cf000555.js"><link rel="prefetch" href="/notebook/assets/js/337.891a7e6c.js"><link rel="prefetch" href="/notebook/assets/js/338.23da071e.js"><link rel="prefetch" href="/notebook/assets/js/339.92d07729.js"><link rel="prefetch" href="/notebook/assets/js/34.f39f39b2.js"><link rel="prefetch" href="/notebook/assets/js/340.09cb4417.js"><link rel="prefetch" href="/notebook/assets/js/341.3591e649.js"><link rel="prefetch" href="/notebook/assets/js/342.568a9320.js"><link rel="prefetch" href="/notebook/assets/js/343.e61b523f.js"><link rel="prefetch" href="/notebook/assets/js/344.61bb135b.js"><link rel="prefetch" href="/notebook/assets/js/345.f861e5aa.js"><link rel="prefetch" href="/notebook/assets/js/346.c5c70e0f.js"><link rel="prefetch" href="/notebook/assets/js/347.9b389847.js"><link rel="prefetch" href="/notebook/assets/js/348.eb62b86e.js"><link rel="prefetch" href="/notebook/assets/js/349.d4852195.js"><link rel="prefetch" href="/notebook/assets/js/35.c31fd7ed.js"><link rel="prefetch" href="/notebook/assets/js/350.f1db6bfd.js"><link rel="prefetch" href="/notebook/assets/js/351.4d86adaf.js"><link rel="prefetch" href="/notebook/assets/js/36.624192b1.js"><link rel="prefetch" href="/notebook/assets/js/37.680f8e12.js"><link rel="prefetch" href="/notebook/assets/js/38.f9ecec66.js"><link rel="prefetch" href="/notebook/assets/js/39.afab4ce6.js"><link rel="prefetch" href="/notebook/assets/js/4.03ba6111.js"><link rel="prefetch" href="/notebook/assets/js/40.f66ecac0.js"><link rel="prefetch" href="/notebook/assets/js/41.87cdca0e.js"><link rel="prefetch" href="/notebook/assets/js/42.08461558.js"><link rel="prefetch" href="/notebook/assets/js/43.ad5cf182.js"><link rel="prefetch" href="/notebook/assets/js/44.0bb6ad3f.js"><link rel="prefetch" href="/notebook/assets/js/45.5d2af6d4.js"><link rel="prefetch" href="/notebook/assets/js/46.8a06257e.js"><link rel="prefetch" href="/notebook/assets/js/47.3e37541c.js"><link rel="prefetch" href="/notebook/assets/js/48.024eda4c.js"><link rel="prefetch" href="/notebook/assets/js/49.a0685cf7.js"><link rel="prefetch" href="/notebook/assets/js/5.1071c8dd.js"><link rel="prefetch" href="/notebook/assets/js/50.130eaac4.js"><link rel="prefetch" href="/notebook/assets/js/51.0fe4dbd0.js"><link rel="prefetch" href="/notebook/assets/js/52.9d0ae64a.js"><link rel="prefetch" href="/notebook/assets/js/53.1ca09933.js"><link rel="prefetch" href="/notebook/assets/js/54.679cd78c.js"><link rel="prefetch" href="/notebook/assets/js/55.95cbe3a2.js"><link rel="prefetch" href="/notebook/assets/js/56.a58ec2af.js"><link rel="prefetch" href="/notebook/assets/js/57.0e59339a.js"><link rel="prefetch" href="/notebook/assets/js/58.487f643f.js"><link rel="prefetch" href="/notebook/assets/js/59.a8e9a1e3.js"><link rel="prefetch" href="/notebook/assets/js/6.707a1f11.js"><link rel="prefetch" href="/notebook/assets/js/60.c3080f7a.js"><link rel="prefetch" href="/notebook/assets/js/61.7f77e449.js"><link rel="prefetch" href="/notebook/assets/js/62.a5528e33.js"><link rel="prefetch" href="/notebook/assets/js/63.a787a8ee.js"><link rel="prefetch" href="/notebook/assets/js/64.7d3edfda.js"><link rel="prefetch" href="/notebook/assets/js/65.80e083e6.js"><link rel="prefetch" href="/notebook/assets/js/66.4076f29c.js"><link rel="prefetch" href="/notebook/assets/js/67.cf46f254.js"><link rel="prefetch" href="/notebook/assets/js/68.6fc8b1fd.js"><link rel="prefetch" href="/notebook/assets/js/69.4a344d72.js"><link rel="prefetch" href="/notebook/assets/js/7.c507c0e3.js"><link rel="prefetch" href="/notebook/assets/js/70.b13eef1a.js"><link rel="prefetch" href="/notebook/assets/js/71.20ad9776.js"><link rel="prefetch" href="/notebook/assets/js/72.30f44ef6.js"><link rel="prefetch" href="/notebook/assets/js/73.857a629d.js"><link rel="prefetch" href="/notebook/assets/js/74.a2b5a703.js"><link rel="prefetch" href="/notebook/assets/js/76.d64e4a53.js"><link rel="prefetch" href="/notebook/assets/js/77.40db9cc6.js"><link rel="prefetch" href="/notebook/assets/js/78.7a635d12.js"><link rel="prefetch" href="/notebook/assets/js/79.b2249421.js"><link rel="prefetch" href="/notebook/assets/js/8.a5f34392.js"><link rel="prefetch" href="/notebook/assets/js/80.d325f684.js"><link rel="prefetch" href="/notebook/assets/js/81.2e8d667e.js"><link rel="prefetch" href="/notebook/assets/js/82.885af8d1.js"><link rel="prefetch" href="/notebook/assets/js/83.b601bf2e.js"><link rel="prefetch" href="/notebook/assets/js/84.758d5dba.js"><link rel="prefetch" href="/notebook/assets/js/85.2e75fb85.js"><link rel="prefetch" href="/notebook/assets/js/86.6c68d815.js"><link rel="prefetch" href="/notebook/assets/js/87.8fba1553.js"><link rel="prefetch" href="/notebook/assets/js/88.e30608d9.js"><link rel="prefetch" href="/notebook/assets/js/89.be2f87c8.js"><link rel="prefetch" href="/notebook/assets/js/9.1c775f56.js"><link rel="prefetch" href="/notebook/assets/js/90.88dd69c4.js"><link rel="prefetch" href="/notebook/assets/js/91.59a69041.js"><link rel="prefetch" href="/notebook/assets/js/92.b46ca339.js"><link rel="prefetch" href="/notebook/assets/js/93.aeaec51d.js"><link rel="prefetch" href="/notebook/assets/js/94.5a852633.js"><link rel="prefetch" href="/notebook/assets/js/95.4f445663.js"><link rel="prefetch" href="/notebook/assets/js/96.6299f802.js"><link rel="prefetch" href="/notebook/assets/js/97.6cf3ba23.js"><link rel="prefetch" href="/notebook/assets/js/98.b48d73e6.js"><link rel="prefetch" href="/notebook/assets/js/99.f61a2e23.js">
    <link rel="stylesheet" href="/notebook/assets/css/0.styles.1b58b254.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/notebook/" class="home-link router-link-active"><img src="/notebook/img/logo.png" alt="notebook" class="logo"> <span class="site-name can-hide">notebook</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/head.jpg"> <div class="blogger-info"><h3>学习笔记</h3> <span>啦啦啦，向太阳~</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/notebook/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="基础" class="dropdown-title"><!----> <span class="title" style="display:;">基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/network/" class="nav-link">计算机网络</a></li><li class="dropdown-item"><!----> <a href="/notebook/computer-system/" class="nav-link">计算机系统</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-structure/" class="nav-link">数据结构与算法</a></li><li class="dropdown-item"><!----> <a href="/notebook/major/" class="nav-link">计算机专业课</a></li><li class="dropdown-item"><!----> <a href="/notebook/design-pattern/" class="nav-link">设计模式</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="开发" class="dropdown-title"><!----> <span class="title" style="display:;">开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://yubincloud.github.io/notebook-front/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  前端
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="/notebook/java/" class="nav-link">Java 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/python/" class="nav-link">Python 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/golang/" class="nav-link">Golang 开发</a></li><li class="dropdown-item"><!----> <a href="/notebook/git/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/notebook/software-architecture/" class="nav-link">软件设计与架构</a></li><li class="dropdown-item"><!----> <a href="/notebook/distributed-system/" class="nav-link">大数据与分布式系统</a></li><li class="dropdown-item"><h4>常见开发工具</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/nginx/" class="nav-link">Nginx</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据科学" class="dropdown-title"><!----> <span class="title" style="display:;">数据科学</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/data-science/spider/" class="nav-link">爬虫</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-science/py-data-analysis/" class="nav-link">Python 数据分析</a></li><li class="dropdown-item"><!----> <a href="/notebook/data-warehouse/" class="nav-link">数据仓库</a></li><li class="dropdown-item"><h4>中间件</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/mysql/" class="nav-link">MySQL</a></li><li class="dropdown-subitem"><a href="/notebook/redis/" class="nav-link">Redis</a></li><li class="dropdown-subitem"><a href="/notebook/elasticsearch/" class="nav-link">Elasticsearch</a></li><li class="dropdown-subitem"><a href="/notebook/kafka/" class="nav-link">Kafka</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/deep-learning/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/machine-learning/" class="nav-link">机器学习</a></li><li class="dropdown-item"><!----> <a href="/notebook/kg/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/notebook/gnn/" class="nav-link">图神经网络</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全" class="dropdown-title"><!----> <span class="title" style="display:;">安全</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/security/application-security/" class="nav-link">应用安全</a></li><li class="dropdown-item"><!----> <a href="/notebook/security/penetration/" class="nav-link">渗透测试</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="运维" class="dropdown-title"><!----> <span class="title" style="display:;">运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/ops/linux/" class="nav-link">Linux</a></li><li class="dropdown-item"><!----> <a href="/notebook/ops/cloud-native/" class="nav-link">云原生</a></li></ul></div></div><div class="nav-item"><a href="/notebook/pages/interview/index/" class="nav-link">面试</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="我的" class="dropdown-title"><!----> <span class="title" style="display:;">我的</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/pages/my/favorite/" class="nav-link">收藏</a></li><li class="dropdown-item"><!----> <a href="/notebook/pages/my/good-sentence/" class="nav-link">paper 好句</a></li></ul></div></div> <a href="https://github.com/yubincloud/notebook" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Posts</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>PyTorch 入门</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>鱼书进阶-自然语言处理</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/notebook/pages/nlp/network-basic/" class="sidebar-link">神经网络的复习</a></li><li><a href="/notebook/pages/nlp/word-representation/" class="sidebar-link">自然语言和单词的分布式表示</a></li><li><a href="/notebook/pages/nlp/word2vec/" class="sidebar-link">word2vec</a></li><li><a href="/notebook/pages/nlp/word2vec-speed/" class="sidebar-link">word2vec 高速化</a></li><li><a href="/notebook/pages/nlp/rnn/" class="sidebar-link">RNN</a></li><li><a href="/notebook/pages/nlp/gated-rnn/" class="sidebar-link">Gated RNN</a></li><li><a href="/notebook/pages/nlp/seq2seq/" class="sidebar-link">seq2seq</a></li><li><a href="/notebook/pages/nlp/attention/" aria-current="page" class="active sidebar-link">Attention</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/notebook/pages/nlp/attention/#_1-attention-的结构" class="sidebar-link">1. Attention 的结构</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_1-1-seq2seq-存在的问题" class="sidebar-link">1.1 seq2seq 存在的问题</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_1-2-改进-encoder" class="sidebar-link">1.2 改进 Encoder</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_1-3-解码器的改进-1" class="sidebar-link">1.3 解码器的改进 ①</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_1-4-解码器的改进-2" class="sidebar-link">1.4 解码器的改进 ②</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_1-5-解码器的改进-3" class="sidebar-link">1.5 解码器的改进 ③</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/nlp/attention/#_2-带-attention-的-seq2seq-的实现" class="sidebar-link">2. 带 Attention 的 seq2seq 的实现</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_2-1-编码器-attentionencoder-的实现" class="sidebar-link">2.1 编码器（AttentionEncoder）的实现</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_2-2-解码器-attentiondecoder-的实现" class="sidebar-link">2.2 解码器（AttentionDecoder）的实现</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_2-3-seq2seq-的实现" class="sidebar-link">2.3 seq2seq 的实现</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/nlp/attention/#_3-attenetion-的评价" class="sidebar-link">3. Attenetion 的评价</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_3-1-带-attention-的-seq2seq-的学习" class="sidebar-link">3.1 带 Attention 的 seq2seq 的学习</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_3-2-attention-的可视化" class="sidebar-link">3.2 Attention 的可视化</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/nlp/attention/#_4-关于-attention-的其他话题" class="sidebar-link">4. 关于 Attention 的其他话题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_4-1-双向-rnn" class="sidebar-link">4.1 双向 RNN</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_4-2-attention-层的使用方法" class="sidebar-link">4.2 Attention 层的使用方法</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_4-3-seq2seq-的深层化和-skip-connection" class="sidebar-link">4.3 seq2seq 的深层化和 skip connection</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/nlp/attention/#_5-attention-的应用" class="sidebar-link">5. Attention 的应用</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_5-1-gnmt" class="sidebar-link">5.1 GNMT</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_5-2-transformer" class="sidebar-link">5.2 Transformer</a></li><li class="sidebar-sub-header level3"><a href="/notebook/pages/nlp/attention/#_5-3-ntm" class="sidebar-link">5.3 NTM</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/notebook/pages/nlp/attention/#参考文献" class="sidebar-link">参考文献</a></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>深度学习-李宏毅</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2017版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>李宏毅-2019版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>预训练语言模型-邵浩2021版</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>王树森</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>机器学习</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>知识图谱</span> <span class="arrow right"></span></p> <!----></section></li></ul> <div class="sidebar-slot sidebar-slot-bottom"><!-- 正方形 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="3508773082"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div></aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/notebook/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/notebook/categories/?category=AI" title="分类" data-v-06225672>AI</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="分类" data-v-06225672>深度学习</a></li><li data-v-06225672><a href="/notebook/categories/?category=%E9%B1%BC%E4%B9%A6%E8%BF%9B%E9%98%B6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86" title="分类" data-v-06225672>鱼书进阶-自然语言处理</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/yubincloud" target="_blank" title="作者" class="beLink" data-v-06225672>yubin</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2022-04-02</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">Attention<!----></h1> <div class="page-slot page-slot-top"><!-- 固定100% * 90px可显示，max-height:90px未见显示-->
     <ins class="adsbygoogle"
          style="display:inline-block;width:100%;max-height:90px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6625304284"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="theme-vdoing-content content__default"><p>本章我们将进一步探索 seq2seq 的可能性（以及 RNN 的可能性）。Attention 毫无疑问是近年来深度学习领域最重要的技术之一。本章的目标是在代码层面理解 Attention 的结构，然后将其应用于实际问题。</p> <h2 id="_1-attention-的结构"><a href="#_1-attention-的结构" class="header-anchor">#</a> 1. Attention 的结构</h2> <p>我们将介绍进一步强化 seq2seq 的<strong>注意力机制</strong>。基于 Attention 机制，seq2seq 可以像我们人类一样，将“注意力”集中在必要的信息上。</p> <blockquote><p>上一章我们已经对 seq2seq 进行了改进，但那些只能算是“小改进”。下面将要说明的 <strong>Attention 技术才是解决 seq2seq 的问题的“大改进”</strong>。</p></blockquote> <h3 id="_1-1-seq2seq-存在的问题"><a href="#_1-1-seq2seq-存在的问题" class="header-anchor">#</a> 1.1 seq2seq 存在的问题</h3> <p>seq2seq 中使用编码器对时序数据进行编码，输出是固定长度的向量，<strong>问题在于无论输入语句的长度如何，其信息都会被塞入一个固定长度的向量中</strong>。而这早晚都会遇到瓶颈，有用的信息会从向量中溢出。</p> <p>现在我们就来改进 seq2seq。首先改进编码器，然后再改进解码器。</p> <h3 id="_1-2-改进-encoder"><a href="#_1-2-改进-encoder" class="header-anchor">#</a> 1.2 改进 Encoder</h3> <p>编码器的输出的长度应该根据输入文本的长度相应地改变。之前我们只将 LSTM 层的最后的隐藏状态传递给解码器，改进为可以使用各个时刻的 LSTM 层的隐藏状态，从而可以获得和输入的单词数相同数量的向量：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402222216049.png" alt="image-20220402222216049" style="zoom:80%;"> <p>如上例，输入了 5 个单词，此时编码器输出 5 个向量。这样一来，编码器就摆脱了“一个固定长度的向量”的制约。</p> <blockquote><p>在许多深度学习框架中，在初始化 RNN 层时，可以选择是返回“全部时刻的隐藏状态向量”，还是返回“最后时刻的隐藏状态向量”。比如，在 Keras 中，在初始化 RNN 层时，可以设置 <code>return_sequences</code> 为 True 或者 False。</p></blockquote> <p>我们需要关注 LSTM 层的隐藏状态的“内容”。有一点可以确定的是，<strong>各个时刻的隐藏状态中包含了大量当前时刻的输入单词的信息</strong>：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402223535528.png" alt="image-20220402223535528" style="zoom:80%;"> <ul><li>编码器输出的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 矩阵就可以视为各个单词对应的向量集合。</li></ul> <blockquote><p>因为编码器是从左向右处理的，所以严格来说，刚才的“猫”向量中含有“吾輩”“は”“猫”这3个单词的信息。<u>考虑整体的平衡性，最好均衡地含有单词“猫”周围的信息。在这种情况下，从两个方向处理时序数据的双向RNN（或者双向LSTM）比较有效</u>。</p></blockquote> <p>以上就是我们的改进：将编码器的全部时刻的隐藏状态取出来，从而编码器可以根据输入语句的长度，成比例地编码信息。</p> <p>接下来，我们对解码器进行改进。因为解码器的改进有许多值得讨论的地方，所以我们分 3 部分进行。</p> <h3 id="_1-3-解码器的改进-1"><a href="#_1-3-解码器的改进-1" class="header-anchor">#</a> 1.3 解码器的改进 ①</h3> <p>编码器和解码器的关系：编码器整体输出各个单词对应的 LSTM 层的隐藏状态向量 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container>，然后传递给解码器进行时间序列的转换。之前的解码器只用到了编码器 LSTM 层的最后一个隐藏状态，现在我们改进解码器，以便能够使用全部 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container>。</p> <p>我们在进行翻译时，大脑做了什么呢？比如，在将“吾輩は猫である”这句话翻译为英文时，肯定要用到诸如“吾輩 = I”“猫 = cat”这样的知识。也就是说，<strong>可以认为我们是专注于某个单词（或者单词集合），随时对这个单词进行转换的</strong>。那么，我们可以在 seq2seq 中重现同样的事情吗？确切地说，我们可以让 seq2seq 学习“输入和输出中哪些单词与哪些单词有关”这样的对应关系吗？</p> <blockquote><p>在机器翻译的历史中，很多研究都利用“猫=cat”这样的单词对应关系的知识。这样的表示单词（或者词组）对应关系的信息称为<strong>对齐</strong>（alignment）。到目前为止，对齐主要是手工完成的，而我们将要介绍的 <strong>Attention 技术则成功地将对齐思想自动引入到了 seq2seq 中</strong>。这也是从“手工操作”到“机械自动化”的演变。</p></blockquote> <p>从现在开始，我们的目标是找出与“翻译目标词”有对应关系的“翻译源词”的信息，然后利用这个信息进行翻译。也就是说，我们的目标是<u>仅关注必要的信息，并根据该信息进行时序转换。这个机制称为 <strong>Attention</strong></u>。</p> <p>先看一下 Decoder 的整体框架：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402225550054.png" alt="image-20220402225550054" style="zoom:80%;"> <p>我们新增一个进行“某种计算”的层。这个“某种计算”接收（解码器）各个时刻的 LSTM 层的隐藏状态和编码器的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container>。然后，从中选出必要的信息，并输出到 Affine 层。与之前一样，编码器的最后的隐藏状态向量（即 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 的最后一行）传递给解码器最初的 LSTM 层。</p> <p>该网络所做的工作是提取单词对齐信息。具体来说，就是<strong>从 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 中选出与各个时刻解码器输出的单词有对应关系的单词向量</strong>（<em>这有点像将注意力集中到重要的信息上</em>），如当解码器输出“I”时，从 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 中选出“吾輩”的对应向量。我们希望“某种计算”实现这种选择操作，不过这里有个问题，就是选择（从多个事物中选取若干个）这一操作是无法进行微分的。</p> <blockquote><p>神经网络的学习一般通过误差反向传播法进行。因此，如果使用可微分的运算构造网络，就可以在误差反向传播法的框架内进行学习；而<strong>如果不使用可微分的运算，基本上也就没有办法使用误差反向传播法</strong>。</p></blockquote> <p>将“选择”这一操作换成可微分的运算的一个思路是：<strong>与其“单选”，不如“全选”，并另行计算表示各个单词重要度（贡献值）的权重</strong>：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402231550018.png" alt="image-20220402231550018" style="zoom:80%;"> <p><strong>这里使用了表示各个单词重要度的权重，记为 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container></strong>。它的各元素是 0.0 ～ 1.0 的标量，总和是 1。然后，计算这个表示各个单词重要度的权重和单词向量 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 的加权和，可以获得目标向量：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402231736575.png" alt="image-20220402231736575" style="zoom:80%;"> <p>计算单词向量的加权和，这里将结果称为<strong>上下文向量</strong>， 并用符号 c 表示。这个加权和计算基本代替了“选择”向量的操作。比如上图中的上下文向量就含有较多的“吾輩”向量的成分（<em>如此便实现了注意力的集中</em>）。</p> <blockquote><p>上下文向量 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="c"></mjx-c></mjx-mi></mjx-math></mjx-container> 中包含了当前时刻进行变换（翻译）所需的信息。更确切地说，模型要从数据中学习出这种能力。</p></blockquote> <p>这里随意地生成编码器的输出 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 和各个单词的权重 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container>，并给出求它们的加权和的实现：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
T<span class="token punctuation">,</span> H <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span>
hs <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>T<span class="token punctuation">,</span> H<span class="token punctuation">)</span>
a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.03</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

ar <span class="token operator">=</span> a<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>ar<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (5, 4)</span>

t <span class="token operator">=</span> hs <span class="token operator">*</span> ar
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (5, 4)</span>

c <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (4,)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><ul><li>时序数据的长度 T=5，隐藏状态向量的元素个数 H=4</li> <li>代码 <code>ar = a.reshape(5, 1).repeat(4, axis=1)</code> 将 a 转化为 ar：</li></ul> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403065249268.png" alt="image-20220403065249268" style="zoom:80%;"> <ul><li>先计算 hs 与 ar 的对应元素的乘积，然后通过 <code>c=sum(hs*ar, axis=0)</code> 消除第 0 个轴得到形状为 (4,) 的张量，即加权和。</li></ul> <blockquote><p><code>repeat()</code> 方法复制多维数组的元素生成新的多维数组，axis 指定要进行复制的轴（维度），比如在 x 的形 状为 (X, Y, Z) 的情况下，<code>x.repeat(3, axis=1)</code> 沿 x 的第1个轴方向（第 1个维度）进行复制，生成形状为 (X, 3*Y, Z) 的多维数组。</p> <p>这里其实也可以不用 repeat 而是使用 numpy 的广播功能，但我们为了显式表现出 repeat 节点，所以采用了显式调用 <code>repeat</code> 函数：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403065525024.png" alt="image-20220403065525024" style="zoom:80%;"></blockquote> <p>这里计算加权和的计算图可以绘制为：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403070500922.png" alt="image-20220403070500922" style="zoom:80%;"> <p>我们将这个计算加权和的计算图实现为 Weight Sum 层。</p> <details class="custom-block details"><summary>Weight Sum 层的实现</summary> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">WeightSum</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>cache <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hs<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H <span class="token operator">=</span> hs<span class="token punctuation">.</span>shape

        ar <span class="token operator">=</span> a<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>H<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        t <span class="token operator">=</span> hs <span class="token operator">*</span> ar
        c <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>cache <span class="token operator">=</span> <span class="token punctuation">(</span>hs<span class="token punctuation">,</span> ar<span class="token punctuation">)</span>
        <span class="token keyword">return</span> c

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dc<span class="token punctuation">)</span><span class="token punctuation">:</span>
        hs<span class="token punctuation">,</span> ar <span class="token operator">=</span> self<span class="token punctuation">.</span>cache
        N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H <span class="token operator">=</span> hs<span class="token punctuation">.</span>shape
        dt <span class="token operator">=</span> dc<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>T<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        dar <span class="token operator">=</span> dt <span class="token operator">*</span> hs
        dhs <span class="token operator">=</span> dt <span class="token operator">*</span> ar
        da <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dar<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> dhs<span class="token punctuation">,</span> da
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><ul><li>这个层没有要学习的参数</li></ul></details> <h3 id="_1-4-解码器的改进-2"><a href="#_1-4-解码器的改进-2" class="header-anchor">#</a> 1.4 解码器的改进 ②</h3> <p>有了表示各个单词重要度的权重 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container>，就可以通过加权和获得上下文向量。那么，<strong>怎么求这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 呢？只需要让模型自动学习它</strong>。</p> <p>下面我们来看一下各个单词的权重 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 的求解方法。下图是从编码器的处理开始到解码器第一个 LSTM 层输出隐藏状态向量的处理为止的流程：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403071308638.png" alt="image-20220403071308638" style="zoom:80%;"> <p>用 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container> 表示解码器的 LSTM 层的隐藏状态向量，此时，我们的目标是用数值表示这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-math></mjx-container> 在多大程度上和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 的各个单词向量“相似”。这里我们使用最简单的向量内积。</p> <blockquote><p>计算向量相似度的方法有好几种。除了内积之外，还有使用小型的神经网络输出得分的做法。</p> <p>文献<sup>[49]</sup>提出了几种输出得分的方法</p></blockquote> <p>下面用图表示基于内积计算向量间相似度的处理流程：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403075145804.png" alt="image-20220403075145804" style="zoom:80%;"> <ul><li>这里通过向量内积算出 h 和 hs 的各个单词向量之间 的相似度，并将其结果表示为 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container>。不过这个 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 是正规化之前的值，也称为得分。</li></ul> <p>接下来使用 softmax 对 s 进行正规化：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403075338015.png" alt="image-20220403075338015" style="zoom:80%;"> <p>使用 Softmax 函数之后，输出的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 的各个元素的值在 0.0 ～ 1.0，总和 为 1，这样就求得了表示各个单词权重的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container>。</p> <p>从代码的角度看一下这个处理过程：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span>
hs <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H<span class="token punctuation">)</span>
h <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> H<span class="token punctuation">)</span>
hr <span class="token operator">=</span> h<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>T<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># hr = h.reshape(N, 1, H) # 广播</span>

t <span class="token operator">=</span> hs <span class="token operator">*</span> hr
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (10, 5, 4)</span>

s <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (10, 5)</span>

softmax <span class="token operator">=</span> Softmax<span class="token punctuation">(</span><span class="token punctuation">)</span>
a <span class="token operator">=</span> softmax<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># (10, 5)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>计算图绘制如下：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403075631059.png" alt="image-20220403075631059" style="zoom:80%;"> <p>我们将这个计算图表示的处理实现为 <code>AttentionWeight</code> 类：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">AttentionWeight</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> Softmax<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>cache <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hs<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">:</span>
        N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H <span class="token operator">=</span> hs<span class="token punctuation">.</span>shape

        hr <span class="token operator">=</span> h<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>T<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        t <span class="token operator">=</span> hs <span class="token operator">*</span> hr
        s <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        a <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>s<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>cache <span class="token operator">=</span> <span class="token punctuation">(</span>hs<span class="token punctuation">,</span> hr<span class="token punctuation">)</span>
        <span class="token keyword">return</span> a

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> da<span class="token punctuation">)</span><span class="token punctuation">:</span>
        hs<span class="token punctuation">,</span> hr <span class="token operator">=</span> self<span class="token punctuation">.</span>cache
        N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H <span class="token operator">=</span> hs<span class="token punctuation">.</span>shape

        ds <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>da<span class="token punctuation">)</span>
        dt <span class="token operator">=</span> ds<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>H<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        dhs <span class="token operator">=</span> dt <span class="token operator">*</span> hr
        dhr <span class="token operator">=</span> dt <span class="token operator">*</span> hs
        dh <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dhr<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> dhs<span class="token punctuation">,</span> dh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><h3 id="_1-5-解码器的改进-3"><a href="#_1-5-解码器的改进-3" class="header-anchor">#</a> 1.5 解码器的改进 ③</h3> <p>之前我们实现了 Weight Sum 层和 Attention Weight 层，现在我们将这两层组合起来：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403082632703.png" alt="image-20220403082632703" style="zoom:80%;"> <p>上图显示了用于获取上下文向量 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="c"></mjx-c></mjx-mi></mjx-math></mjx-container> 的计算图的全貌：</p> <ul><li>Attention Weight 层关注编码器输出的各个单词向量 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container>，并计算各个单词的权重 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container></li> <li>Weight Sum 层计算 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi></mjx-math></mjx-container> 和 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 的加权和，并输出上下文向量 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="c"></mjx-c></mjx-mi></mjx-math></mjx-container></li></ul> <p>我们将进行这一系列计算的层称为 <strong>Attention 层</strong>：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403083525528.png" alt="image-20220403083525528" style="zoom:80%;"> <p>以上就是 Attention 技术的核心内容：它关注编码器传递的信息 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 中的重要元素，基于它算出上下文向量，再传递给上一层。下面是 Attention 层的实现：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>attention_weight_layer <span class="token operator">=</span> AttentionWeight<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight_sum_layer <span class="token operator">=</span> WeightSum<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention_weight <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hs<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">:</span>
        a <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weight_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>hs<span class="token punctuation">,</span> h<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>weight_sum_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>hs<span class="token punctuation">,</span> a<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention_weight <span class="token operator">=</span> a
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dhs0<span class="token punctuation">,</span> da <span class="token operator">=</span> self<span class="token punctuation">.</span>weight_sum_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>
        dhs1<span class="token punctuation">,</span> dh <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weight_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>da<span class="token punctuation">)</span>
        dhs <span class="token operator">=</span> dhs0 <span class="token operator">+</span> dhs1
        <span class="token keyword">return</span> dhs<span class="token punctuation">,</span> dh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>我们将这个 Attention 层放在 LSTM 层和 Affine 层的中间：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403084318023.png" alt="image-20220403084318023" style="zoom:80%;"> <p>编码器的输出 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="s"></mjx-c></mjx-mi></mjx-math></mjx-container> 被输入到各个时刻的 Attention 层。另外，这里将 LSTM 层的隐藏状态向量输入 Affine 层。如下图所示，与之前的网络结构相比，我们将基于 Attention 的上下文向量信息“添加”到了之前实现的解码器上：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403084502759.png" alt="image-20220403084502759" style="zoom:80%;"> <blockquote><p>上下文向量和隐藏状态向量这两个向量被输入 Affine 层。如前所述，这意味着将这两个向量拼接起来，将拼接后的向量输入 Affine 层。</p></blockquote> <p>最后，我们在时序方向上扩展多个 Attention 层整体实现为 Time Attention 层：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403085338283.png" alt="image-20220403085338283" style="zoom:80%;"> <ul><li>Time Attention 层只是组合了多个 Attention 层</li></ul> <details class="custom-block details"><summary>Time Attention 的实现</summary> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">TimeAttention</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>attention_weights <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hs_enc<span class="token punctuation">,</span> hs_dec<span class="token punctuation">)</span><span class="token punctuation">:</span>
        N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H <span class="token operator">=</span> hs_dec<span class="token punctuation">.</span>shape
        out <span class="token operator">=</span> np<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>hs_dec<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>attention_weights <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layer <span class="token operator">=</span> Attention<span class="token punctuation">(</span><span class="token punctuation">)</span>
            out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>hs_enc<span class="token punctuation">,</span> hs_dec<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>t<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>attention_weight<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H <span class="token operator">=</span> dout<span class="token punctuation">.</span>shape
        dhs_enc <span class="token operator">=</span> <span class="token number">0</span>
        dhs_dec <span class="token operator">=</span> np<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>

        <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layer <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span>t<span class="token punctuation">]</span>
            dhs<span class="token punctuation">,</span> dh <span class="token operator">=</span> layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            dhs_enc <span class="token operator">+=</span> dhs
            dhs_dec<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>t<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> dh

        <span class="token keyword">return</span> dhs_enc<span class="token punctuation">,</span> dhs_dec
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><ul><li>这里仅创建必要数量的 Attention 层（代码中为 T 个），各自进行正向传播和反向传播</li> <li><code>attention_weights</code> 列表中保存了各个 Attention 层对各个单词的权重</li></ul></details> <p>下面我们使用 Attention 来实现 seq2seq，并尝试挑战一个真实问题，以确认 Attention 的效果。</p> <h2 id="_2-带-attention-的-seq2seq-的实现"><a href="#_2-带-attention-的-seq2seq-的实现" class="header-anchor">#</a> 2. 带 Attention 的 seq2seq 的实现</h2> <p>我们分别实现 3 个类：AttentionEncoder、AttentionDecoder 和 AttentionSeq2seq。</p> <h3 id="_2-1-编码器-attentionencoder-的实现"><a href="#_2-1-编码器-attentionencoder-的实现" class="header-anchor">#</a> 2.1 编码器（AttentionEncoder）的实现</h3> <p>它与上一章的 Encoder 唯一的区别在于 Encoder 类的 forward 仅返回 LSTM 层的最后一个隐藏状态向量，而 AttentionEncoder 返回所有的隐藏状态向量：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">AttentionEncoder</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">)</span><span class="token punctuation">:</span>
	xs <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
	hs <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
	<span class="token keyword">return</span> hs
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h3 id="_2-2-解码器-attentiondecoder-的实现"><a href="#_2-2-解码器-attentiondecoder-的实现" class="header-anchor">#</a> 2.2 解码器（AttentionDecoder）的实现</h3> <p>使用了 Attention 的解码器的层结构如下图所示：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403090855672.png" alt="image-20220403090855672" style="zoom:80%;"> <p>与之前一样，解码器还多了一个生成新单词序列的 <code>generate()</code> 方法。</p> <p>这里给出其核心实现：</p> <div class="language-python line-numbers-mode"><div class="highlight-lines"><br><br><br><br><br><br><br><br><br><br><br><br><br><br><div class="highlighted"> </div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><div class="highlighted"> </div><div class="highlighted"> </div><br><br><br><br></div><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">AttentionDecoder</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        V<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size
        rn <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn

        embed_W <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>V<span class="token punctuation">,</span> D<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'f'</span><span class="token punctuation">)</span>
        lstm_Wx <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>D<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>D<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'f'</span><span class="token punctuation">)</span>
        lstm_Wh <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>H<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'f'</span><span class="token punctuation">)</span>
        lstm_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'f'</span><span class="token punctuation">)</span>
        affine_W <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>H<span class="token punctuation">,</span> V<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>H<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'f'</span><span class="token punctuation">)</span>
        affine_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'f'</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> TimeEmbedding<span class="token punctuation">(</span>embed_W<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> TimeLSTM<span class="token punctuation">(</span>lstm_Wx<span class="token punctuation">,</span> lstm_Wh<span class="token punctuation">,</span> lstm_b<span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> TimeAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>affine <span class="token operator">=</span> TimeAffine<span class="token punctuation">(</span>affine_W<span class="token punctuation">,</span> affine_b<span class="token punctuation">)</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>embed<span class="token punctuation">,</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">,</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>params<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>params <span class="token operator">+=</span> layer<span class="token punctuation">.</span>params
            self<span class="token punctuation">.</span>grads <span class="token operator">+=</span> layer<span class="token punctuation">.</span>grads

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">,</span> enc_hs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        h <span class="token operator">=</span> enc_hs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>set_state<span class="token punctuation">(</span>h<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
        dec_hs <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        c <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>enc_hs<span class="token punctuation">,</span> dec_hs<span class="token punctuation">)</span>
        out <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> dec_hs<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        score <span class="token operator">=</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token keyword">return</span> score
</code></pre><div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br></div></div><p>最后，我们使用 AttentionEncoder 类和 AttentionDecoder 类来实现 AttentionSeq2seq 类。</p> <h3 id="_2-3-seq2seq-的实现"><a href="#_2-3-seq2seq-的实现" class="header-anchor">#</a> 2.3 seq2seq 的实现</h3> <p>AttentionSeq2seq 类的实现也和上一章实现的 seq2seq 几乎一样。区别仅在于，编码器使用 AttentionEncoder 类，解码器使用 AttentionDecoder 类。因此，只要继承上一章的 Seq2seq 类，并改一下初始化方法，就可以实现 AttentionSeq2seq 类：</p> <div class="language-python line-numbers-mode"><div class="highlight-lines"><br><br><br><div class="highlighted"> </div><div class="highlighted"> </div><br><br><br><br><br></div><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">AttentionSeq2seq</span><span class="token punctuation">(</span>Seq2seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        args <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> AttentionEncoder<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> AttentionDecoder<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> TimeSoftmaxWithLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>params <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>params <span class="token operator">+</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>params
        self<span class="token punctuation">.</span>grads <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>grads <span class="token operator">+</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>grads
</code></pre><div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h2 id="_3-attenetion-的评价"><a href="#_3-attenetion-的评价" class="header-anchor">#</a> 3. Attenetion 的评价</h2> <p>我们通过研究“日期格式转换”问题来确认带 Attention 的 seq2seq 的效果：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403093356251.png" alt="image-20220403093356251" style="zoom:67%;"> <blockquote><p>其实应该研究翻译问题来确认其效果，但没能找到合适的数据集。<strong>WMT</strong> 是一个有名的翻译数据集，在许多研究中都被作为基准使用，经常用于评价 seq2seq 的性能，不过它的数据量很大（超过 20 GB），使用起来不是很方便。</p></blockquote> <p>采用该问题的原因：这个问题的输入形式较为复杂，所以手工编写转换规则也比较复杂。其次问句与回答之间存在明显对应关系，可以用于确认 Attention 有没有有正确地关注各自的对应元素。</p> <p>我们的数据集：<img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403093612685.png" alt="image-20220403093612685" style="zoom:67%;"></p> <p>我们对输入语句通过填充空格来对齐。因为这个问题输出的字符数是恒定的，所以无须使用分隔符来指示输出的结束。</p> <h3 id="_3-1-带-attention-的-seq2seq-的学习"><a href="#_3-1-带-attention-的-seq2seq-的学习" class="header-anchor">#</a> 3.1 带 Attention 的 seq2seq 的学习</h3> <p>我们在日期转换用的数据集上进行 AttentionSeq2seq 的学习，具体的学习代码可见鱼书的附带资源。</p> <p>在学习数据的过程中还使用了反转输入语句的技巧，在每个 epoch 使用测试数据计算正确率。随着学习的进行，结果如图：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403094641348.png" alt="image-20220403094641348" style="zoom:80%;"> <p>随着学习的深入，带 Attention 的 seq2seq 变聪明了。实际上，没过多久，它就对大多数问题给出了正确答案：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403095011338.png" alt="image-20220403095011338" style="zoom:80%;"> <p>与之前的模型相比：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403095042615.png" alt="image-20220403095042615" style="zoom:80%;"> <p>在这次的实验中，就最终精度来看，Attention 和 Peeky 取得了差不多的结果。但是，随着时序数据变长、变复杂，除了学习速度之外，Attention 在精度上也会变得更有优势。</p> <h3 id="_3-2-attention-的可视化"><a href="#_3-2-attention-的可视化" class="header-anchor">#</a> 3.2 Attention 的可视化</h3> <p>在我们的实现中，Time Attention 层中的成员变量 attention_weights 保存了各个时刻的 Attention 权重，据此可以将输入语句和输出语句的各个单词的对应关系绘制成一张二维地图：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403100734306.png" alt="image-20220403100734306" style="zoom:80%;"> <ul><li>我们可以看到，当 seq2seq 输出第 1 个“1”时，注意力集中在输入语句的“1”上</li> <li>输入语句的“AUGUST”对应于表示月份的“08”，这表明 seq2seq 从数据中学习到了“August”和“8 月”的对应关系。</li></ul> <p>像这样，使用 Attention，seq2seq 能像我们人一样将注意力集中在必要的信息上。</p> <blockquote><p>我们没有办法理解神经网络内部进行了什么工作（基于何种逻辑工作），而 Attention 赋予了模型“人类可以理解的结构和意义”。在上面的例子中，通过 Attention，我们看到了单词和单词之间的关联性。由此，我们可以判断模型的工作逻辑是否符合人类的逻辑。</p></blockquote> <p>下一节我们继续围绕 Attention，介绍它的几个高级技巧。</p> <h2 id="_4-关于-attention-的其他话题"><a href="#_4-关于-attention-的其他话题" class="header-anchor">#</a> 4. 关于 Attention 的其他话题</h2> <p>我们研究了带 Attention 的 seq2seq，现在我们介绍几个之前未涉及的话题。</p> <h3 id="_4-1-双向-rnn"><a href="#_4-1-双向-rnn" class="header-anchor">#</a> 4.1 双向 RNN</h3> <p>这里我们关注 seq2seq 的编码器：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220402222216049.png" alt="image-20220402222216049" style="zoom:80%;"> <p>这里我们是从左向右阅读句子的，因此单词“猫”的对应向量编码了“吾輩”“は”“猫”这 3 个单词的信息。如果考虑整体的平衡性，我们希望向量能更均衡地包含单词“猫”周围的信息。</p> <p>为此，<u>可以让 LSTM 从两个方向进行处理，这就是名为<strong>双向 LSTM</strong> 的技术</u>：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403102331734.png" alt="image-20220403102331734" style="zoom:80%;"> <ul><li>双向 LSTM 在之前的 LSTM 层上添加了一个反方向处理的 LSTM 层。然后，拼接各个时刻的两个 LSTM 层的隐藏状态，将其作为最后的隐藏状态向量（除了拼接之外，也可以“求和”或者“取平均”等）</li></ul> <p><strong>通过这样的双向处理，各个单词对应的隐藏状态向量可以从左右两个方向聚集信息。这样一来，这些向量就编码了更均衡的信息</strong>。</p> <p>双向 LSTM 的实现非常简单。一种实现方式是准备两个 LSTM 层（本章中是 Time LSTM 层），并调整输入各个层的单词的排列。具体而言，其中一个层的输入语句与之前相同，这相当于从左向右处理输入语句的常规的 LSTM 层。而<strong>另一个 LSTM 层的输入语句则按照从右到左的顺序输入</strong>。如果原文是“A B C D”，就改为“D C B A”。通过输入改变了顺序的输入语句，另一个 LSTM 层从右向左处理输入语句。之后，只需要拼接这两个 LSTM 层的输出，就可以创建双向 LSTM 层。</p> <h3 id="_4-2-attention-层的使用方法"><a href="#_4-2-attention-层的使用方法" class="header-anchor">#</a> 4.2 Attention 层的使用方法</h3> <p>之前我们将 Attention 层插入了 LSTM 层和 Affine 层之间：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403084318023.png" alt="image-20220403084318023" style="zoom:80%;"> <p>实际上，使用 Attention 的模型还有其他好几种方式。文献<sup>[48]</sup>以下图的结构 使用了 Attention：</p> <img src="C:\Users\yubin\AppData\Roaming\Typora\typora-user-images\image-20220403103834178.png" alt="image-20220403103834178" style="zoom:80%;"> <ul><li>Attention 层的输出（上下文向量）被连接到了下一时刻的 LSTM 层的输入处。通过这种结构，LSTM 层得以使用上下文向量的信息。相对地，我们实现的模型则是 Affine 层使用了上下文向量。</li></ul> <p><strong>Attention 层的位置的不同对最终精度有何影响呢？答案要试一下才知道</strong>。实际上，这只能使用真实数据来验证。不过，在上面的两个模型中，上下文向量都得到了很好的应用。因此，在这两个模型之间，我们可能看不到太大的精度差异。</p> <h3 id="_4-3-seq2seq-的深层化和-skip-connection"><a href="#_4-3-seq2seq-的深层化和-skip-connection" class="header-anchor">#</a> 4.3 seq2seq 的深层化和 skip connection</h3> <p>通过加深层，可以创建表现力更强的模型，带 Attention 的 seq2seq 也是如此。那么，如果我们加深带 Attention 的 seq2seq，结果会怎样呢？以下图为例：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403104141207.png" alt="image-20220403104141207" style="zoom:67%;"> <ul><li>编码器和解码器使用了 3 层 LSTM 层</li> <li>这里将解码器 LSTM 层的隐藏状态输入 Attention 层，然后将上下文向量（Attention 层的输出）传给解码器的多个层（LSTM 层和 Affine 层）</li></ul> <blockquote><p>如本例所示，<strong>编码器和解码器中通常使用层数相同的 LSTM 层</strong>。</p></blockquote> <p>另外，在加深层时使用到的另一个重要技巧是<strong>残差连接</strong>（skip connection，也称为 residual connection 或 shortcut），这时一种跨层连接的简单技巧：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403104347206.png" alt="image-20220403104347206" style="zoom:67%;"> <p>所谓残差连接，就是指“跨层连接”，在残差连接的连接处，有两个输出被相加。因为加法在反向传播时“按原样”传播梯度，所以<strong>残差连接中的梯度可以不受任何影响地传播到前一个层</strong>。这样一来，即便加深了层，梯度也能正常传播，而不会发生梯度消失（或者梯度爆炸），学习可以顺利进行。</p> <blockquote><ul><li><strong>在时间方向上</strong>，RNN 层的反向传播会出现梯度消失或梯度爆炸的问题。梯度消失可以通过 LSTM、GRU 等 Gated RNN 应对，梯度爆炸可以通过梯度裁剪应对。</li> <li>而<strong>在深度方向上</strong>的梯度消失，这里介绍的残差连接很有效。</li></ul></blockquote> <h2 id="_5-attention-的应用"><a href="#_5-attention-的应用" class="header-anchor">#</a> 5. Attention 的应用</h2> <p>到目前为止，我们仅将 Attention 应用在了 seq2seq 上，但是 Attention 这一想法本身是通用的。本节我们将介绍 3 个使用了 Attention 的前沿研究。</p> <h3 id="_5-1-gnmt"><a href="#_5-1-gnmt" class="header-anchor">#</a> 5.1 GNMT</h3> <p>回看机器翻译的历史，我们可以发现主流方法随着时代的变迁而演变。从“基于规则的翻译”到“基于用例的翻译”，再到“基于统计的翻译”。现在，<strong>神经机器翻译</strong>（Neural Machine Translation）取代了这些过往的技术，获得了广泛关注。</p> <blockquote><p>神经机器翻译现在已经成为使用了 seq2seq 的机器翻译的统称。</p></blockquote> <p>谷歌推出的 GNMT（Google Neural Machine Translation）也是由由编码器、解码器和 Attention 构成，还有许多为了提高翻译精度而做的改进。除此以外，还进行了低频词处理、用于加速推理的量化等工作，从而得到了非常好的结果。</p> <h3 id="_5-2-transformer"><a href="#_5-2-transformer" class="header-anchor">#</a> 5.2 Transformer</h3> <p>使用 RNN 可以很好地处理可变长度的时序数据，但<strong>存在并行处理的问题</strong>。RNN 需要基于上一个时刻的计算结果逐步进行计算，导致了无法在时间方向上并行计算，这会成为一个很大的瓶颈。</p> <p>现在关于去除 RNN 的研究（可以并行计算的 RNN 的研究）很活跃，其中一个著名的模型是 Transformer 模型。Transformer 是在“Attention is all you need”这篇论文中提出来的方法。如论文标题所示，<strong>Transformer 不用 RNN，而用 Attention 进行处理</strong>。</p> <blockquote><p>除此之外，还有研究用 CNN 代替 RNN 来实现并行计算。</p></blockquote> <p>Transformer 是基于 Attention 构成的，其中使用了 Self-Attention 技巧，这一点很重要。<u><strong>Self-Attention</strong> 是以一个时序数据为对象的 Attention，旨在观察一个时序数据中每个元素与其他元素的关系</u>：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403110408187.png" alt="image-20220403110408187" style="zoom:80%;"> <p>上面左图的 Time Attention 层的两个输入中输入的是不同的时序数据，而右图的 Self-Attention 的两个输入中输入的是同一个时序数据，这样可以求得一个时序数据内各个元素之间的对应关系。</p> <p>至此，对 Self-Attention 的说明就结束了，下面我们看一下 Transformer 的层结构：</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403114543780.png" alt="image-20220403114543780"> <ul><li>Transformer 中用 Attention 代替了 RNN，编码器和解码器两者都使用了 Self-Attention</li> <li>Feed Forward 层表示前馈神经网络（在时间方向上独立的网络）</li> <li>图中的 <mjx-container jax="CHTML" class="MathJax"><mjx-math class=" MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="N"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container> 表示灰色背景包围的元素被堆叠了 N 次</li> <li>这个图是简化的 Transformer，实际上，Skip Connection、Layer Normalization 等技巧也会被用到。</li></ul> <p>使用 Transformer 可以控制计算量，充分利用 GPU 并行计算带来的好处，使得学习时间得以大幅减少。在翻译精度方面也实现了精度的提升。</p> <img src="https://notebook-img-1304596351.cos.ap-beijing.myqcloud.com/img/image-20220403124532511.png" alt="image-20220403124532511" style="zoom:80%;"> <p>由这个研究可知，<strong>Attention 其实可以用来替换 RNN</strong>。这样一来，利用 Attention 的机会可能会进一步增加。</p> <h3 id="_5-3-ntm"><a href="#_5-3-ntm" class="header-anchor">#</a> 5.3 NTM</h3> <p>可见计算机的内存操作可以通过神经网络复现。我们可以立刻想到一个方法：在 RNN 的外部配置一个存储信息的存储装置，并使用 Attention 向这个存储装置读写必要的信息。实际上，这样的研究有好几个，NTM （Neural Turing Machine，神经图灵机）<sup>[55]</sup> 就是其中比较有名的一个。</p> <p>基于外部存储装置的扩展技术和 Attention 会越来越重要，今后将被应用在各种地方。</p> <p>本部分不再展开，内容可参考鱼书或其他资料。</p> <h2 id="参考文献"><a href="#参考文献" class="header-anchor">#</a> 参考文献</h2> <details class="custom-block details"><summary>文献引用</summary> <p>[48] Bahdanau, Dzmitry, Kyunghyun Cho, Yoshua Bengio：Neural  machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.</p> <p>[49] Luong, Minh-Thang, Hieu Pham, Christopher D. Manning.Effective  approaches to attention-based neural machine translation[J]. arXiv  prelprint arXiv:1508.04025, 2015.</p> <p>[55] Graves, Alex, Greg Wayne, Ivo Danihelka,Neural turing machines[J].  arXiv preprint arXiv:1410.5401, 2014.</p></details></div></div> <div class="page-slot page-slot-bottom"><!-- 横向自适应 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6620245489"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="page-edit"><div class="edit-link"><a href="https://github.com/yubincloud/notebook/edit/master/docs/AI/01.深度学习/10.鱼书进阶-自然语言处理/08.Attention.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2022/08/26, 13:48:42</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/notebook/pages/nlp/seq2seq/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">seq2seq</div></a> <a href="/notebook/pages/lhy/regression/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Regression</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/notebook/pages/nlp/seq2seq/" class="prev">seq2seq</a></span> <span class="next"><a href="/notebook/pages/lhy/regression/">Regression</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/notebook/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/notebook/pages/ml/lhy/drl17/"><div>
            Deep Reinforcement Learning
            <!----></div></a> <span class="date">10-03</span></dt></dl><dl><dd>02</dd> <dt><a href="/notebook/pages/mysql/geektime/misdeletion/"><div>
            误删数据后怎么办
            <!----></div></a> <span class="date">04-06</span></dt></dl><dl><dd>03</dd> <dt><a href="/notebook/pages/mysql/geektime/multi-slaves/"><div>
            MySQL 一主多从
            <!----></div></a> <span class="date">03-22</span></dt></dl> <dl><dd></dd> <dt><a href="/notebook/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="yubin_inbuy@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/yubincloud" title="GitHub" target="_blank" class="iconfont icon-github"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2021-2024
    <span>yubincloud | <a href="https://github.com/yubincloud/notebook/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <div class="custom-html-window custom-html-window-rb" style="display:;"><div class="custom-wrapper"><span class="close-but">×</span> <div><!-- 固定160*160px -->
      <ins class="adsbygoogle"
          style="display:inline-block;max-width:160px;max-height:160px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="8377369658"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      </div></div></div></div><div class="global-ui"><div></div></div></div>
    <script src="/notebook/assets/js/app.2bf3b6c1.js" defer></script><script src="/notebook/assets/js/2.0ad58009.js" defer></script><script src="/notebook/assets/js/75.252e6fc0.js" defer></script>
  </body>
</html>
